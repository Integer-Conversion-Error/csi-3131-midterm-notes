What is the primary characteristic of virtual memory as a technique?,It requires entire processes to be loaded into physical memory for execution.,It allows processes to execute even if they are not entirely in physical memory.,It limits program size to the available physical memory.,It combines physical and logical memory into a single contiguous block.,It is solely responsible for managing CPU scheduling.,B,"The text states, ""Virtual memory: technique allowing execution of processes not entirely in memory."""
Virtual memory is primarily characterized by its separation of which two memory views?,Cache memory from main memory.,ROM from RAM.,Logical memory (programmer's view) from physical memory.,Primary storage from secondary storage.,System memory from user memory.,C,"The text explicitly states virtual memory ""Separates logical memory (programmer's view) from physical memory."""
What is considered a major advantage of using virtual memory?,It reduces the overall CPU clock speed.,It ensures all programs run faster regardless of I/O.,It allows programs to be larger than the physical memory available.,It eliminates the need for any form of memory management unit.,It simplifies hardware design by removing memory hierarchies.,C,"The text lists ""Major advantage: programs larger than physical memory."""
How does virtual memory primarily benefit programmers regarding memory limitations?,It forces programmers to manually manage physical memory addresses.,It restricts programmers to writing smaller programs.,It frees programmers from physical memory-storage limitations.,It increases the complexity of memory allocation for programmers.,It requires programs to be written in assembly language.,C,"The text states, ""Frees programmers from memory-storage limitations"" and ""Simplifies programming: no worry about physical memory limits."""
How does virtual memory contribute to increased CPU utilization and throughput?,By requiring more physical memory per program.,By decreasing the number of programs run concurrently.,"By allowing less physical memory per program, thus running more programs concurrently.",By increasing response and turnaround time.,By eliminating the need for context switching.,C,"The text explains, ""Less physical memory per program \u2192 more programs run concurrently \u2192 increased CPU utilization and throughput (no increase in response/turnaround time)."""
"One benefit of partial program execution in memory, facilitated by virtual memory, is its effect on I/O and program speed. What is this effect?","More I/O for loading/swapping, leading to slower program execution.","No change in I/O operations, but faster execution due to dedicated memory.","Less I/O for loading/swapping, leading to faster program execution.",Increased I/O but only for specific error handling routines.,It eliminates all I/O operations during program runtime.,C,"The text says, ""Less I/O for loading/swapping \u2192 faster program execution."""
What was the primary limitation of traditional memory management techniques before virtual memory?,Inability to run multiple programs simultaneously.,Programs were limited in size by the available physical memory.,It required complex hardware for memory allocation.,It could only manage read-only memory.,It prevented any form of shared memory.,B,"The text states under ""Background"" that ""Limitation: program size limited by physical memory."""
Which of the following is cited as a reason why real programs often do not need their entire code in physical memory?,Core program logic is always kept in cache memory.,All program options are used in every execution.,Error handling code is seldom executed.,Arrays and lists are always allocated the exact memory they need.,Programmers intentionally write redundant code.,C,"The text lists ""Error handling code: seldom executed"" as one reason why entire program code is not always needed."
"According to the provided text, what is the definition of a ""virtual address space""?",The physical memory available on the system.,The cache memory used by the CPU.,A logical view of how a process is stored in memory.,The total amount of disk space allocated for paging.,The address space used by the operating system kernel only.,C,"The text and glossary define ""virtual address space"" as ""Logical view of how a process is stored in memory."""
What is the primary function of the Memory-Management Unit (MMU) in the context of virtual memory?,To manage the growth of the heap and stack.,To map logical pages to physical page frames.,To directly store program instructions.,To determine the size of the virtual address space.,To allocate physical memory contiguously for all processes.,B,"The text states, ""Memory-management unit (MMU): maps logical pages to physical page frames."""
"What best describes a ""sparse"" address space?",An address space where all memory is contiguous and fully utilized.,An address space that is smaller than the physical memory.,An address space with many holes or unused regions.,An address space reserved exclusively for system libraries.,An address space that only supports read-only access.,C,"The glossary defines ""sparse"" as ""an address space with many holes,"" and the text notes ""Sparse address spaces: virtual address spaces with holes."""
One benefit of sparse address spaces is the facilitation of what process?,Static linking of all libraries at compile time.,Reducing the total size of physical memory required for the system.,Dynamic linking of libraries/shared objects during execution.,Ensuring all parts of a program are loaded into memory simultaneously.,Preventing any form of memory sharing between processes.,C,"The text lists ""Dynamic linking of libraries/shared objects during execution"" as a benefit of sparse address spaces."
How does virtual memory enable processes to share system libraries like the standard C library?,By duplicating the library code for each process.,"By loading libraries into a dedicated, non-sharable memory region.","By mapping the libraries into the virtual address space of processes, often read-only, sharing physical pages.",By forcing processes to include the library code directly in their executable.,By making libraries accessible only to the operating system kernel.,C,"The text states, ""System libraries (e.g., standard C library) shared by mapping into virtual address space. Libraries mapped read-only, physical pages shared by processes."""
"How does virtual memory contribute to speeding up process creation, particularly with operations like `fork()`?",By requiring complete duplication of all memory pages.,By preventing memory sharing during creation.,By enabling pages to be shared between parent and child processes.,"By using a slower, more deliberate copying mechanism.",By eliminating the need for a memory management unit.,C,"The text states, ""Pages shared during process creation (fork()) \u2192 speeds up process creation."""
Which of the following is an accurate statement about the impact of virtual memory on system performance?,It always decreases CPU utilization because of increased overhead.,It can increase CPU utilization and throughput by allowing more programs to run concurrently.,It leads to an increase in response and turnaround time for programs.,It eliminates the need for context switching between processes.,It only benefits single-process systems.,B,"The text explicitly states, ""Less physical memory per program \u2192 more programs run concurrently \u2192 increased CPU utilization and throughput (no increase in response/turnaround time)."""
Virtual memory abstracts main memory into what kind of storage structure?,"A small, fixed-size cache.","A fragmented, discontinuous block.","A large, uniform storage array.","A temporary, volatile disk space.","A hardware-specific, non-abstracted region.",C,"The text says virtual memory ""Abstracts main memory into large, uniform storage array."""
What is a potential negative consequence if virtual memory is used carelessly?,It can lead to an increase in physical memory requirements.,It can simplify the overall system architecture.,It can decrease performance.,It can make process creation less efficient.,It will always lead to programs being smaller than physical memory.,C,"The text warns, ""Implementation complex, can decrease performance if used carelessly."""
Which virtual memory management technique does Linux primarily use?,Segmentation,Swapping,Demand paging,Static partitioning,Overlaying,C,"Linux manages virtual memory using demand paging, which brings pages into memory only when they are needed."
What kind of page-replacement policy does Linux implement?,Local LRU,FIFO,Optimal,Global LRU-approximation clock algorithm (second-chance),Most Recently Used (MRU),D,"Linux uses a global page-replacement policy similar to the LRU-approximation clock algorithm, also known as second-chance."
Linux maintains two page lists for memory management. What are they called?,Allocated and Deallocated,In-use and Free,Active and Inactive,Primary and Secondary,Resident and Swapped,C,Linux maintains an `active_list` for pages considered in use and an `inactive_list` for pages not recently referenced and eligible for reclamation.
"In Linux, what happens when a page in the `inactive_list` is referenced?",Its accessed bit is reset.,It is immediately reclaimed for the free list.,It moves back to the rear of the `active_list`.,It is written to secondary storage.,It remains in the `inactive_list` until reclaimed by kswapd.,C,"If a page in the `inactive_list` is referenced, it moves back to the rear of the `active_list`, indicating it is now in use."
Which daemon process in the Linux kernel is responsible for managing free memory and reclaiming pages?,pageservd,memmgrd,kswapd,pagedaemon,freememd,C,"The Linux kernel's page-out daemon process, `kswapd`, periodically awakens to check free memory and reclaims pages from the `inactive_list` if free memory falls below a threshold."
When does the Linux `kswapd` process typically scan the `inactive_list` to reclaim pages?,When the `active_list` becomes empty.,When a new process is created.,If free memory falls below a predetermined threshold.,"Every 5 seconds, regardless of memory status.",Only when the system is booting up.,C,`kswapd` scans the `inactive_list` and reclaims pages for the free list if free memory falls below a specific threshold.
What is the primary function of the `accessed` bit for pages in Linux's virtual memory management?,To indicate if a page is dirty and needs to be written to disk.,To track the total number of times a page has been referenced.,To determine if a page has been recently referenced for page replacement.,To signify that a page is currently being used by the kernel.,To mark a page as read-only.,C,The `accessed` bit is set when a page is referenced and is used by the LRU-approximation algorithm to determine which pages are least recently used and thus eligible for reclamation.
What is 'clustering' in the context of Windows' virtual memory management?,A technique to group processes for better CPU scheduling.,Bringing in the faulting page along with several immediately preceding/following pages during a page fault.,A method for encrypting memory pages.,Storing multiple small files on a single disk block.,A mechanism for remote memory access over a network.,B,Clustering in Windows handles page faults by bringing in the faulting page plus several immediately preceding and following pages to potentially reduce future page faults.
"According to the text, what is the default virtual address space for a 32-bit Windows system?",4 GB,8 TB,2 GB,128 TB,24 TB,C,"For 32-bit systems, Windows 10 has a default 2 GB virtual address space, extendable to 3 GB."
What is the typical cluster size for a data page fault in Windows?,1 page,2 pages,3 pages,5 pages,7 pages,C,"For data page faults, Windows clustering brings in 3 pages: the faulting page, one immediately before, and one immediately after."
"In Windows, what does the 'working-set minimum' define?",The maximum number of pages a process can ever use.,The minimum number of pages guaranteed to a process in memory.,The initial number of pages allocated to a process at creation.,The smallest possible size of a virtual page.,The threshold for initiating automatic working-set trimming.,B,The 'working-set minimum' is the minimum number of frames (pages) guaranteed to a process in memory.
What is the default 'working-set maximum' assigned to a process upon creation in Windows?,50 pages,100 pages,256 pages,345 pages,512 pages,D,"Upon process creation, Windows assigns a default 'working-set maximum' of 345 pages."
What is the purpose of 'automatic working-set trimming' in Windows?,To increase the working-set size of active processes.,To decrease working-set frames for processes if a minimum free memory threshold is reached.,To periodically write all process pages to disk.,To ensure all processes reach their working-set maximum.,To optimize the initial allocation of memory to new processes.,B,Automatic working-set trimming is a global replacement tactic in Windows that decreases working-set frames for processes if the free memory falls below a threshold.
When does Windows perform a local LRU page replacement policy?,Only when the system boots up.,When a process is at its working-set maximum and there is insufficient free memory.,When a process is below its working-set minimum.,When `automatic working-set trimming` is active.,When the `hard working-set limits` are ignored.,B,"If a process is at its working-set maximum and there is insufficient free memory, the kernel selects a page from that process's working set for replacement using a local LRU policy."
Which of the following virtual memory features is NOT explicitly mentioned as implemented by Windows?,Demand paging,Copy-on-write,Memory compression,"First-In, First-Out (FIFO) page replacement",Shared libraries,D,"Windows implements demand paging, copy-on-write, paging, memory compression, and shared libraries. Its page replacement is an LRU-approximation clock algorithm (second-chance), not FIFO."
"In Solaris, what is the role of the `lotsfree` parameter?",It defines the maximum amount of physical memory available.,It is the threshold for Solaris to begin paging activities.,It specifies the number of pages to scan per second.,It determines the desired free memory level.,It represents the number of pages reclaimed by `kswapd`.,B,"`lotsfree` is a parameter in Solaris that serves as a threshold; if free pages fall below this value, the pageout process starts."
How often does the Solaris kernel check free memory against the `lotsfree` parameter?,Once per second,Twice per second,Four times per second,Every 30 seconds,Only when a page fault occurs,C,The Solaris kernel checks free memory against `lotsfree` four times per second.
Describe the 'two hands' mechanism used by the Solaris pageout process.,"One hand for reading from disk, one for writing to disk.","One hand for user pages, one for kernel pages.",A front hand to reset reference bits and a back hand to check them and reclaim pages.,"One hand for active pages, one for inactive pages.",Two distinct processes that simultaneously scan memory.,C,"The Solaris pageout process uses two hands: a front hand scans pages setting reference bits to 0, and a back hand checks those bits, reclaiming pages if they are still 0."
"In Solaris, what is `scanrate`?",The frequency at which the pageout process awakens.,The speed at which data is transferred to secondary storage.,The number of pages scanned per second by the pageout process.,The rate at which processes are swapped out of memory.,The time taken to clear and check a page's reference bit.,C,"`scanrate` in Solaris refers to the number of pages per second scanned by the pageout process, ranging from `slowscan` to `fastscan`."
What action does the Solaris kernel take if it cannot maintain `desfree` memory for a 30-second average?,It increases the `scanrate` to its maximum.,It calls the pageout process for every new page request.,"It starts to swap out idle processes, freeing all their pages.",It stops all new process creation.,It reconfigures the `lotsfree` parameter.,C,"If Solaris is unable to maintain `desfree` memory for a 30-second average, the kernel swaps processes, specifically looking for idle ones, to free their pages."
What is 'priority paging' in Solaris?,Giving higher priority to kernel pages over user pages.,"Prioritizing selection of victim frames based on criteria, such as avoiding shared library pages.",A method to quickly page out low-priority processes.,Ensuring critical system processes always reside in physical memory.,Assigning different page replacement algorithms based on process priority.,B,"Priority paging is when Solaris prioritizes the selection of victim frames based on certain criteria, such as explicitly skipping shared library pages even if they are eligible for reclamation."
"In Solaris, what happens if free memory falls below `minfree`?",The system halts to prevent data corruption.,The pageout process is called for every new page request.,The `scanrate` is automatically set to `slowscan`.,Only shared library pages are reclaimed.,The `lotsfree` parameter is adjusted dynamically.,B,"If the system is unable to maintain `minfree`, the Solaris pageout process is called for every new page request, indicating an extreme memory shortage."
Which of the following is NOT a feature or mechanism mentioned as part of Solaris's memory management?,Two-hand pageout process,`lotsfree` parameter,`kswapd` daemon,Priority paging,`desfree` parameter,C,"`kswapd` is the page-out daemon process specific to Linux, not Solaris. Solaris uses its own 'pageout process'."
What is the definition of 'clustering' as provided in the text?,A method for grouping CPUs to handle memory tasks.,Paging in a group of contiguous pages when a single page is requested via a page fault.,The act of collecting fragmented memory pages.,A system's ability to run multiple virtual machines.,Organizing processes into distinct memory regions.,B,Clustering is defined as 'Paging in a group of contiguous pages when a single page is requested via a page fault'.
"According to the provided definitions, what does 'working-set maximum' refer to?",The minimum number of frames guaranteed to a process in Windows.,The total physical memory available on the system.,The maximum number of frames allowed to a process in Windows.,The number of frames that are actively being used by the kernel.,The upper limit of virtual address space.,C,The 'working-set maximum' is defined as the 'Maximum number of frames allowed to a process in Windows'.
What is the meaning of 'hard working-set limit' in Windows?,A soft suggestion for memory usage that can always be exceeded.,The minimum guaranteed memory for a process.,The maximum amount of physical memory a process is allowed to use.,The total size of the swap file.,"A limit that can only be set by the operating system, not by the user.",C,The 'hard working-set limit' is defined as the 'Maximum amount of physical memory a process is allowed to use in Windows'.
What is the definition of 'priority paging' from the provided text?,Assigning a priority level to each page in memory.,"Prioritizing selection of victim frames based on criteria, e.g., avoiding shared library pages.",A mechanism to give preference to paging in critical system pages.,The process of quickly freeing memory for high-priority applications.,The ability to dynamically adjust page sizes based on priority.,B,"Priority paging is defined as 'Prioritizing selection of victim frames based on criteria, e.g., avoiding shared library pages'."
"In Linux, if the `active_list` grows larger than the `inactive_list`, what action is taken regarding pages from the `active_list`?",They are immediately written to swap space.,They are moved from the front of the `active_list` to the rear of the `inactive_list`.,Their accessed bits are permanently set to 1.,They are marked as read-only.,They are deallocated and added to the free frames list.,B,"If the `active_list` grows larger than the `inactive_list`, pages from the front (least recently used) of the `active_list` move to the `inactive_list`, making them eligible for reclamation."
Which of the following best describes virtual memory?,A contiguous block of physical RAM reserved for the operating system.,A technique to increase CPU clock speed by reducing memory access times.,An abstraction of physical memory into an extremely large uniform array of storage.,A specialized cache for frequently accessed data that is faster than RAM.,A method to manage network bandwidth and optimize data transfer.,C,"Virtual memory abstracts physical memory into an extremely large uniform array of storage, making it appear larger and uniform to processes."
One significant benefit of virtual memory is that it allows a program to:,Execute faster than if it were entirely in physical memory due to caching.,Be larger than the available physical memory.,Directly access hardware registers without operating system intervention.,Avoid the need for any form of secondary storage like hard drives.,Communicate directly with other processes using shared CPU registers.,B,Virtual memory enables programs to be larger than physical memory by loading only parts of the program into RAM as needed.
Which of the following is NOT listed as a direct benefit of virtual memory?,Programs do not need to be entirely in memory to execute.,Processes can share memory regions more efficiently.,Processes can be created more efficiently.,It eliminates the need for any form of memory swapping or paging to disk.,It allows a program to be larger than the physical memory available.,D,"The text states that virtual memory enables programs to be larger than physical memory, not entirely in memory, allows memory sharing, and makes process creation more efficient. It does not eliminate swapping; in fact, it relies on it."
What is the primary characteristic of 'demand paging'?,All pages of a program are loaded into memory at program start.,Pages are preloaded into memory based on anticipated future access patterns.,Pages are loaded into memory only when they are referenced or 'demanded' during program execution.,Pages are compressed before being loaded into memory to save space.,Pages are always written back to the backing store immediately after modification.,C,Demand paging ensures that pages are loaded into physical memory only when they are actively needed ('demanded') during program execution.
"Based on the concept of demand paging, what happens to pages that are never 'demanded' during program execution?",They are loaded into a special cache for later use.,They are immediately written to the backing store to free up space.,They are never loaded into physical memory.,They are marked for prefetching in a subsequent execution.,They are compressed and stored in a kernel-only region of memory.,C,A direct consequence of demand paging is that pages that are never referenced are never loaded into physical memory.
A 'page fault' occurs when:,A page is successfully written from physical memory to the backing store.,An attempt is made to access a page that is not currently present in physical memory.,Two processes try to write to the same memory page simultaneously.,The operating system detects an unrecoverable error in a page's data.,A page is successfully moved from the CPU cache to main memory.,B,A page fault is triggered when a process attempts to access a virtual memory page that is not currently loaded into physical memory.
What action typically follows a page fault?,The operating system terminates the process immediately.,The process is paused indefinitely until more physical memory becomes available.,The requested page must be brought from the backing store into an available page frame in physical memory.,The system initiates a full memory scan to diagnose the cause of the fault.,The page is marked as 'read-only' to prevent further issues.,C,"Upon a page fault, the necessary page must be fetched from secondary storage (backing store) and loaded into an empty page frame in RAM."
What is the primary characteristic of 'copy-on-write' in the context of process creation?,All pages are copied from the parent to the child process at the moment of creation.,"The child process receives a read-only copy of the parent's memory, which is never modified.","The child process initially shares the same address space as the parent, with copies made only upon modification.","Both parent and child processes immediately get independent, fully duplicated memory spaces.",Memory is copied only when the parent process terminates or exits.,C,Copy-on-write means that a child process initially shares the parent's memory pages. A copy of a page is only created if either the parent or child attempts to modify it.
"Under a copy-on-write mechanism, when is a copy of a shared page made?",Only when the child process is initially created.,When either the child or parent process attempts to modify the shared page.,Only when the parent process modifies the page.,Only when the child process modifies the page.,When the system memory becomes critically low.,B,"A copy of a page is made if either the child process or the parent process modifies that page, ensuring isolation without immediate full duplication."
A page-replacement algorithm is typically invoked when:,A new process is created and needs its initial memory allocation.,Available physical memory is low and a new page needs to be loaded from the backing store.,A process voluntarily releases its allocated memory regions.,The CPU cache becomes full and needs to be flushed.,The system is booting up and initializing its memory structures.,B,"When available memory is low, a page-replacement algorithm is used to select an existing page to be evicted from memory to make room for a new one."
Which of the following is explicitly listed as a page-replacement algorithm in the provided text?,LFU (Least Frequently Used),MRU (Most Recently Used),Optimal,Random,Segmented FIFO,C,"The text lists FIFO, optimal, and LRU as page-replacement algorithms."
Why is a 'pure LRU' page-replacement algorithm often impractical to implement in real systems?,It requires too much CPU time for calculations on every memory access.,It necessitates complex hardware support to accurately track the exact usage time for every page.,It inherently leads to an excessive number of page faults compared to other algorithms.,It is difficult to define 'least recently used' precisely across all processes.,It cannot be combined with other memory management techniques like demand paging.,B,"The text states that pure LRU is impractical to implement, implying the difficulty in accurately tracking precise usage times, leading most systems to use LRU-approximation algorithms."
A 'global page-replacement algorithm' selects a page for replacement from:,Only the pages belonging to the faulting process.,"A fixed set of system-wide shared pages, ignoring process-specific ones.",Any process currently loaded in the system's memory.,Only pages that have not been modified since being loaded.,Pages belonging to processes with the lowest priority or least activity.,C,"Global page-replacement algorithms are defined as selecting a page from any process for replacement, not just the faulting one."
"In contrast to a global algorithm, a 'local page-replacement algorithm' selects a page for replacement from:",Only pages that are currently in the CPU cache.,Only pages belonging to the faulting process.,"Any page in memory, regardless of its owning process.",Pages that are least frequently accessed system-wide.,"Pages within a specific, pre-defined memory region reserved for the OS.",B,Local page-replacement algorithms are defined as selecting a page from the faulting process itself.
'Thrashing' refers to a state where a system:,"Spends an excessive amount of time executing user-mode processes, neglecting system tasks.","Is unable to allocate any more physical memory, leading to system crash.",Spends more time paging (swapping pages in and out of memory) than executing useful work.,"Experiences frequent CPU context switches, but without significant performance impact.","Has its entire working set loaded into memory, leading to optimal performance.",C,"Thrashing is a severe performance degradation where the system is spending a disproportionate amount of time moving pages between RAM and disk, rather than performing actual computation."
"In the context of virtual memory, what does 'locality' refer to?",The physical location of memory modules on the motherboard.,A set of pages that are actively used together by a process.,The geographical location of data centers for distributed memory systems.,The closest available page frame for allocation when a page fault occurs.,A measure of how frequently a specific memory address is accessed.,B,"Locality is defined as a set of pages actively used together, often referring to spatial or temporal locality of reference."
How does process execution typically relate to 'locality'?,Processes remain strictly within a single locality throughout their entire execution.,Processes continuously move randomly across all available memory pages without structure.,Process execution typically moves from one locality to another as different functions or data are accessed.,"Locality only applies to kernel processes, not user-mode processes.",Locality prevents processes from ever needing to page to disk.,C,"The text states that process execution moves from locality to locality, reflecting changing working sets over time."
The 'working set' of a process is defined as:,The total number of pages allocated to a process since its creation.,"The set of pages currently in active use by a process, based on the concept of locality.","The set of all executable code pages belonging to a process, excluding data pages.",The maximum number of pages a process is allowed to hold in memory at any given time.,The set of pages that have been recently written to disk as part of a checkpointing process.,B,The working set is based on locality and represents the set of pages currently in use by a process.
What is 'memory compression' in the context of virtual memory?,A technique that reduces the physical size of RAM modules for compact devices.,A method of compressing a number of individual pages into a single page frame in memory.,An algorithm designed to increase the speed of memory access by reducing latency.,A way to store data redundantly across multiple pages for fault tolerance.,A security measure to encrypt memory contents to prevent unauthorized access.,B,Memory compression involves compressing multiple logical pages of data into a single physical page frame.
"Memory compression is noted as an alternative to paging, primarily used on which type of systems?",High-performance computing clusters that require extreme speed.,Enterprise server systems managing large databases.,Mobile systems that often lack full paging support.,Desktop workstations with large amounts of RAM.,Virtual machines running in cloud environments.,C,"Memory compression is used as an alternative to paging, particularly on mobile systems without full paging support."
How is kernel memory allocation described as being different from user-mode process allocation?,"Kernel memory is allocated in non-contiguous chunks, unlike user memory.","Kernel memory is allocated dynamically based on demand, while user memory is always pre-allocated.",Kernel memory is allocated in contiguous chunks of varying sizes.,"Kernel memory uses a FIFO allocation scheme, while user memory uses LRU.",Kernel memory is always much larger in total size than user memory.,C,"Kernel memory is allocated differently than user-mode processes, specifically in contiguous chunks of varying sizes."
Which of the following are mentioned as common techniques for kernel memory allocation?,LIFO and LRU.,Paging and Swapping.,Buddy system and Slab allocation.,First-fit and Best-fit.,Segmented memory and Pure demand paging.,C,The text explicitly lists the Buddy system and Slab allocation as common techniques for kernel memory allocation.
What does 'TLB reach' refer to in the context of memory management?,The physical distance between the CPU and main memory.,The speed at which the Translation Lookaside Buffer (TLB) can translate virtual addresses.,The total amount of memory that can be directly accessed and mapped by the TLB at any given time.,The maximum number of entries the TLB can hold.,The number of active processes that can simultaneously utilize the TLB.,C,TLB reach is defined as the amount of memory accessible from the Translation Lookaside Buffer (TLB).
How is TLB reach calculated?,Number of TLB entries divided by the system's smallest page size.,Total physical memory size multiplied by the system's largest page size.,Number of entries in the TLB multiplied by the page size.,CPU clock speed multiplied by the TLB access time.,Number of active processes multiplied by the number of TLB entries.,C,TLB reach is calculated as the number of entries in the TLB multiplied by the page size.
A common technique to increase TLB reach is to:,Decrease the number of entries in the TLB.,Use smaller page sizes for memory allocation.,Increase the page size used by the system.,Allocate significantly more physical memory (RAM).,Increase the frequency of TLB flushes.,C,The text states that a technique to increase TLB reach is to increase the page size.
"Which of the following virtual memory management techniques are commonly used by Linux, Windows, and Solaris?","Pure LRU, LIFO, and segmented memory allocation.","Demand paging, copy-on-write, and variations of LRU approximation (like the clock algorithm).","Buddy system, Slab allocation, and optimal page replacement.","Memory compression, pre-paging, and exact LRU implementations.","Strictly global page replacement, local page replacement, and pure FIFO.",B,"Linux, Windows, and Solaris are noted to manage virtual memory similarly, using demand paging, copy-on-write, and variations of LRU approximation (e.g., clock algorithm)."
What is the primary characteristic of demand paging?,Loading the entire program into physical memory before execution.,Loading program pages into memory only when they are needed during execution.,Storing all program pages permanently in physical memory.,Executing programs directly from secondary storage without loading them into RAM.,Prioritizing the loading of kernel pages over user program pages.,B,"Demand paging specifically loads pages into memory only when they are accessed or 'demanded' by the running program, rather than loading the entire program at once."
Which problem is primarily addressed by the use of demand paging?,Insufficient CPU processing speed for large programs.,"The need to load an entire program into physical memory, even if not all parts are immediately required.",Difficulty in managing multiple processes simultaneously.,The high cost of secondary storage devices.,Lack of proper security mechanisms for memory access.,B,Demand paging solves the problem of needing to load an entire program (which may include unneeded parts like unselected options) by only loading portions as they are accessed.
"In a system utilizing demand paging, what happens to pages that are never accessed during a program's execution?",They are loaded into a special cache for future use.,They remain in secondary storage and are never loaded into physical memory.,They are loaded into physical memory at program startup but later swapped out.,They cause a page fault upon program termination.,They are marked as invalid and immediately deleted from secondary storage.,B,"A key benefit of demand paging is that unaccessed pages are never loaded into physical memory, saving memory resources."
What hardware support is essential for distinguishing between pages that are in memory and those that are in secondary storage in a demand paging system?,A dedicated memory management unit (MMU) with a translation lookaside buffer (TLB).,A valid-invalid bit scheme in the page table.,High-speed solid-state drives for secondary storage.,Multiple CPU cores for parallel processing.,A large amount of RAM.,B,The valid-invalid bit in each page-table entry is used by hardware to indicate whether a page is legal and in memory (valid) or not (invalid).
"In the context of demand paging's valid-invalid bit scheme, what does a ""valid bit"" typically signify?",The page is corrupted and needs to be reloaded.,The page is legal and currently resides in physical memory.,The page is protected and cannot be modified.,The page is in secondary storage and needs to be swapped in.,The page is part of the operating system kernel.,B,A valid bit indicates that the corresponding page is both part of the process's logical address space and is currently loaded into a physical memory frame.
"What does an ""invalid bit"" in a page-table entry signify in a demand paging system?",The page contains invalid data and should be discarded.,"The page is legal and in memory, but read-only.","The page is either not a valid part of the logical address space, or it is valid but currently in secondary storage.",The page has been recently accessed and is likely to be accessed again.,The page is exclusively reserved for operating system use.,C,"An invalid bit indicates one of two possibilities: either the page address is illegal (not in the process's logical address space), or it is legal but currently resides in secondary storage."
"What immediately triggers a ""page fault"" in a demand paging system?",A process attempting to write to a read-only page.,The operating system deciding to swap out a less-used page.,An access attempt to a page marked as invalid in the page table.,The CPU requesting an instruction that is already in memory.,A system administrator manually initiating a page swap.,C,A page fault is explicitly defined as occurring when an access attempt is made to a page whose corresponding page-table entry is marked invalid.
What is the first action taken by the system immediately after a page fault occurs?,The process is immediately terminated.,The requested page is read from secondary storage.,A trap to the operating system is generated.,The page table is updated to mark the page as valid.,A new free frame is allocated.,C,"The text states, ""Access to invalid page → page fault. Page fault causes trap to OS."" This is the immediate consequence before any handling procedure begins."
"According to the provided text, what is the first step the operating system performs when handling a page fault?",Find a free frame in physical memory.,Schedule a secondary storage operation to read the page.,Check an internal table (like the process control block) to validate the memory access.,Modify the page table to mark the page as valid.,Restart the interrupted instruction.,C,"The first step in the page fault handling procedure is ""Check internal table (process control block) for valid/invalid memory access."""
"During the page fault handling procedure, if the OS determines that the memory access causing the fault is invalid (e.g., beyond the process's logical address space), what action is taken?",The page is loaded into memory regardless.,The process is terminated.,The page is moved to swap space.,The page table is updated with a valid bit.,"An error log is created, and the instruction is retried.",B,"Step 2 of the page fault handling procedure states: ""If invalid, terminate process."""
What is the final step in the page fault handling procedure as described in the text?,The process control block is updated.,The operating system finds a free frame.,The page is read from secondary storage.,The interrupted instruction is restarted.,The page is marked as invalid in the page table.,D,"The last step listed in the procedure is ""Restart interrupted instruction; process accesses page as if always in memory."""
"What defines ""pure demand paging""?",All pages are pre-loaded into memory before execution begins.,No pages are ever swapped out once loaded into memory.,"The process starts with no pages in memory, and pages are faulted in as needed.",Only read-only pages are loaded into memory on demand.,"Memory pages are only loaded from the file system, not swap space.",C,"Pure demand paging is defined as starting a process with no pages in memory, and then faulting for pages only as they are referenced."
"How does the ""locality of reference"" principle relate to the performance of demand paging?",It guarantees that all pages will be loaded into memory at startup.,"It increases the probability of page faults, thus degrading performance.","It suggests that processes access memory in patterns, which allows demand paging to perform reasonably well.","It indicates that memory access times are constant, regardless of paging.",It is irrelevant to the efficiency of demand paging.,C,"The text states, ""Programs tend to have locality of reference → reasonable demand paging performance,"" meaning the non-random access patterns make demand paging viable."
"What is the primary role of ""secondary memory"" (or swap device/swap space) in a demand paging system?",To store the entire operating system kernel.,To serve as a temporary cache for frequently accessed pages.,To hold pages that are not currently resident in main memory.,To execute programs directly without loading them into RAM.,To maintain a log of all page faults.,C,"Secondary memory, specifically swap space, is explicitly described as holding non-main-memory pages."
A crucial requirement for the successful implementation of demand paging is the ability to:,Pre-allocate all required memory frames at compile time.,Completely ignore the state of a process after a page fault.,Restart any instruction after a page fault has been handled.,Always keep the entire program in main memory.,Prevent any process from accessing pages in secondary storage.,C,"The text highlights this as a ""Crucial requirement: ability to restart any instruction after page fault,"" which involves saving and restoring process state."
"For instructions that modify multiple memory locations (like IBM System 360/370 MVC), how can a page fault be handled to ensure correct process state restoration?","The entire instruction is always aborted, and the process is restarted from scratch.",The instruction is immediately retried without any state restoration.,"The system uses microcode to check both ends of blocks before modification, or uses temporary registers to restore overwritten values.",Such instructions are prohibited in demand-paged systems.,The CPU automatically re-executes only the failed part of the instruction.,C,The text provides two solutions for this difficulty: microcode checks before modification or using temporary registers to restore old values on fault.
"From the perspective of the running process, how should paging be perceived?",It should be explicitly managed by the process's code.,It should be visible and require process intervention for page swaps.,"It should be transparent, meaning the process is unaware of it.",It should cause noticeable delays and require user interaction.,It should only occur during process startup.,C,"The text states, ""Paging should be transparent to process."""
"What is the primary purpose of the ""free-frame list"" maintained by the operating system?",To track all pages currently loaded into memory.,To store information about processes terminated due to page faults.,To provide a pool of available physical memory frames for allocation to processes.,To list all pages currently residing in secondary storage.,To manage the cache memory hierarchy.,C,"The free-frame list is described as a ""pool of free frames for page faults,"" and also for stack/heap segment expansion."
"What is the purpose of ""zero-fill-on-demand"" in the context of allocating free frames?",To fill unused frames with random data for testing purposes.,To prevent memory fragmentation.,To ensure security by clearing previous data from a frame before allocating it to a new process.,To optimize memory access by pre-fetching data.,To count the number of available frames.,C,"Zero-fill-on-demand is described as ""frames 'zeroed-out' before allocation (security)."""
"What does ""effective access time"" refer to in the context of demand-paged memory performance?",The time it takes for the CPU to access its internal registers.,"The average time required to access a memory location, considering both hits and page faults.",The maximum possible time for a memory access in the worst-case scenario.,The time it takes to transfer data between the CPU and cache.,The time it takes for a page to be written back to secondary storage.,B,"Effective access time is defined as the measured/calculated time to access something, and the formula explicitly combines memory access time with page fault time based on the probability of a fault."
Which of the following is NOT listed as a component of page fault service time?,Servicing the page-fault interrupt.,Reading the page into memory from secondary storage.,Restarting the interrupted process.,Compressing the page data before writing to swap space.,All listed are components.,D,"The three listed components are servicing the interrupt, reading the page, and restarting the process. Compressing data is not mentioned as a direct component of service time."
"Approximately how long does a typical HDD page-switch operation (including latency and seek time) take, as stated in the text?",10 nanoseconds,100 microseconds,8 milliseconds,3 seconds,200 nanoseconds,C,"The text explicitly states, ""HDD page-switch time: ~8 milliseconds (3ms latency, 5ms seek, 0.05ms transfer)."""
How does the page-fault rate (p) affect the effective access time in a demand paging system?,It is inversely proportional to the effective access time.,It has no significant impact on effective access time.,It is directly proportional to the effective access time.,It only affects the seek time of the hard disk.,It reduces the memory access time (ma).,C,"The formula and subsequent example clearly show that as 'p' increases, the effective access time also increases, making it directly proportional."
"To maintain acceptable performance in a demand paging system (e.g., less than 10% slowdown with an 'ma' of 200ns and page fault time of 8ms), what must be true about the page-fault rate (p)?",p must be exactly 1.,p must be greater than 0.1.,"p must be very low, specifically less than 0.0000025.",p should be close to 1/1000.,p should be around 8 milliseconds.,C,"The example calculation explicitly states that for a slowdown less than 10% (220ns effective access time), ""p < 0.0000025 (fewer than 1 fault per 399,990 accesses)."""
"Generally, how does I/O to swap space compare to I/O to the file system?",Swap space I/O is slower due to complex file system structures.,Swap space I/O is faster because it often involves larger blocks and no file lookups.,Both have identical performance characteristics.,File system I/O is always preferred for paging operations.,"Swap space is only used for caching, not for actual paging.",B,"The text states, ""Swap space I/O generally faster than file system I/O (larger blocks, no file lookups)."""
"In some operating systems like Linux and BSD UNIX, where are binary executables demand-paged from, and what acts as their backing store?","They are copied entirely to swap space at startup, which acts as the backing store.","They are demand-paged directly from the file system, which acts as their backing store.",They are kept entirely in RAM and never paged out.,They are always generated on-the-fly from source code.,They use anonymous memory backed by swap space.,B,"The text states: ""Demand-page binary executables directly from file system; overwrite frames when replaced (never modified); file system acts as backing store (Linux, BSD UNIX)."""
For what type of memory is swap space typically always used as a backing store when pages are modified and paged out?,Read-only code segments of binary executables.,Shared library pages.,"Anonymous memory, such as stack and heap.",Pages that have been compressed.,Files mapped into memory by multiple processes.,C,"The text states, ""Anonymous memory (stack, heap) still uses swap space."" The glossary also defines anonymous memory as ""Memory not associated with a file; stored in swap space if dirty and paged out."""
How do mobile operating systems like iOS typically handle memory management without traditional swapping?,They pre-load all necessary pages into memory.,They demand-page from the file system and reclaim read-only pages if memory is constrained.,They primarily use compressed memory as their only form of virtual memory.,They store all anonymous memory in non-volatile storage.,They rely solely on larger physical memory to avoid paging altogether.,B,"The text states: ""Mobile OS (e.g., iOS) typically no swapping: demand-page from file system, reclaim read-only pages if memory constrained."""
What alternative memory management technique is mentioned as being used in mobile systems in place of traditional swapping?,Memory pooling.,Compressed memory.,Static memory allocation.,Direct memory access (DMA).,Disk caching.,B,"The text explicitly mentions, ""Compressed memory is an alternative to swapping in mobile systems."""
"According to the glossary, what is ""swap space""?",The portion of RAM used for caching frequently accessed data.,Secondary storage backing-store space for paged-out memory.,A temporary directory for downloaded files.,The area in physical memory where the operating system kernel resides.,A list of available free memory frames.,B,"The glossary defines swap space as ""Secondary storage backing-store space for paged-out memory."""
"What does ""page-fault rate"" measure?",The speed at which pages can be loaded into memory.,The total number of page faults that have occurred.,How often a page fault occurs per memory access attempt.,The amount of memory occupied by invalid pages.,The average time taken to service a page fault.,C,"The glossary defines page-fault rate as ""Measure of how often a page fault occurs per memory access attempt."""
"In the context of memory management, what is ""anonymous memory""?",Memory that is shared between multiple processes.,"Memory not associated with a file, typically stored in swap space if paged out and dirty.",Memory segments containing only operating system code.,Memory used for storing file system metadata.,Memory that has been encrypted for security purposes.,B,"The glossary defines anonymous memory as ""Memory not associated with a file; stored in swap space if dirty and paged out."""
"At system startup, what is the initial state of the free-frame list?","It is empty, as all memory is initially reserved by the kernel.",It contains all available physical memory on the list.,It contains only frames allocated for the stack and heap.,It is populated only after the first page fault occurs.,It stores only frames that have been zero-filled.,B,"The text states: ""System startup: all available memory on free-frame list."""
"Beyond simply loading pages as needed, what general benefit does demand paging provide?",It eliminates the need for any secondary storage.,It ensures faster CPU clock speeds.,It enables more efficient memory use by loading only needed portions.,It prevents all forms of memory fragmentation.,It makes processes completely immune to performance slowdowns.,C,"One of the initial benefits listed is ""More efficient memory use by loading only needed portions."""
Demand paging is noted to be similar to which other memory management technique?,Segmentation.,Contiguous memory allocation.,Paging with swapping.,Static memory allocation.,Memory compression.,C,"The text mentions, ""Similar to paging with swapping."""
"In demand paging, what exactly constitutes a ""page fault""?",An error caused by trying to access a non-existent memory address.,A successful retrieval of data from the CPU cache.,A reference to a page that is currently not resident in physical memory.,An attempt to modify a read-only page.,The completion of a page write operation to secondary storage.,C,"The glossary defines page fault as ""Fault from reference to a non-memory-resident page."""
What is a primary objective of using copy-on-write (COW) during process creation with `fork()`?,"To ensure the child process always receives a complete, independent copy of the parent's address space.",To maximize the allocation of new pages to the child process immediately after creation.,To minimize the number of new pages allocated to the child process initially.,To prevent the child process from ever modifying any data inherited from the parent.,To bypass demand paging for all pages in the child process.,C,Copy-on-write (COW) is a technique that minimizes new pages allocated to the child process by initially sharing pages with the parent.
How does the copy-on-write (COW) mechanism fundamentally alter the traditional `fork()` behavior?,"Traditionally, `fork()` used a page-sharing mechanism similar to COW, whereas COW copies all pages.","Traditional `fork()` immediately copied the parent's entire address space, while COW initially shares pages.","COW ensures all parent pages are duplicated, while traditional `fork()` only duplicates modified pages.","Traditional `fork()` suspended the parent, which COW does not do.",COW is less efficient than traditional `fork()` for process creation.,B,"Traditionally, `fork()` copied the parent's entire address space for the child. With copy-on-write, the parent and child processes initially share the same pages."
"According to the copy-on-write (COW) technique, what action triggers the copying of a shared page?",When the child process terminates.,When the parent process resumes from suspension.,When either the parent or child process attempts to write to a shared page.,When the operating system runs out of free memory frames.,When an `exec()` call is made by the child process.,C,"If either the parent or child process writes to a shared page that is marked as copy-on-write, a copy of that shared page is created."
Which statement accurately describes how pages are handled under the copy-on-write (COW) mechanism?,"All pages, including executable code, are always copied to the child process.",Only stack pages are initially shared; all other pages are immediately copied.,"Unmodified pages, such as executable code, continue to be shared between parent and child.",Both modified and unmodified pages are always copied to ensure process isolation.,Pages are only copied if the child process modifies them and never if the parent modifies them.,C,"Only modified pages are copied under COW; unmodified pages (e.g., executable code) can continue to be shared between the parent and child processes."
"What is the primary benefit of using copy-on-write (COW) in the context of process creation, especially when the child process might immediately call `exec()`?",It ensures the child process has a completely independent address space from the start.,It increases the complexity of memory management for the operating system.,"It avoids unnecessary copying of memory pages, which might be discarded by `exec()`.",It allows the parent process to continue execution without suspension.,It prioritizes disk I/O over memory operations.,C,"Copying pages may be unnecessary if the child immediately calls `exec()`, as those copied pages would simply be overwritten. COW avoids this wasteful copying."
In which of the following operating systems is the copy-on-write (COW) technique commonly employed for process creation?,MS-DOS,"Windows, Linux, and macOS",Early versions of UNIX only,Only specialized real-time operating systems,Only systems that do not support virtual memory.,B,"Copy-on-write is a common technique used in modern operating systems such as Windows, Linux, and macOS."
What is a defining characteristic of the `vfork()` system call concerning the parent and child processes' address spaces?,The child process receives a completely independent copy of the parent's address space.,The child process uses its own unique address space while the parent's is suspended.,The child process shares the parent's address space directly for read and write operations.,"The child process initially shares pages, but a copy-on-write mechanism is used for modifications.","The parent and child processes share only read-only pages, with separate writable pages.",C,"With `vfork()`, the child process uses the parent's address space directly for read/write operations."
What is a fundamental difference in memory management between `vfork()` and `fork()` using copy-on-write?,"`vfork()` explicitly uses copy-on-write, whereas `fork()` does not.","`vfork()` does not use copy-on-write, and child modifications are visible to the parent, unlike `fork()` with COW.","`fork()` always copies all pages, while `vfork()` only copies modified pages.","`vfork()` allocates new pages for the child process, while `fork()` shares them.","`vfork()` allows concurrent execution of parent and child, while `fork()` suspends the parent.",B,"`vfork()` does not use copy-on-write, which means any changes made by the child to the shared address space are visible to the parent upon resumption. `fork()` with COW, conversely, ensures changes are made to a private copy."
For what specific scenario is the `vfork()` system call primarily intended?,When the child process is expected to perform extensive data processing within the inherited address space.,When the child process needs to create many new threads.,When the child process is intended to immediately call `exec()`.,When the parent process requires concurrent execution with the child without suspension.,When robust memory isolation between parent and child is paramount.,C,"`vfork()` is intended for use when the child process calls `exec()` immediately after its creation, as this avoids unnecessary copying."
What is a critical caution associated with using `vfork()`?,It is significantly slower than `fork()` for process creation.,"The child process might inadvertently modify the parent's address space, with visible effects.",It automatically applies copy-on-write to all shared pages.,It prevents the child process from calling `exec()`.,It is not supported on modern UNIX-like operating systems.,B,"A critical caution when using `vfork()` is that the child must not modify the parent's address space, as any changes made by the child will be visible to the parent upon resumption."
How does `vfork()`'s efficiency compare to other process creation methods?,It is less efficient due to the overhead of parent suspension.,It is extremely efficient due to the absence of page copying.,Its efficiency is comparable to traditional `fork()` with full page copying.,It is only efficient for very small child processes.,Its efficiency is entirely dependent on the amount of data modified by the child.,B,"`vfork()` is described as extremely efficient for process creation because it involves no page copying, unlike traditional `fork()` or even `fork()` with COW when pages are written to."
What happens to the parent process when `vfork()` is called?,The parent process continues execution concurrently with the child.,The parent process is immediately terminated.,The parent process is suspended until the child process completes its execution or calls `exec()`.,"The parent process enters a waiting state, but its address space is duplicated.",The parent process relinquishes its address space to the child.,C,"When `vfork()` is called, the parent process is suspended, and the child process uses the parent's address space directly. The parent typically resumes after the child calls `exec()` or exits."
"According to the provided glossary, which statement best defines 'copy-on-write'?",A mechanism where data is copied then modified upon a write attempt to a shared page.,"A technique where all shared pages are copied immediately upon process creation, then modified.",A system call that allows a child process to share the parent's address space for read/write operations without copying.,An optimization that prevents any page from being copied under any circumstances.,A process creation method that always bypasses demand paging entirely.,A,"The glossary defines 'copy-on-write' as: 'Write causes data to be copied then modified; on shared page write, page copied, write to copy.'"
"Based on the section glossary, what is the correct definition of 'virtual memory fork'?",A system call that duplicates the parent's entire virtual memory space into the child's.,A technique that uses copy-on-write to share virtual memory pages between parent and child processes.,"The `vfork()` system call; child shares parent's address space for read/write, parent suspended.",A method for creating virtual machines that share physical memory.,A process creation method that ensures absolute memory isolation between parent and child.,C,"The glossary defines 'virtual memory fork' as: '`vfork()` system call; child shares parent's address space for read/write, parent suspended.'"
Which of the following is a primary benefit of demand paging in memory management?,It ensures all pages are loaded into memory simultaneously for faster access.,It reduces the degree of multiprogramming to prevent memory contention.,It saves I/O by loading only the pages that are actively used.,It eliminates the need for swap space on secondary storage.,It guarantees that no page faults will occur during process execution.,C,"Demand paging saves I/O by loading only used pages, rather than loading an entire process into memory at once."
What is 'over-allocating memory' in the context of demand paging?,Providing a process with more physical memory frames than it requests.,"Allocating more virtual memory than physical memory is available, increasing the degree of multiprogramming.",Reserving all available memory frames for a single process to ensure optimal performance.,"Loading all pages of a process into memory at startup, regardless of immediate need.",The process of deallocating memory frames that are no longer in use.,B,"Over-allocating memory means providing access to more virtual memory than physically available, which can increase the degree of multiprogramming by allowing more processes to run concurrently."
A situation where a page fault occurs but no free memory frames are available is a manifestation of which memory management issue?,Optimal page replacement,Belady's anomaly,Over-allocation of memory,Insufficient I/O buffer space,The use of a dirty bit,C,"Over-allocation manifests as a page fault with no free frames, indicating that the system has committed more virtual memory than it can currently support with physical frames."
"When a free frame is needed but none are available, what action is taken to free up a frame?",The system immediately terminates the process causing the page fault.,"The contents of a selected frame are written to swap space, and its page table entry is updated.",The operating system waits for the user to manually free up memory.,All other processes in memory are swapped out to create space.,The CPU utilization is drastically reduced to free memory.,B,"If no free frame is available, a page-replacement algorithm selects a victim frame. Its contents are written to swap space (if modified), and its page table entry is updated to indicate it's no longer in memory."
What is the purpose of a 'modify bit' (or 'dirty bit') in the context of page replacement?,To indicate that a page has been recently referenced by the CPU.,"To mark a page as read-only, preventing any writes.","To signify that a page's contents have been altered, requiring it to be written back to secondary storage before replacement.",To prioritize pages that are frequently used for replacement.,To identify pages that are part of the operating system kernel.,C,"The modify bit, set by hardware, indicates if a page has been written to. If it's set, the page must be written back to secondary storage before being replaced; if not, it can simply be discarded, significantly reducing page-fault service time."
Which of the following is NOT one of the two major problems for demand paging that algorithms aim to solve?,Which frames to replace (page-replacement algorithm).,How many frames to allocate to each process (frame-allocation algorithm).,How to convert logical addresses to physical addresses.,Minimizing the page-fault rate.,Selecting a victim frame.,C,"The two major problems for demand paging are the frame-allocation algorithm (how many frames) and the page-replacement algorithm (which frames). Converting logical to physical addresses is handled by the MMU, not a problem demand paging algorithms solve directly."
What is a 'reference string' primarily used for in the context of page replacement?,To identify the physical location of a page in memory.,To determine the optimal size of the swap space.,To trace the sequence of memory accesses for evaluating page-replacement algorithms.,To allocate frames to different processes dynamically.,To encrypt data before it is written to secondary storage.,C,"A reference string is a trace of memory accesses (specifically, page numbers) used to evaluate the performance of different page-replacement algorithms by simulating their behavior."
Which page replacement algorithm is known for its simplicity and for replacing the page that has been in memory the longest?,Least Recently Used (LRU),Optimal (OPT),"First-In, First-Out (FIFO)",Least Frequently Used (LFU),Second-Chance,C,"FIFO (First-In, First-Out) replaces the oldest page in memory, which is the first one that was brought in. It is easy to understand and program."
What is 'Belady's anomaly'?,The phenomenon where the page-fault rate decreases as the number of allocated frames increases.,A situation where a page replacement algorithm cannot find a victim frame.,The counter-intuitive result where the page-fault rate increases even when more memory frames are allocated to a process.,The inability of an algorithm to use a modify bit effectively.,The problem of having too many free frames in memory.,C,"Belady's anomaly is a specific issue where, for some page-replacement algorithms (like FIFO), increasing the number of available memory frames can paradoxically lead to an increase in the page-fault rate."
Which page replacement algorithm guarantees the lowest possible page-fault rate but is impossible to implement in practice for general-purpose computing?,Least Recently Used (LRU),"First-In, First-Out (FIFO)",Least Frequently Used (LFU),Optimal (OPT),Second-Chance,D,"The Optimal (OPT) page-replacement algorithm replaces the page that will not be used for the longest period of time, thus guaranteeing the lowest page-fault rate. However, it requires future knowledge of the reference string, making it impractical for real-world implementation."
Which page replacement algorithm approximates the Optimal algorithm by replacing the page that has not been used for the longest period of time?,"First-In, First-Out (FIFO)",Most Frequently Used (MFU),Least Recently Used (LRU),Second-Chance,Additional-reference-bits,C,"The Least Recently Used (LRU) algorithm approximates the Optimal algorithm by choosing the page that has not been accessed for the longest duration, based on past usage."
Why is true LRU implementation often expensive?,It requires frequent disk I/O operations for every page replacement.,It needs complex software to track page usage patterns.,It requires substantial hardware assistance and per-memory-reference updates.,"It suffers from Belady's anomaly, leading to high page fault rates.",It necessitates a very large amount of swap space.,C,"True LRU implementation, whether using counters or a stack, requires updating usage information (e.g., timestamps or stack reordering) on every memory reference, which is computationally and hardware-intensively expensive."
What is a 'stack algorithm' in the context of page replacement?,An algorithm that uses a stack data structure to store pages in memory.,A class of algorithms that suffer from Belady's anomaly.,An algorithm that prioritizes pages based on their stack depth in the program's call stack.,A class of page-replacement algorithms that do not suffer from Belady's anomaly.,An algorithm that must be implemented using a hardware-supported stack.,D,"A stack algorithm is a class of page-replacement algorithms that have the property that the set of pages in memory with N frames is always a subset of the pages in memory with N+1 frames. This property ensures they do not suffer from Belady's anomaly, and LRU is an example."
Which hardware component is crucial for LRU-approximation algorithms like the 'additional-reference-bits' algorithm?,A dedicated high-speed cache memory.,A direct memory access (DMA) controller.,A reference bit set by hardware on page access.,A sophisticated I/O buffer management unit.,A specialized cryptographic co-processor.,C,"LRU-approximation algorithms like the additional-reference-bits algorithm rely on a 'reference bit' (or 'use bit') that is automatically set by hardware whenever a page is referenced, providing a basic indication of recent usage."
"In the 'additional-reference-bits' algorithm, how is the history of page use primarily tracked?",By maintaining a precise timestamp of the last access for each page.,By storing page references in a FIFO queue.,By periodically shifting the reference bit into an 8-bit byte for each page.,By counting the total number of accesses for each page since system startup.,By using a doubly linked list to order pages by recency.,C,"The additional-reference-bits algorithm uses an 8-bit byte for each page. On a timer interrupt, the current reference bit is shifted into the most significant bit of this byte, and the other bits are shifted right, creating a history (shift register) of recent usage."
The 'second-chance page-replacement algorithm' is also known by what other name?,Optimal algorithm,Clock algorithm,Stack algorithm,LFU algorithm,MFU algorithm,B,The second-chance page-replacement algorithm is also commonly referred to as the 'clock' algorithm because of its implementation using a circular queue (like a clock face) with a pointer.
"In the 'second-chance algorithm', what happens if a selected page's reference bit is 1?",The page is immediately replaced.,The page is written to disk and then replaced.,"The reference bit is cleared, the arrival time is reset, and the page is given a 'second chance'.",The page is moved to the head of the queue for immediate replacement.,The system determines it's an error and halts.,C,"If a selected page's reference bit is 1, the second-chance algorithm clears the bit, resets the page's 'arrival time' to the current time, and gives it a 'second chance' by moving the pointer to the next page without replacing it immediately."
The 'enhanced second-chance algorithm' primarily uses which two bits to classify pages for replacement?,Valid bit and Present bit,Read bit and Write bit,Protection bit and Share bit,Reference bit and Modify bit,Access bit and Execute bit,D,"The enhanced second-chance algorithm considers the (reference bit, modify bit) pair to classify pages into four categories, allowing for a more informed replacement decision that prioritizes clean, unused pages."
"According to the 'enhanced second-chance algorithm', which class of pages is considered the 'best to replace' due to minimal overhead?","(0, 1) - not recently used but modified","(1, 0) - recently used but clean","(0, 0) - neither recently used nor modified","(1, 1) - recently used and modified",All classes are equally good for replacement.,C,"The (0, 0) class (neither recently used nor modified) is the best to replace because the page hasn't been used lately and doesn't need to be written out to secondary storage, thus minimizing overhead."
Which counting-based page replacement algorithm selects the page with the smallest access count?,Most Frequently Used (MFU),Optimal (OPT),Least Recently Used (LRU),Least Frequently Used (LFU),"First-In, First-Out (FIFO)",D,"The Least Frequently Used (LFU) algorithm replaces the page that has the smallest count of references, assuming it's the least useful."
A common problem with the LFU algorithm is that a page heavily used initially might remain in memory even if unused later. What is a suggested solution for this?,Periodically resetting all counts to zero.,Converting it to an LRU algorithm.,Shifting counts right periodically to create an exponentially decaying average.,Increasing the count of frequently used pages to infinity.,Only allowing pages with zero counts to be replaced.,C,"To address the issue where initially heavily used pages retain high counts and aren't replaced, LFU can be modified by periodically shifting counts right, effectively creating an exponentially decaying average that favors more recent usage patterns."
Page-buffering algorithms are typically used in addition to main page-replacement algorithms. What is one of their benefits?,They eliminate the need for swap space entirely.,They ensure that all pages are modified before being written to disk.,They allow a process to restart faster after a page fault by reading the desired page into a free frame before the victim is written out.,They reduce the total number of page faults that occur.,They are a direct replacement for the LRU algorithm.,C,"Page-buffering algorithms often maintain a pool of free frames. On a page fault, the desired page can be read into a free frame from this pool immediately, allowing the process to restart faster, even if the victim frame has not yet been written out to swap space."
What is 'raw disk' or 'raw I/O' in the context of applications and page replacement?,A highly compressed format for storing data on disk to save space.,"A method for accessing secondary storage directly as an array of logical blocks, bypassing file-system services.",A technique that uses solid-state drives exclusively for all I/O operations.,A system where all disk operations are buffered in physical memory before being written.,A protocol for secure data transmission over a network.,B,"Raw disk (or raw I/O) refers to allowing special programs to use secondary storage directly as a large sequential array of logical blocks, completely bypassing standard file-system services like demand paging, locking, prefetching, and directory management."
"Why might some applications, like databases, prefer to use raw disk access over standard OS virtual memory buffering?",Raw disk access allows them to utilize unlimited virtual memory.,Applications can have a better understanding of their specific memory and storage usage patterns than general-purpose OS algorithms.,Raw disk access enables the application to run directly on the hardware without an OS.,It provides a mechanism for automatic data encryption and compression.,Raw disk access eliminates the need for any form of page replacement.,B,"Applications such as databases often perform worse with OS virtual memory buffering because they are designed with their own sophisticated memory and storage management, which can be more efficient for their specific workloads than the general-purpose algorithms provided by the OS. Raw disk access allows them to implement their own optimized I/O."
"In a pure demand paging system, what happens when a process terminates?",Its frames are immediately swapped to backing store.,Its frames are reallocated to other running processes without being freed.,Its frames are returned to the free-frame list.,The OS reclaims them for exclusive kernel use.,They remain allocated but marked as inactive until system shutdown.,C,"When a process terminates in a pure demand paging system, its allocated frames are returned to the free-frame list for future use."
Which of the following describes a common variation in frame allocation where the OS might utilize frames from the free-frame list?,The OS permanently reserves all free frames for future kernel upgrades.,"The OS allocates buffer/table space from the free-frame list, which can be temporarily used for user paging.","The OS strictly partitions free frames, dedicating half to kernel and half to user processes.","The OS uses free frames exclusively for disk caching, never for user processes.",The OS converts all free frames into swap space on secondary storage.,B,"A variation allows the OS to allocate buffer/table space from the free-frame list, with these frames being available for user paging when not otherwise in use."
What is the primary constraint regarding the minimum number of frames that must be allocated to a process?,It cannot exceed half of the total available frames.,It must be enough to hold the entire program code.,It must be at least the number required for all pages an instruction can reference.,"It is always a fixed value, such as three frames per process.",It depends solely on the process's priority.,C,"A process must be allocated at least a minimum number of frames, which is defined by the computer architecture and must be enough for all pages an instruction can reference, preventing an instruction from faulting before completion."
Why does a lower number of allocated frames generally lead to a higher page-fault rate and slower execution?,Fewer frames mean more I/O operations are needed to load the entire program at once.,"With fewer frames, the system has less buffer space for disk operations.","A smaller working set of pages can reside in memory, leading to more frequent page-ins.",The CPU cache becomes less effective with fewer main memory frames.,The operating system takes more time to manage a smaller number of frames.,C,"Fewer frames allocated to a process means that its working set of pages cannot be fully resident in memory, increasing the likelihood of page faults as needed pages are swapped out and then back in."
"According to the text, what is the minimum number of frames a process requires for a one-level indirect addressing instruction?",1 frame,2 frames,3 frames,4 frames,6 frames,C,"For a one-level indirect addressing instruction, a process needs at least three frames: one for the instruction, one for the address, and one for the operand."
Which allocation algorithm assigns an equal number of frames to each process?,Proportional allocation,Priority-based allocation,Dynamic allocation,Equal allocation,Global allocation,D,Equal allocation is the strategy where available frames are split equally among all active processes.
What is the primary drawback of 'equal allocation' when processes have significantly different memory needs?,"It can lead to over-allocation for larger processes, causing system instability.",It requires complex algorithms to manage frame distribution.,"It allocates frames inefficiently, potentially wasting frames for smaller processes.",It makes it difficult to implement page replacement algorithms.,It prioritizes system processes over user processes.,C,"Equal allocation can be inefficient because it gives the same number of frames to all processes, regardless of their actual memory needs, potentially wasting frames for processes that require less memory."
How does 'proportional allocation' determine the number of frames assigned to each process?,Based on the CPU priority of the process.,In inverse proportion to the process's virtual memory size.,In proportion to the process's virtual memory size.,By allocating frames from a reserved pool for each process.,Randomly assigns frames from the free-frame list.,C,"Proportional allocation assigns memory frames to a process in proportion to its virtual memory size, aiming for a more efficient distribution based on need."
Under what circumstance might processes lose frames in an equal or proportional allocation scheme?,When the system's overall memory utilization decreases.,When the multiprogramming level decreases.,When a process's priority is lowered.,When the multiprogramming level increases.,When the system switches from demand paging to pre-paging.,D,"If the multiprogramming level increases (more processes share the same total frames), each process's share of frames will necessarily decrease."
"Which page-replacement strategy allows a process to select a replacement frame from any frame in the system, even if it is currently allocated to another process?",Local replacement,Segmented replacement,Proportional replacement,Global replacement,Priority-based replacement,D,"Global replacement allows a process to choose a replacement frame from the set of all frames in the system, potentially taking frames from other processes."
What is a disadvantage of a global replacement strategy compared to a local replacement strategy?,It typically results in lower system throughput.,A process's performance becomes dependent on the paging behavior of other processes.,It is more complex to implement in the operating system kernel.,It prevents high-priority processes from acquiring more frames.,It requires a larger minimum number of frames for each process.,B,"In global replacement, a process's performance can be adversely affected by the paging behavior of other processes (external circumstances), as they might take its frames."
A page fault that requires reading the page from the backing store is known as a:,Minor page fault,Soft page fault,Cache miss,Major page fault,TLB miss,D,A major page fault (or hard fault) occurs when the referenced page is not in memory and must be read from secondary storage (backing store).
Which of the following scenarios would typically result in a minor page fault?,Accessing a page that was swapped out to disk due to memory pressure.,Referencing a page that has never been loaded into physical memory.,A process referencing a shared library page that is already in memory but lacks a logical mapping for the process.,"Attempting to write to a read-only page, triggering a protection fault.",An invalid memory address being accessed by a process.,C,"A minor page fault occurs when the page is already in physical memory (e.g., a shared library page used by another process), but the current process does not have a valid logical mapping to it, only requiring a page table update."
"In the context of page reclaiming, what are 'reapers'?",Processes terminated by the OOM killer.,Background routines that scan memory to free frames and maintain minimum free memory.,Algorithms used to select a victim page during a major page fault.,User-space applications that monitor memory usage.,Hardware components responsible for memory garbage collection.,B,"Reapers are kernel routines that are triggered when free memory falls below a minimum threshold, and they reclaim pages from processes to maintain sufficient free memory."
What is the function of the Linux 'out-of-memory (OOM) killer'?,It alerts the system administrator when memory is full.,It terminates processes to free memory when free memory is critically low.,It defragments memory pages to improve performance.,It prevents processes from allocating too much memory initially.,It migrates processes to different NUMA nodes to balance memory usage.,B,"The Linux OOM killer is a mechanism that terminates processes, typically those with the highest 'OOM score' (indicating high memory usage), to free up memory when the system runs out."
What defines a Non-Uniform Memory Access (NUMA) system?,All CPU cores have equal access time to all memory locations.,Memory access time varies depending on the CPU core accessing it.,It is a system that uses only solid-state drives for storage.,Memory is exclusively accessed via a shared bus.,It uses a single large physical memory module for all CPUs.,B,"NUMA systems are characterized by varying memory access times depending on which CPU core is accessing a particular memory location, typically faster for local memory and slower for remote memory."
How do NUMA-aware allocation strategies typically handle page faults?,They always allocate the new frame from a centralized free-frame pool.,They allocate the new frame as close as possible to the CPU that caused the fault.,They swap out a page from a remote memory node to make space for the new page.,They randomly select a free frame from anywhere in the system.,They request user intervention to specify the allocation location.,B,"NUMA-aware systems prioritize allocating frames 'as close as possible' to the CPU that caused the page fault, typically from memory on the same system board, to minimize latency."
What is the purpose of 'lgroups' in Solaris for NUMA systems?,To group similar applications for better resource management.,To define security boundaries for memory access.,To gather CPUs and memory into locality groups for optimized access and reduced latency.,To manage inter-process communication between different CPU cores.,To dynamically adjust CPU clock speeds based on memory load.,C,Solaris uses 'lgroups' (locality groups) to group CPUs and their associated memory into hierarchical structures. The system then schedules threads and allocates memory within these lgroups to minimize memory latency and maximize cache hit rates.
A page fault that is resolved by updating the page table without needing to read data from secondary storage is referred to as a:,Hard page fault,Major page fault,Minor page fault,Swap-in fault,Protection fault,C,"A minor page fault (or soft fault) is resolved without performing I/O from the backing store, usually because the page is already in memory but needs a page table entry update."
"If a system has 93 frames available for user processes and 5 processes are running, how many frames would each process receive under 'equal allocation' and how many would be leftover for buffer space?","15 frames each, 18 leftover","18 frames each, 3 leftover","20 frames each, 3 leftover","17 frames each, 8 leftover","18 frames each, 0 leftover",B,"Under equal allocation, 93 frames / 5 processes = 18 frames per process with a remainder of 3 frames (93 - 5*18 = 3) which can be used for buffer space or other system needs."
Which of the following best defines 'thrashing' in an operating system context?,A process spending more time executing than paging.,"A low rate of page faults, indicating efficient memory utilization.",High paging activity where a process spends more time paging than executing.,The process of swapping entire processes between main memory and disk.,The mechanism by which the CPU scheduler increases multiprogramming.,C,"Thrashing is defined as high paging activity where a process spends more time paging than executing, leading to severe performance problems."
What is a primary symptom of a process that is thrashing?,It rarely experiences page faults.,It replaces pages that are not needed immediately.,"It quickly page-faults and replaces pages needed immediately, leading to repeated faults.",It consistently uses its minimum required frames efficiently.,Its CPU utilization remains consistently high.,C,"A process without 'enough' frames (minimum needed for working set) will quickly page-fault and replace pages needed immediately, causing it to fault again and again."
Which of the following operating system behaviors can exacerbate or cause thrashing?,Maintaining a high CPU utilization by reducing multiprogramming.,Increasing the degree of multiprogramming when CPU utilization is low.,Implementing a local page-replacement algorithm for all processes.,Allocating enough frames to each process to cover its working set.,Prioritizing processes with high page-fault rates for CPU time.,B,"The text describes a scenario where low CPU utilization prompts the OS to increase multiprogramming, which can lead to processes taking frames from each other, increasing page faults, and eventually resulting in thrashing as CPU utilization drops further and the OS continues to increase multiprogramming."
How does the use of a global page-replacement algorithm contribute to thrashing?,It ensures each process maintains its minimum frame requirement.,"It allows a thrashing process to steal frames from other processes, causing them to fault.","It prevents new processes from being initiated, thus reducing multiprogramming.",It allocates frames based on a process's working-set size.,It limits paging device queues by prioritizing non-faulting processes.,B,"A global page-replacement algorithm replaces pages without regard to the process they belong to. This allows a process needing more frames (e.g., one that's starting to thrash) to take frames from other processes, which then causes those processes to fault, cascading the problem across the system."
"During thrashing, what typically happens to system throughput and CPU utilization?",Both system throughput and CPU utilization increase.,"System throughput increases, but CPU utilization decreases.",Both system throughput and CPU utilization plunge/drop sharply.,"System throughput plunges, while CPU utilization remains stable.","CPU utilization increases rapidly, while throughput remains low.",C,"The text states that when thrashing occurs, 'system throughput plunges' and 'CPU utilization drops sharply' (as illustrated in Figure 10.6.1). Processes spend all their time paging, and no work gets done."
"According to the text, what is the most direct action to stop thrashing once it has begun?",Increase the degree of multiprogramming.,Implement a global page-replacement algorithm.,Decrease the degree of multiprogramming.,Increase the system's effective memory-access time.,Allow processes to frequently change their working sets.,C,The text explicitly states: 'To stop thrashing: decrease degree of multiprogramming.'
What is the primary advantage of a 'local replacement algorithm' or 'priority replacement algorithm' in mitigating thrashing?,It allows a thrashing process to steal frames from other processes more efficiently.,It ensures that all processes have an equal number of frames.,It prevents a thrashing process from stealing frames from other processes.,It completely eliminates the need for a paging device.,It automatically adjusts the working-set window size for each process.,C,Local (or priority) replacement algorithms are defined as avoiding thrashing by not allowing a process to steal frames from other processes. A process selects pages only from its own frames.
"While local replacement algorithms help limit thrashing effects, why do they not entirely solve the problem?","They increase the CPU utilization, leading to more processes being admitted.",They don't prevent processes from needing more frames than available in their own set.,"Thrashing processes still queue for the paging device, increasing average service time for page faults for all processes.",They lead to an increase in the number of frames required for each process.,They only work for processes with very small working sets.,C,"The text explains that while local replacement prevents a thrashing process from stealing frames, the thrashing processes still queue for the paging device, leading to increased average service time for page faults and, consequently, increased effective access time for all processes."
"What is the fundamental approach to preventing thrashing, as stated in the text?",To always use a global page-replacement algorithm.,To continuously increase the degree of multiprogramming.,To provide a process with enough frames.,To ignore the page-fault rate of processes.,To exclusively use the Page-Fault Frequency strategy.,C,The text explicitly states: 'To prevent thrashing: provide process with enough frames.'
"What concept is introduced to determine how many frames a process needs, based on the pages it actually uses?",The Page-Fault Frequency strategy.,The Global Replacement Algorithm.,The Locality Model.,The Least Recently Used (LRU) algorithm.,"The First-In, First-Out (FIFO) algorithm.",C,The text states: 'How many frames needed? Look at frames actually used -> locality model.'
"According to the locality model, what is a 'locality'?",The total number of frames available in physical memory.,The set of all pages ever referenced by a process.,The set of pages actively used together by a process during a phase of its execution.,A fixed memory region allocated to a process at startup.,The physical address space where a process's code resides.,C,A 'locality' is defined as the set of pages actively used together. The locality model states that a process moves from locality to locality during execution.
The working-set model is based on which fundamental assumption?,Processes typically require an infinite number of frames.,Memory accesses are completely random.,The principle of locality.,All processes should have the same page-fault frequency.,CPU utilization is inversely proportional to multiprogramming.,C,"The working-set model is based on the locality assumption, meaning that programs exhibit patterned memory accesses, not random ones."
"In the working-set model, what does the parameter Δ (Delta) define?",The maximum allowed page-fault frequency.,The total number of available frames in memory.,"The working-set window, representing the most recent page references.",The number of processes that can be simultaneously active.,The minimum number of frames required for any process.,C,"The working-set model uses parameter Δ to define the 'working-set window', which is the time interval or number of page references examined to determine the working set."
How is a 'working set' defined in the working-set model?,All pages currently present in physical memory.,The set of all pages ever referenced by a process since its inception.,The set of pages referenced in the most recent Δ (Delta) page references.,The minimum number of frames required for a process to execute without faulting.,The sum of all working-set sizes for all active processes.,C,"The 'working set' is defined as the set of pages in the most recent Δ references, approximating a program's current locality."
"In the working-set model, if the total demand for frames (D) (sum of all $WSS_i$) exceeds the total available frames (m), what is the expected outcome?",The operating system will increase the degree of multiprogramming.,All processes will run with optimal performance.,Thrashing will occur because some processes will lack enough frames.,The working-set window (Δ) will automatically decrease.,The system will automatically switch to a global replacement algorithm.,C,The text states: 'If D > m (total available frames) -> thrashing (some processes lack frames).'
How does an operating system typically manage processes using the working-set model to prevent thrashing?,It randomly allocates frames to processes without monitoring their working sets.,It decreases the degree of multiprogramming whenever CPU utilization is high.,"It allocates enough frames for each process's working-set size; if total demand exceeds available frames, it suspends a process.",It constantly swaps all processes in and out of memory to balance load.,It maintains a fixed number of frames for all processes regardless of their demand.,C,"The OS monitors the working set of each process, allocates enough frames for its size. If the sum of working-set sizes exceeds available frames, the OS suspends a process, swaps out its pages, reallocates frames, and restarts it later."
What is a major difficulty in implementing the working-set model precisely?,Determining the total number of available physical frames.,Calculating the page-fault frequency accurately.,Tracking the moving working-set window in real-time.,Deciding which processes to suspend when memory is insufficient.,Defining what constitutes a 'locality' for a given program.,C,The text points out the difficulty: 'tracking moving working-set window.'
How does an operating system approximate the working-set window in practice?,By continuously monitoring all page references in real-time with no approximation.,By using a fixed-interval timer interrupt and reference bits.,By always assuming an infinite working-set window.,By setting the Delta (Δ) parameter to zero for all processes.,By only considering pages that have been modified (dirty bits).,B,"The working-set window is approximated using a fixed-interval timer interrupt and reference bits. At each interrupt, reference bits are copied and cleared, helping to track recent page usage."
Why was the Page-Fault Frequency (PFF) strategy developed as an alternative to the working-set model for thrashing control?,The working-set model was too simple and not comprehensive enough.,The working-set model was entirely unsuccessful and never used.,"The working-set model was successful and useful for prepaging, but clumsy for thrashing control.","PFF allows for a completely random allocation of frames, simplifying management.",PFF does not require monitoring of page references.,C,"The text states: 'Working-set model successful, useful for prepaging, but clumsy for thrashing control. Page-fault frequency (PFF) strategy: more direct.'"
How does the Page-Fault Frequency (PFF) strategy adjust the number of frames allocated to a process?,"If PFF is too high, it removes frames; if too low, it allocates more frames.",It always allocates a fixed number of frames regardless of PFF.,It ignores the PFF and only considers the process's priority.,"If actual PFF exceeds an upper limit, it allocates another frame; if it falls below a lower limit, it removes a frame.",It only removes frames when the system is about to crash.,D,"The PFF strategy establishes upper and lower bounds on the desired page-fault rate. If the actual PFF exceeds the upper limit, the process needs more frames, so another is allocated. If it falls below the lower limit, the process may have too many frames, so one is removed."
"According to current practice, what is considered the best way to avoid thrashing and swapping?",Continuously monitor and adjust the degree of multiprogramming.,Implement complex working-set and page-fault frequency algorithms.,Include enough physical memory to meet virtual memory demand.,Force all applications to use a local replacement algorithm.,Increase the frequency of timer interrupts for better working-set approximation.,C,The 'Current practice' section states: 'Best practice: include enough physical memory to avoid thrashing/swapping.' This provides the best user experience.
Memory compression is presented as an alternative to which common memory management technique?,Virtualization,Disk caching,Paging,Segmentation,Multithreading,C,The text states: 'Alternative to paging: memory compression.'
What is the primary method memory compression uses to reduce memory usage?,Swapping frames to disk more frequently,Consolidating multiple frames into a single frame,Increasing the size of individual memory frames,Eliminating the need for a free-frame list,Reducing the number of processes in memory,B,The text explains: 'Compress several frames into a single frame' and 'Reduces memory usage without swapping pages.'
"According to the example, what condition typically triggers memory compression as part of page replacement?",A page fault occurs for a non-resident page,The CPU utilization exceeds a certain threshold,The free-frame list drops below a specific threshold,A new process requests a large block of memory,An application explicitly requests memory compression,C,The example states: 'Free-frame list below threshold → triggers page replacement.'
"When frames are selected for compression, what happens instead of writing them to swap space?",They are immediately discarded to free memory,They are marked as read-only and kept in memory,They are compressed and stored within another single page frame,They are moved to a special 'quarantine' area of memory,They are replicated across multiple physical frames for redundancy,C,"The text describes: 'Instead of writing to swap space, compress frames (e.g., three) into single page frame.'"
"After frames like 15, 3, and 35 are compressed and stored into a new frame (e.g., frame 7), what happens to the original frames (15, 3, 35)?",They remain on the modified-frame list,They are permanently deleted from memory,They are moved to the free-frame list,They are immediately swapped out to disk,They are marked as reserved for future use by the compressed data,C,"The example states: 'Frames 15, 3, 35 moved to free-frame list.'"
What action occurs if a compressed frame is referenced by the system?,The system issues a warning to the user,The compressed frame is immediately swapped out,"A page fault occurs, leading to decompression and restoration of original pages",The frame is simply re-compressed into an even smaller size,"The reference is ignored, as the data is not directly accessible",C,"The text explains: 'If compressed frame referenced → page fault, decompressed, restoring original pages.'"
Which mobile operating systems are noted for using memory compression as an integral part of their memory-management strategy due to generally not supporting standard swapping/paging?,Windows Mobile and BlackBerry OS,Android and iOS,Symbian and WebOS,Tizen and Sailfish OS,Ubuntu Touch and Firefox OS,B,"The text states: 'Mobile systems (Android, iOS) generally don't support standard swapping/paging. Memory compression integral to their memory-management strategy.'"
Which of the following desktop operating systems are mentioned as supporting memory compression?,Linux and Chrome OS,FreeBSD and OpenBSD,Windows 7 and macOS (Pre-10.9),Windows 10 and macOS (Version 10.9+),MS-DOS and Windows XP,D,The text explicitly states: 'Windows 10 and macOS support memory compression.'
"On Windows 10, which type of applications are specifically mentioned as candidates for memory compression on mobile devices?",Classic Win32 applications,Universal Windows Platform (UWP) apps,Java-based applications,Linux subsystem applications,Legacy .NET Framework applications,B,The text specifies: 'Windows 10: Universal Windows Platform (UWP) apps on mobile devices are candidates.'
How does macOS (Version 10.9+) prioritize memory compression relative to paging?,It always pages to SSD before attempting any compression.,It only compresses pages that are actively being used.,"It compresses LRU pages when free memory is short, then pages if needed.",It compresses pages only after all swap space has been exhausted.,It uses memory compression and paging simultaneously with equal priority.,C,"The text states: 'macOS (Version 10.9+): compresses LRU pages when free memory is short, then pages if needed.'"
"According to performance tests on macOS, how does memory compression compare to paging to an SSD?",Memory compression is significantly slower than paging to SSD.,Memory compression and paging to SSD have comparable performance.,Memory compression is faster than paging to SSD.,Memory compression is only faster if the SSD is nearly full.,There is no significant performance difference between the two.,C,The text notes: 'Performance tests: memory compression faster than paging to SSD on macOS.'
What is a requirement for memory compression to store compressed pages?,A dedicated hardware compression unit,Allocating free frames for the compressed data,Pre-existing swap space on a hard drive,Disabling the CPU's caching mechanism,Increasing the total physical RAM in the system,B,The text states: 'Memory compression requires allocating free frames for compressed pages.'
What is the main contention or trade-off in designing memory compression algorithms?,Between data integrity and system stability,Between network bandwidth and CPU cycles,Between compression speed and compression ratio,Between power consumption and memory capacity,Between security vulnerabilities and ease of implementation,C,The text mentions: 'Contention between compression speed and compression ratio (amount of reduction).'
What is generally true about memory compression algorithms that aim for higher compression ratios?,They are typically faster and less computationally expensive.,They are only effective on very small amounts of data.,They tend to be slower and more computationally expensive.,They require specialized hardware found only in servers.,They have no impact on CPU usage or speed.,C,"The text states: 'Higher compression ratios → slower, more computationally expensive algorithms.'"
How can the performance of memory compression be improved using hardware resources?,By increasing the clock speed of the single core,By reducing the total amount of RAM available,By using parallel compression with multiple cores,By offloading compression tasks to the GPU,"By using slower, but more energy-efficient, storage",C,The text indicates: 'Improved by parallel compression using multiple cores.'
Microsoft's Xpress and Apple's WKdm are cited as examples of compression algorithms that balance which two factors?,Security and accessibility,Storage capacity and network latency,High ratios with fast algorithms,Power efficiency and manufacturing cost,Compatibility and proprietary features,C,"The text states these examples 'balance factors: high ratios with fast algorithms' and are 'fast, compress to 30-50% original size.'"
What is the definition of 'compression ratio' as provided in the glossary?,The speed at which data can be compressed.,The amount of memory required to perform compression.,A measurement of compression effectiveness (ratio of compressed to uncompressed space).,The number of frames that can be compressed into one.,The percentage of CPU time spent on compression tasks.,C,The glossary defines 'compression ratio' as 'Measurement of compression effectiveness (ratio of compressed to uncompressed space).'
"What does the acronym 'UWP' stand for, and what is its purpose according to the glossary?",Unified Windows Protocol; a new network communication standard.,Universal Wireless Power; a technology for wireless charging.,Unique Workstation Platform; a system for specialized workstations.,Universal Windows Platform; Windows 10 architecture providing common app platform for all devices running it.,Underlying Web Portal; a framework for web-based applications.,D,The glossary defines 'Universal Windows Platform (UWP)' as 'Windows 10 architecture providing common app platform for all devices running it.'
What is the estimated typical compression ratio achieved by fast algorithms like Microsoft's Xpress and Apple's WKdm?,5-10% of original size,15-25% of original size,30-50% of original size,60-75% of original size,80-95% of original size,C,"The text mentions: 'Examples: Microsoft's Xpress, Apple's WKdm → fast, compress to 30-50% original size.'"
What is the typical behavior when a user-mode process requests memory from the kernel?,Pages are allocated from the kernel's free page frame list.,Memory is directly allocated from the physically contiguous kernel pool.,A fixed-size 256 KB segment is always granted.,The request is immediately denied due to security policies.,The memory is allocated from a dedicated user-mode heap.,A,User-mode process requests memory typically result in pages being allocated from the kernel's free page frame list.
"When a user-mode process requests a single byte of memory, what type of fragmentation is commonly observed due to the allocation of an entire page frame?",External fragmentation,Internal fragmentation,Contiguous fragmentation,Virtual fragmentation,Paging fragmentation,B,"A single byte request resulting in an entire page frame being granted leads to internal fragmentation, as the full page is allocated but only a small portion is used."
Which of the following is a primary reason why kernel memory is often allocated from a different free-memory pool compared to user-mode memory?,Kernel requests always require larger memory blocks than user-mode processes.,Kernel code and data are always subject to paging.,"Kernel requests vary in data structure sizes, some being less than a page, requiring conservative memory use.",User-mode pages must always be physically contiguous.,Kernel memory is never deallocated once allocated.,C,"Kernel memory allocation differs because kernel requests often involve varying data structure sizes, some less than a page, necessitating conservative memory use to minimize fragmentation."
"Why might hardware devices interacting with physical memory require physically contiguous pages, unlike typical user-mode pages?",Hardware devices use virtual memory interfaces.,User-mode pages are always physically contiguous.,Hardware devices directly interact with physical memory and may not have a virtual memory interface.,Kernel code is always mapped to non-contiguous physical memory.,It's a security measure to prevent unauthorized access.,C,"Hardware devices interact directly with physical memory and often lack a virtual memory interface, thus requiring physically contiguous pages for their operations."
What are the two main strategies discussed for managing kernel free memory?,First-fit and Best-fit,Paging and Swapping,Buddy system and Slab allocation,Segmentation and Partitioning,LIFO and FIFO,C,The text explicitly states 'buddy system' and 'slab allocation' as the strategies for managing kernel free memory.
The 'buddy system' is characterized as a 'power-of-2 allocator'. What does this mean for how it satisfies memory requests?,"It allocates memory in arbitrary sizes, then rounds down to the nearest power of 2.","It satisfies requests in units sized as a power of 2 (e.g., 4 KB, 8 KB, 16 KB).",It always allocates exactly 2 KB or 4 KB segments.,It divides memory into two equal parts repeatedly until the exact size is found.,It only allocates memory if the request is an exact power of 2.,B,A 'power-of-2 allocator' means the buddy system satisfies memory requests in units that are sized as a power of 2.
"If a kernel process requests 21 KB of memory using the buddy system, what is the size of the segment that would typically be allocated?",16 KB,21 KB,24 KB,32 KB,64 KB,D,"Requests not appropriately sized are rounded up to the next highest power of 2. For 21 KB, the next highest power of 2 is 32 KB."
What is the primary advantage of the buddy system for kernel memory management?,It eliminates all forms of internal fragmentation.,It ensures that all memory requests are fulfilled with exact sizes.,It allows for quick combining of adjacent freed segments into larger ones through coalescing.,It guarantees that less than 10% of allocated memory is wasted.,It exclusively allocates non-contiguous memory blocks.,C,A key advantage of the buddy system is its ability to quickly combine adjacent buddies to form larger segments using coalescing.
What is a significant drawback of the buddy system regarding memory utilization?,It requires manual intervention to allocate memory.,It always results in external fragmentation.,It suffers from internal fragmentation due to requests being rounded up to the next power of 2.,It cannot allocate memory segments larger than 4 KB.,It has very slow allocation and deallocation times.,C,A major drawback is the internal fragmentation caused by rounding up memory requests to the next highest power of 2.
"In the context of slab allocation, what is a 'slab' defined as?",A collection of unrelated kernel objects.,One or more physically contiguous pages.,A single byte of allocated memory.,A fixed-size 256 KB segment for kernel data.,A temporary copy of user data for performance.,B,A slab is defined as one or more physically contiguous pages used in slab allocation.
What is a 'cache' in the context of slab allocation?,A single kernel object.,A temporary data storage for CPU registers.,A collection of one or more slabs.,A list of free page frames managed by the kernel.,A mechanism for virtual to physical address translation.,C,"In slab allocation, a cache consists of one or more slabs and is used to store objects of a specific kernel data structure."
"In slab allocation, a single cache is typically maintained for what purpose?",To handle all general-purpose memory requests.,To store executable kernel code.,For each unique kernel data structure.,To manage the free page frame list.,To buffer I/O operations.,C,"Slab allocation uses a single cache for each unique kernel data structure, such as process descriptors or file objects."
"When the slab allocator needs to fulfill a new object request, what is its preferred order of satisfaction?","First from a newly allocated slab, then an empty slab, then a partial slab.","First from an empty slab, then a partial slab, then a newly allocated slab.","First from a partial slab, then an empty slab, then a newly allocated slab.",It randomly picks any available slab.,Only from a full slab after objects are released.,C,"The slab allocator first attempts to find a free object in a partial slab, then an empty slab, and finally allocates a new slab if necessary."
Which of the following is a primary benefit of slab allocation regarding memory fragmentation?,It introduces external fragmentation but reduces internal fragmentation.,It ensures memory is always allocated in fixed 4 KB pages.,It eliminates memory wasted due to fragmentation by returning the exact amount requested for kernel objects.,It relies on periodic defragmentation to consolidate free space.,It rounds up memory requests to the next power of 2.,C,A main benefit of slab allocation is that it avoids memory wasted due to fragmentation by dividing slabs into object-sized chunks and returning the exact amount requested.
How does slab allocation contribute to satisfying memory requests quickly?,By deallocating all objects immediately after use.,By creating objects in advance and making released objects immediately available from the cache.,By always allocating a brand new slab for every request.,By using a best-fit algorithm for memory placement.,By outsourcing memory management to user-mode processes.,B,"Slab allocation speeds up memory requests because objects are created in advance, and released objects are marked free and returned to the cache, making them immediately available for reuse."
"In the Linux kernel's slab allocator, what is the state of a slab if all of its objects are marked as 'used'?",Empty,Partial,Available,Full,Idle,D,A slab is considered 'Full' when all objects within it are marked as 'used'.
"Which Linux kernel memory allocator is specifically designed for systems with limited memory, such as embedded systems?",SLAB,SLUB,SLOB,Buddy system,Page frame allocator,C,"The SLOB allocator is explicitly mentioned as being for systems with limited memory, such as embedded systems."
"What is a significant improvement of the SLUB allocator over the original SLAB allocator in Linux, as mentioned in the text?",It introduced the concept of physically contiguous pages for slabs.,It added per-CPU queues for objects to enhance performance.,"It stores metadata in the 'page' structure instead of with each slab, reducing overhead.",It ensures that all allocated memory is an exact power of 2.,It is primarily used for user-mode memory requests.,C,"The SLUB allocator reduced overhead by storing metadata in the 'page' structure instead of with each slab, and it also removed per-CPU queues for objects."
"The SLOB allocator manages memory through three lists: small, medium, and large. What policy does it use to allocate from the appropriate list?",Least Recently Used (LRU),Best-fit,Worst-fit,First-fit,Round Robin,D,The SLOB allocator allocates from its lists using a first-fit policy.
What is the definition of 'coalescing' in the context of the buddy system?,Dividing a large memory segment into smaller 'buddies'.,Rounding up a memory request to the next highest power of 2.,Combining freed memory in adjacent buddies into larger segments.,Allocating memory from a fixed-size segment of physically contiguous pages.,Marking objects as 'used' in a slab.,C,Coalescing refers to the process of combining freed memory in adjacent buddies to form larger segments.
What defines an 'object' in the context of slab allocation?,A generic block of memory.,A single physically contiguous page.,An instantiation of a kernel data structure.,A cache of memory.,A list of free memory regions.,C,"In slab allocation, an 'object' is an instantiation of a kernel data structure (e.g., process descriptors)."
"What is the primary purpose of ""prepaging"" in memory management?",To reduce the overall number of physical memory pages used by a process.,To prevent a high number of initial page faults when a process starts.,To improve the hit ratio of the Translation Look-aside Buffer (TLB).,To allow pages to be locked in memory during I/O operations.,To ensure that all pages of a process are always present in memory.,B,Prepaging is an attempt to prevent the high number of initial page faults that occur due to initial locality when a process starts.
Which strategy describes how prepaging attempts to achieve its goal?,By swapping out less frequently used pages to disk.,By bringing some or all potentially needed pages into memory at once.,By increasing the size of the TLB to reduce lookup times.,By reordering process instructions to improve data locality.,By dynamically adjusting the page replacement algorithm.,B,"The strategy for prepaging is to bring some or all needed pages into memory at once, anticipating their use."
How does the working-set model relate to prepaging when a suspended process resumes?,"The working set is ignored, and all pages are brought in on demand.",Only the modified pages from the working set are brought back.,The entire working set is automatically brought back into memory before restarting the process.,The working set is copied to disk to free up memory for other processes.,"Prepaging is only applied to the code segment, not the working set.",C,An example of prepaging is remembering the working set for a suspended process and automatically bringing the entire working set back before restarting it.
"In the cost analysis of prepaging, if 's' pages are prepaged and 'alpha' is the fraction of those pages actually used, what scenario would cause prepaging to be disadvantageous?",Alpha is approximately 1.,The cost of 's * (1 - alpha)' unnecessary pages is much higher than 's * alpha' saved page faults.,The cost of 's * alpha' saved page faults is much higher than 's * (1 - alpha)' unnecessary pages.,The total number of prepaged pages 's' is very small.,The system has an infinite supply of free memory frames.,B,"Prepaging loses if alpha is approximately 0, meaning many prepaged pages are not used. This makes the cost of unnecessary pages outweigh the benefit of saved page faults."
For which type of data is prepaging generally more predictable and effective?,Dynamically linked libraries.,Executable programs.,Data structures with poor locality.,Files accessed sequentially.,Encrypted data streams.,D,"Prepaging files is more predictable because they are often accessed sequentially, making it easier to anticipate which pages will be needed."
Which Linux system call is explicitly mentioned as a mechanism for prefetching file contents into memory?,mmap(),sync(),readahead(),mlock(),fork(),C,The text states that the Linux `readahead()` system call prefetches file contents into memory.
Page sizes in new machine designs are invariably chosen to be what type of number?,Prime numbers.,Multiples of 100.,Powers of 2.,Numbers divisible by 3.,Arbitrary integers.,C,The text states that page sizes are 'invariably powers of 2'.
How does decreasing the page size affect the size of the page table?,"It decreases the number of pages, thus decreasing the page table size.","It increases the number of pages, thus increasing the page table size.",It has no effect on the page table size.,"It reduces internal fragmentation, making the page table smaller.","It only affects the TLB reach, not the page table size.",B,"Decreasing page size increases the number of pages required for a given virtual memory space, which in turn increases the size of the page table."
Which page size generally leads to better memory utilization and minimizes internal fragmentation?,Larger page sizes.,Smaller page sizes.,Page sizes that are multiples of 1024 bytes.,Page sizes equal to the working set size.,Page sizes that are prime numbers.,B,Memory utilization is better with smaller pages because the average waste due to internal fragmentation (part of the final page allocated but unused) is minimized.
"Considering the components of I/O time (seek, latency, transfer), which page size is generally argued to minimize the *total* I/O time for a given amount of data?","Extremely small page sizes (e.g., 1 byte) to minimize transfer time.","Moderate page sizes (e.g., 4KB) for a balance.","Larger page sizes, because seek and latency times often dwarf transfer time, so fewer I/O operations are better.",Page sizes that are not powers of 2.,Variable page sizes managed by the application.,C,"Although transfer time is proportional to page size, seek and latency times often dwarf it. Therefore, reading a larger page size in a single I/O operation (rather than multiple smaller ones) generally leads to less total I/O time."
"From the perspective of locality and resolution, why are smaller page sizes considered advantageous?","They lead to higher internal fragmentation, which improves resolution.","They allow for a larger TLB reach, improving locality.","They reduce total I/O and improve locality by matching program locality more accurately, isolating only memory actually needed.",They ensure that all pages of a process fit into physical memory.,"They increase the number of page faults, thereby forcing better locality.",C,"Smaller page sizes improve locality and resolution because each page matches program locality more accurately, isolating only the memory actually needed, and reducing total I/O."
Which page size generally helps in minimizing the number of page faults for a process?,"Smaller page sizes, as they allow for finer-grained memory management.","Larger page sizes, as more data can be brought in with each single page fault.",Page sizes that are a multiple of the cache line size.,Page sizes that are dynamically adjusted based on process behavior.,Page sizes determined by the operating system at runtime.,B,"Each page fault incurs significant overhead. Larger page sizes mean that a single page fault brings in more data, thereby reducing the total number of page faults for a given working set."
What has been the historical trend regarding page sizes in computing systems?,A trend towards smaller page sizes to reduce internal fragmentation.,"A trend towards variable page sizes, with no single optimal size.","A trend towards larger page sizes, even for mobile systems.",A trend towards eliminating paging in favor of larger physical memory.,A trend towards fixed 4KB page sizes across all systems.,C,"The text states, 'Historical trend: toward larger page sizes, even for mobile systems.'"
What is 'TLB reach'?,The maximum number of entries a TLB can hold.,The amount of memory accessible by the Translation Look-aside Buffer (TLB).,The percentage of virtual address translations that miss the TLB.,The speed at which the TLB can perform lookups.,The total physical memory available on a system.,B,TLB reach is defined as 'the amount of memory accessible from the TLB.'
How is TLB reach calculated?,Number of TLB entries divided by page size.,Total virtual memory size minus physical memory size.,Number of TLB entries multiplied by page size.,Hit ratio multiplied by the number of TLB entries.,The sum of all active process working sets.,C,TLB reach is calculated as 'number of entries × page size.'
"Besides increasing the number of TLB entries, what is another primary approach mentioned to increase TLB reach?",Decreasing the overall page table size.,Increasing the amount of physical RAM.,Reducing the CPU clock speed.,Increasing the page size or providing multiple page sizes.,Implementing a software-managed TLB.,D,"The text states, 'Another approach: increase page size or provide multiple page sizes.'"
"In ARM v8 architecture, what is the purpose of the ""contiguous bit"" in a TLB entry?",It indicates if the TLB entry has been modified recently.,It marks the TLB entry as invalid and ready for replacement.,It signifies that the entry maps a block of memory that is physically contiguous.,It determines if the page is read-only or writable.,It ensures that the TLB entry is always in the cache.,C,"The text states, 'Contiguous bit set: entry maps contiguous (adjacent) blocks of memory.'"
"What is a potential downside of increasing page size to improve TLB reach, especially for some applications?",Decreased number of page faults.,Reduced I/O overhead.,Increased internal fragmentation.,Slower TLB lookup times.,Higher cache hit ratio.,C,"The text mentions, 'Downside of larger page size: increased fragmentation for some applications.'"
What is the main purpose of using inverted page tables?,To improve the TLB hit ratio.,To reduce the amount of physical memory needed for virtual-to-physical address translations.,To accelerate page fault handling.,To allow for dynamic adjustment of page sizes.,To prevent internal fragmentation.,B,The primary purpose of inverted page tables is 'to reduce physical memory needed for virtual-to-physical address translations.'
How is an inverted page table structured?,"One entry per virtual page, indexed by process ID.","One entry per physical memory frame, indexed by <process-id, page-number>.","One entry per process, listing all its virtual pages.",A single global table indexed by virtual address.,A table stored entirely in the TLB.,B,"The method is 'one entry per page of physical memory, indexed by <process-id, page-number>.'"
"What is a significant downside of inverted page tables, and what is the typical solution for it?",They increase TLB lookup time; solved by larger TLB.,They increase internal fragmentation; solved by smaller page sizes.,They no longer contain complete info about a process's logical address space; solved by keeping external page tables.,They are prone to deadlock; solved by a dedicated lockout mechanism.,They require more physical memory; solved by using smaller entries.,C,"A downside is that they 'no longer contains complete info about logical address space of a process,' which is problematic for demand paging. The solution is to keep 'external page table (one per process).'"
"What special consideration is required for kernel handling when using inverted page tables, especially during a page fault?",Page faults can never occur with inverted page tables.,The kernel must immediately restart the faulting process without delay.,A page fault may cause another page fault when paging in the external page table.,The external page table must always be locked in memory.,Inverted page tables eliminate the need for page fault handling.,C,"A special case is mentioned: 'page fault may cause another page fault (paging in external page table),' requiring careful kernel handling."
"While demand paging is designed to be transparent to the user program, when is system performance improved regarding program structure?",When the user program intentionally introduces more page faults.,When the user and compiler are aware of demand paging and optimize for it.,When all data is stored contiguously in memory.,When the operating system uses a purely random page replacement policy.,When the TLB is completely disabled.,B,System performance is 'improved if user/compiler aware of demand paging.'
"In the example of initializing a 128x128 array with 128-word pages, which access order significantly reduces the number of page faults?","Row major order (data[i][j] with outer loop 'j', inner loop 'i').","Column major order (data[i][j] with outer loop 'i', inner loop 'j').",Random access pattern.,Diagonal access pattern.,Concurrent access by multiple threads.,B,"Column major order 'reduces page faults to 128' compared to 16,384 for row major order, by improving locality."
Which of the following data structures is cited as an example of having good locality of reference?,Hash table.,Linked list.,Stack.,Binary tree.,Heap.,C,The text states: 'Good locality: stack (access always to top).'
How can compilers and loaders contribute to better paging performance?,By placing frequently calling routines across page boundaries.,By marking all code pages as writable.,By separating code and data and packing frequently calling routines into the same page.,By increasing the overall number of pages used by a program.,By always using the smallest possible page size.,C,"Compilers and loaders can improve performance by 'separating code and data, reentrant code' and 'Pack frequently calling routines into same page' to enhance locality."
What is the main problem that I/O interlock and page locking aim to solve in demand paging?,Preventing a process from exceeding its allocated physical memory.,Ensuring that the TLB always has the correct translation for I/O buffers.,"Preventing pages containing I/O buffers from being paged out while I/O is in progress, leading to incorrect I/O.",Reducing the overhead of page table lookups during I/O operations.,Accelerating the transfer of data between the CPU and I/O devices.,C,"The problem scenario describes how an I/O buffer's page can be paged out by other processes (due to global replacement), causing I/O to occur to a frame now used for a different page. Locking prevents this."
What is the most common solution mentioned to prevent I/O buffers from being paged out during an I/O operation?,Copying data between system memory and user memory for every I/O operation.,Disabling page faults globally during I/O.,"Associating a 'lock bit' with every frame, preventing a locked frame from being selected for replacement.",Increasing the priority of the I/O-performing process to prevent preemption.,Using a dedicated I/O TLB that is never flushed.,C,"The text presents 'Allow pages to be locked into memory: lock bit associated with every frame' as a solution, explicitly stating that a 'Locked frame: cannot be selected for replacement.'"
"What is the term used when user processes, like database applications, request to lock pages into memory?",Swapping.,Flushing.,Pinning.,Caching.,Throttling.,C,The text states: 'User processes: may need to lock pages (pinning).'
How can a lock bit be used in the context of normal page replacement to improve performance or fairness for newly brought-in pages?,It prevents a low-priority process from ever experiencing a page fault.,It ensures that newly brought-in pages are immediately written to disk.,It allows a newly brought-in page to be protected from replacement until the faulting process has been dispatched again.,It forces a high-priority process to wait until all newly brought-in pages are used.,It automatically unlocks all pages after a fixed time interval.,C,The text describes this policy decision: 'Preventing replacement of newly brought-in page until used once: use lock bit. Page selected for replacement: lock bit on. Remains on until faulting process dispatched again.'
What is a potential danger or risk associated with the use of lock bits for pages?,It always leads to higher internal fragmentation.,"The lock bit may get turned on but never off, making the frame unusable.",It significantly increases the page table size.,It reduces the overall system throughput.,It makes it impossible to distinguish between clean and dirty pages.,B,"The text warns, 'Danger of lock bit: may get turned on but never off (bug). Locked frame becomes unusable.'"
"In the context of a cache, such as a TLB, what does ""hit ratio"" refer to?",The total number of successful memory accesses.,The percentage of virtual address translations resolved in the cache.,The speed at which data can be retrieved from the cache.,The amount of memory that can be stored in the cache.,The frequency of cache updates.,B,"'Hit ratio' is defined as the 'Percentage of times a cache provides a valid lookup (e.g., TLB effectiveness).'"
"What is the primary purpose of ""huge pages"" in modern operating systems like Linux?",To provide smaller page sizes for applications with high internal fragmentation.,"To designate a region of physical memory for especially large pages, often to improve TLB reach.",To reduce the total physical memory required for page tables.,To enable more efficient I/O operations by always locking pages.,To allow for pages to be encrypted transparently.,B,'Huge pages' are defined as a 'Feature designating a region of physical memory for especially large pages' and are mentioned in the context of increasing TLB reach.
Which two types of devices are primarily considered modern secondary storage?,RAM and CPU caches.,Hard Disk Drives (HDDs) and Nonvolatile Memory (NVM) devices.,Magnetic tapes and optical disks.,USB drives and network-attached storage.,Main memory (DRAM) and tertiary storage.,B,"Modern computers use secondary storage, primarily defined as hard disks (HDDs) and nonvolatile memory (NVM) devices."
Which of the following is NOT listed as a characteristic that varies among secondary storage devices?,Transfer method (character at a time vs. block of characters).,Access method (sequentially vs. randomly).,Data transfer timing (synchronously vs. asynchronously).,Power consumption (high vs. low).,Read-only vs. read-write capability.,D,"The text lists variations in transfer, access, data transfer timing, usage (dedicated vs. shared), read-only vs. read-write, and speed, but not specifically 'power consumption (high vs. low)' as a general varying characteristic."
What are the two key goals of the OS I/O subsystem concerning mass storage?,To provide complex interfaces and minimize concurrency.,To ensure maximum data redundancy and eliminate bottlenecks.,To offer the simplest interface possible and optimize I/O for maximum concurrency.,To prioritize sequential access and reduce random access.,To manage only volatile memory and external peripherals.,C,"The key OS I/O subsystem goals are to provide the simplest interface possible to the rest of the system and to optimize I/O for maximum concurrency, as devices are a performance bottleneck."
"What is the general term for all types of non-volatile storage systems of a computer, including HDDs and NVM devices?",Main memory.,Volatile storage.,Mass storage.,Cache memory.,System memory.,C,"Mass storage is the general term for the nonvolatile storage system of a computer, encompassing secondary and tertiary storage devices like HDDs, NVM devices, and magnetic tapes."
Which of the following is NOT a component or logical division of a Hard Disk Drive (HDD) platter?,Platter.,Read-write head.,Solid-state die.,Track.,Cylinder.,C,"Solid-state die (flash NAND die semiconductor chips) are components of Nonvolatile Memory (NVM) devices, not Hard Disk Drives. HDDs consist of platters, read-write heads, disk arms, tracks, sectors, and cylinders."
"In a Hard Disk Drive (HDD), what is a 'cylinder'?","A fixed-size section of a track, the smallest unit of transfer.",The flat circular shape covered with magnetic material.,The time it takes for the desired sector to rotate to the disk head.,The set of tracks that are under the read-write heads on all platters at a given arm position.,The mechanism that moves all read-write heads as a unit.,D,A cylinder is defined as the set of tracks at a given arm position on all platters in the device.
What is the smallest unit of transfer on a Hard Disk Drive (HDD) platter?,Track.,Cylinder.,Platter.,Sector.,Bit.,D,"Tracks are subdivided into sectors, and each sector is a fixed size, representing the smallest unit of transfer on an HDD."
Which two components constitute the 'positioning time' in a Hard Disk Drive (HDD)?,Transfer rate and effective transfer rate.,Seek time and rotational latency.,Platter spin-up time and head crash recovery.,Read time and write time.,Data access time and buffer flush time.,B,"Positioning time, also known as random-access time, is composed of seek time (moving the disk arm to the desired cylinder) and rotational latency (waiting for the desired sector to rotate to the disk head)."
What event describes a 'head crash' in a Hard Disk Drive (HDD)?,The drive motor failing to spin the platters.,The read-write head physically touching the disk surface.,A logical error in the disk's file system structure.,A power surge damaging the disk's controller.,The disk reaching its maximum read/write cycle limit.,B,A head crash occurs when the disk head makes contact with the disk surface. This is normally irreparable and results in data loss.
How do published transfer rates for disks typically compare to real-world 'effective transfer rates'?,Published rates are usually much lower than effective rates.,Published rates are always identical to effective rates.,Published rates are generally higher than effective rates.,"Effective rates are only relevant for sequential access, not random access.","Published rates only account for data transfer, not positioning time.",C,Published performance numbers for disks are often not the same as real-world performance; stated transfer rates are always higher than effective transfer rates (the rate blocks are actually delivered to the OS).
What is the fundamental difference in operation between Nonvolatile Memory (NVM) devices and Hard Disk Drives (HDDs)?,"NVM devices use magnetic media, while HDDs use electrical charges.","NVM devices are mechanical, while HDDs are electrical.","NVM devices are electrical, while HDDs are mechanical.","NVM devices are volatile, while HDDs are nonvolatile.","NVM devices require seek time, while HDDs do not.",C,"NVM devices are electrical in nature, storing data using semiconductor chips and electrical charges, whereas HDDs are mechanical, relying on spinning magnetic platters and moving read-write heads."
Which of the following is a common advantage of Nonvolatile Memory (NVM) devices over Hard Disk Drives (HDDs)?,Lower cost per megabyte.,Higher storage capacity for a given form factor.,Greater susceptibility to physical shock.,Faster access times due to no moving parts.,Requirement for complex mechanical synchronization.,D,NVM devices are more reliable (no moving parts) and faster (no seek or rotational latency) compared to HDDs.
What is a major challenge associated with NAND semiconductors used in NVM devices regarding data writes?,"Data can be directly overwritten without prior erasure, leading to fragmentation.","Data can only be written in entire block increments, not pages.","NAND cells must be erased first before data can be overwritten, and erasure is slower and occurs in larger 'block' increments.","NAND cells require constant power to retain data, leading to high energy consumption.",The read/write speed of NAND is inversely proportional to the number of parallel operations.,C,"A key challenge with NAND semiconductors is that data cannot be directly overwritten; NAND cells must be erased first, and erasure occurs in slower, larger 'block' increments rather than faster 'page' increments for reads/writes."
How is the lifespan of an NVM flash device primarily measured?,Total Gigabytes Written (GBW).,Number of power cycles.,Drive Writes Per Day (DWPD).,Mean Time Between Failures (MTBF).,Maximum Operating Temperature (MOT).,C,"The lifespan of NVM flash devices is measured in Drive Writes Per Day (DWPD), which indicates how many times the drive capacity can be written per day before failure over its warranty period."
What is the primary function of the Flash Translation Layer (FTL) in a Nonvolatile Memory (NVM) device controller?,To convert logical block addresses to traditional cylinder-head-sector addresses.,To manage the device's power consumption and sleep states.,To map physical pages to currently valid logical blocks and track physical block states for erasure.,To perform error correction on data during read operations.,To physically move data between different NAND die for performance optimization.,C,The Flash Translation Layer (FTL) maintains a mapping of physical pages to currently valid logical blocks and tracks which physical blocks contain only invalid pages and can be erased.
What is 'garbage collection' in the context of Nonvolatile Memory (NVM) devices?,A process of deleting temporary files from the operating system.,"The recovery of space by copying valid data from partially invalid blocks to new locations, thereby freeing up blocks for erasure and new writes.",The automatic defragmentation of data within a NAND flash die.,A mechanism to reduce the number of write cycles on specific blocks.,"The process of moving old, rarely accessed data to slower tertiary storage.",B,"Garbage collection occurs when no free blocks are available for new writes, but individual pages within blocks hold invalid data. Valid data is copied to other locations, freeing the original blocks for erasure and subsequent writes."
What is the purpose of 'over-provisioning' in Nonvolatile Memory (NVM) devices?,To increase the advertised capacity of the drive.,"To ensure data is always written to the oldest, most-worn blocks first.","To set aside a percentage of the total NAND capacity as always-available write area, improving write performance and wear leveling.",To allow the device to connect to multiple I/O buses simultaneously.,To enable the use of volatile memory as a temporary cache for writes.,C,"Over-provisioning involves the device setting aside pages (e.g., 20% of total) as always available write area. This space helps improve write performance and aids in wear leveling by providing readily available blocks."
What is the primary goal of 'wear leveling' in Nonvolatile Memory (NVM)?,To make all logical block addresses map to physically contiguous areas.,To uniformly distribute write and erase operations across all NAND cells to extend the device's lifespan.,To decrease the rotational latency of the NVM device.,To ensure that all data is encrypted before being written to the device.,To reduce the cost per gigabyte of NVM storage.,B,"Wear leveling is an effort to select all NAND cells over time as write targets, avoiding premature media failure due to frequently erased blocks by distributing writes evenly."
What are 'RAM drives' (RAM disks) and how are they used?,Physical hard drives that use RAM for caching.,Volatile memory sections carved out of DRAM and presented as secondary storage devices for temporary safekeeping of data.,Nonvolatile memory devices connected via a RAM interface.,External storage devices that require RAM for their operation.,A type of Solid-State Disk (SSD) that uses DRAM for persistent storage.,B,"RAM drives are created by device drivers to carve out a section of the system's DRAM and present it as a storage device. Despite being volatile, they are useful for placing temporary files or sharing data using standard file operations."
Which of the following best describes the access time characteristics of magnetic tapes?,"Extremely fast, comparable to SSDs for random access.","Moderate, similar to HDDs for both sequential and random access.","Very slow for random access, but comparable to HDDs for sequential reads/writes once positioned.","Always slower than HDDs, even for sequential access.",Faster than main memory for frequently used information.,C,"Magnetic tape access time is slow compared to main memory or drives, especially for random access (~1000x slower than HDDs, ~100,000x slower than SSDs). However, once positioned, tape drives can read/write at speeds comparable to HDDs."
Which of the following is NOT listed as a common type of I/O bus for connecting secondary storage devices?,SATA.,USB.,PCIe.,Fibre Channel (FC).,Ethernet.,E,"The text lists ATA, SATA, eSATA, SAS, USB, FC, and NVMe (which connects to PCIe) as I/O bus types. Ethernet is a networking protocol, not a direct storage I/O bus in this context."
What is NVMe (NVM Express) specifically designed for?,Connecting magnetic tape drives to older systems.,Providing a high-speed interface for Nonvolatile Memory (NVM) devices directly to the system PCI bus.,Enabling wireless communication between storage devices and the host.,Standardizing the connection for removable Hard Disk Drives.,Optimizing the seek time for traditional HDDs.,B,"NVMe is a special fast interface designed for NVM devices, connecting them directly to the system PCI bus to increase throughput and decrease latency."
"In a mass storage I/O operation, what is the role of the device controller?",To place commands into the host controller's memory-mapped I/O ports.,To send commands from the host to the device controller via messages.,To manage and operate the drive hardware itself based on commands received from the host controller.,To transfer data directly from the storage media to the host DRAM via DMA.,To implement wear leveling and garbage collection algorithms for NVM.,C,"The device controller, built into each storage device, operates the drive hardware based on commands received from the host controller."
How are storage devices typically addressed by the operating system and higher-level algorithms?,As a series of physical platter-track-sector tuples.,As a large one-dimensional array of logical blocks.,As individual magnetic charges on the media.,"As separate physical cylinders, each managed independently.",As a two-dimensional grid of sectors.,B,"Storage devices are addressed as large one-dimensional arrays of logical blocks, where each logical block is the smallest unit of transfer and maps to a physical sector or semiconductor page."
"Why is converting a Logical Block Address (LBA) to an old-style disk address (cylinder, track, sector) difficult in modern drives?","LBA only applies to NVM devices, not HDDs.",The number of logical blocks is too large for such conversion.,"Defective sectors are hidden by substituting spare sectors, the number of sectors per track may not be constant, and manufacturers manage the mapping internally.","The LBA scheme is only used for optical drives, not magnetic or solid-state.",Conversion is always straightforward and directly proportional.,C,"Converting LBA to old-style disk addresses is difficult because defective sectors are hidden by substituting spares, the number of sectors per track is not constant on some drives, and disk manufacturers manage the LBA to physical address mapping internally with little direct relationship."
What is 'Constant Linear Velocity' (CLV) and in which devices is it commonly used?,"A method where the disk spins at a constant speed, used in hard disks.","A method where bit density is uniform per track by varying rotational speed, used in CD-ROM and DVD-ROM drives.",A method to keep data rate constant by decreasing bit density from inner to outer tracks.,A technique to optimize seek time in HDDs.,A form of error correction used in NVM devices.,B,"Constant Linear Velocity (CLV) is a device-recording method where bit density is uniform per track, achieved by varying the rotational speed. This method is used in CD-ROM and DVD-ROM drives."
How does 'Constant Angular Velocity' (CAV) operate in hard disks?,The disk rotation speed is varied to keep bit density constant across tracks.,The read-write head maintains a constant velocity relative to the track.,"The medium spins at a constant velocity, and bit density decreases from inner to outer tracks to maintain a constant data rate.",It increases the number of sectors per track from inner to outer zones.,It is a data compression technique applied to disk sectors.,C,"In Constant Angular Velocity (CAV), the disk rotation speed is constant. To maintain a constant data rate, bit density decreases from inner to outer tracks. This method is used in hard disks."
What is a primary responsibility of the Operating System concerning hardware like HDDs?,To physically connect hardware components.,To optimize application specific settings.,To use hardware efficiently.,To exclusively manage user access permissions.,To develop new hardware drivers.,C,The text states that 'OS responsibility: use hardware efficiently.'
What are the two primary goals for the OS when managing Hard Disk Drives (HDDs)?,Maximize rotational latency and minimize seek time.,Minimize access time and maximize data transfer bandwidth.,Maximize disk space utilization and minimize power consumption.,Minimize read errors and maximize write speeds.,Maximize drive longevity and minimize noise levels.,B,"For HDDs, the OS aims to 'minimize access time, maximize data transfer bandwidth.'"
Which two components contribute to the access time for Hard Disk Drives (HDDs) or mechanical storage?,Throughput and latency.,Seek time and rotational latency.,Bandwidth and transfer rate.,Read time and write time.,Cache hit time and cache miss time.,B,Access time for HDDs is composed of 'seek time' and 'rotational latency.'
"In HDD access time, what does ""seek time"" refer to?",The time for the platter to rotate the desired sector to the head.,The time for data to be transferred from the disk to memory.,The time for the device arm to move heads to the desired cylinder.,The time taken for the read/write head to settle on a track.,The total time from request issuance to completion.,C,Seek time is defined as 'time for device arm to move heads to desired cylinder.'
"What is ""rotational latency"" in the context of HDD access time?",The time it takes for the disk controller to process a request.,The time for the device arm to move heads to the desired cylinder.,The additional time for the platter to rotate the desired sector to the head.,The time required to read a block of data after the head is positioned.,The delay caused by network congestion during data transfer.,C,Rotational latency is defined as 'additional time for platter to rotate desired sector to head.'
"How is ""device bandwidth"" defined for storage devices?",The maximum speed at which data can be transferred to/from the device.,The total bytes transferred divided by the total time from the first request to the last transfer completion.,The number of I/O operations per second (IOPS).,The amount of data that can be stored on the device.,The time delay between data requests.,B,Device bandwidth is defined as 'total bytes transferred / total time (first request to last transfer completion).'
How can access time and bandwidth for HDDs be improved according to the text?,By increasing the rotational speed of the platters.,By reducing the physical size of the disk.,By managing the order of storage I/O requests.,By upgrading the memory modules connected to the disk.,By using a higher capacity power supply.,C,"The text states, 'Improve access time and bandwidth: manage order of storage I/O requests.'"
What happens to new I/O requests if the drive/controller is busy?,They are immediately rejected.,They are placed in a queue.,They are redirected to another drive.,They cause a system halt.,They are buffered in CPU registers.,B,The text states: 'Drive/controller busy: new requests placed in queue.'
What is a key benefit of having a queue of pending I/O requests for a device driver?,It simplifies hardware diagnostics.,It allows device drivers to improve performance by ordering requests.,It reduces the need for system calls.,It guarantees immediate service for all requests.,It ensures data integrity through checksums.,B,A 'Queue of requests: allows device drivers to improve performance via ordering (avoiding head seeks).'
How do modern drives typically handle Logical Block Addresses (LBAs) compared to past HDD interfaces?,"Modern drives expose track/head controls to the host, while past interfaces mapped LBAs.","Modern drives perform disk scheduling externally, while past interfaces did it internally.","Modern drives do not expose physical controls, mapping LBAs to physical addresses internally.",Modern drives require manual LBA to physical address mapping by the OS.,"Modern drives rely solely on FCFS scheduling, unlike past interfaces.",C,"The text explains: 'Modern drives: do not expose these controls, map LBA to physical addresses internally.'"
Which of the following is NOT listed as a goal for current disk scheduling?,Fairness.,Timeliness.,Maximizing rotational latency.,Optimizations like bunching sequential reads/writes.,None of the above (all are goals).,C,"Current disk scheduling goals include 'fairness, timeliness, optimizations'. Maximizing rotational latency is an undesirable outcome, not a goal."
What type of I/O do drives perform best with?,Random I/O.,Mixed read/write I/O.,Sequential I/O.,Small block I/O.,Concurrent I/O from many processes.,C,The text states: 'Drives perform best with sequential I/O.'
"On modern drives, how is the physical location of data approximated using Logical Block Addresses (LBAs)?",LBAs are randomly assigned to physical addresses.,Increasing LBAs mean decreasing physical addresses.,Absolute knowledge of physical locations is always possible through LBAs.,"Increasing LBAs generally mean increasing physical addresses, and close LBAs equate to physical proximity.","LBAs only indicate logical file structure, not physical location.",D,The text states: 'Approximation: increasing LBAs mean increasing physical addresses; close LBAs equate to physical proximity.'
What is considered the simplest disk scheduling algorithm?,SCAN.,C-SCAN.,"First-Come, First-Served (FCFS).",Shortest Seek Time First (SSTF).,LOOK.,C,"The text identifies 'First-Come, First-Served (FCFS) or FIFO' as the simplest disk scheduling."
Which statement accurately describes the characteristics of FCFS disk scheduling?,It is intrinsically unfair and provides the fastest service.,It is intrinsically fair but generally does not provide the fastest service.,It prioritizes urgent requests and minimizes head movement.,It guarantees no starvation but can have high overhead.,It is complex to implement but highly efficient for all workloads.,B,FCFS is 'Intrinsically fair' but 'Generally does not provide fastest service.'
"Based on the example provided, what is a primary problem with FCFS disk scheduling?",It causes excessive rotational latency.,It results in wild swings and high total head movement.,"It prioritizes writes over reads, leading to read delays.",It requires complex LBA to physical address mapping.,It is not suitable for multiprogramming systems.,B,The example highlights 'Problem: wild swing' and the high 'Total head movement: 640 cylinders' as issues with FCFS.
Which of the following best describes the SCAN disk scheduling algorithm?,It services requests strictly in the order they arrive.,It services requests based on their proximity to the current head position.,"The disk arm starts at one end, moves to the other servicing requests, then reverses direction upon reaching the end.","The disk arm moves from one end to the other servicing requests, then immediately returns to the beginning without servicing.",It prioritizes requests based on a configured deadline.,C,"The SCAN algorithm is defined as: 'disk arm starts at one end, moves to other, servicing requests. Reaches other end: direction reversed, servicing continues.'"
The SCAN algorithm is also commonly known by what other name?,FIFO algorithm.,LBA algorithm.,Elevator algorithm.,Deadline algorithm.,Shortest Seek Time First.,C,The text states: 'Also called elevator algorithm.'
"In the SCAN algorithm, which type of request is serviced almost immediately?",The request at the very end of the disk.,The request that has been waiting the longest.,The request just in front of the head.,The request that is furthest from the head.,All requests are serviced with equal immediacy.,C,The text mentions: 'Request just in front of head: serviced almost immediately.'
"In the SCAN algorithm, when does a request ""just behind the head"" get serviced?",Immediately after the current request.,"After the arm moves to the end, reverses, and comes back.",Only if it is a read request.,Only if there are no other requests in front of the head.,"Never, due to starvation.",B,"The text states: 'Request just behind head: waits until arm moves to end, reverses, comes back.'"
What is the defining characteristic of Circular SCAN (C-SCAN) scheduling that distinguishes it from SCAN?,It services requests in a fixed circular order without reversing direction.,It prioritizes requests based on their age.,It only services requests on its outward trip and immediately returns to the beginning without servicing.,"It only services read requests, ignoring writes.",It moves the head randomly to avoid starvation.,C,"C-SCAN 'Moves head from one end to other, servicing requests. Reaches other end: immediately returns to beginning of disk, no servicing on return trip.'"
How does C-SCAN conceptually treat the cylinders of a disk?,As a linear list.,As a random collection.,As a circular list.,As a priority queue.,As a stack.,C,C-SCAN 'Essentially treats cylinders as circular list.'
"What happens if a disk queue has only one outstanding request, regarding disk scheduling algorithms?",SCAN and C-SCAN perform significantly better.,All algorithms behave like FCFS.,The request is always delayed.,Only specialized algorithms like deadline scheduler can handle it.,The system enters an idle state until more requests arrive.,B,The text provides an example: 'queue with one outstanding request => all algorithms behave like FCFS.'
Which two disk scheduling algorithms are generally better for heavy disk loads and less likely to cause starvation compared to FCFS?,FCFS and SSTF.,SCAN and C-SCAN.,NOOP and CFQ.,FIFO and LBA.,Read-priority and Write-priority.,B,"The text states: 'SCAN and C-SCAN: better for heavy disk load, less likely to cause starvation.'"
"What problem did the Linux `deadline` scheduler specifically aim to address, which was still possible with SCAN and C-SCAN?",High CPU utilization.,Excessive rotational latency.,Starvation.,Inefficient sequential I/O.,Difficulty in mapping LBAs.,C,The text states: 'Starvation still possible: Linux created deadline scheduler.'
How does the Linux `deadline` scheduler organize its I/O requests for reads and writes?,It maintains a single queue for all requests sorted by LBA.,"It maintains separate read and write queues, giving reads priority.",It only processes requests based on their arrival time.,It creates a queue for each individual process.,It merges read and write requests into a single FCFS queue.,B,The deadline scheduler 'Maintains separate read and write queues. Gives reads priority (processes more likely to block on read).'
How does the `deadline` scheduler incorporate a C-SCAN-like mechanism?,By moving the head back and forth continuously.,By returning to the start of the disk after servicing without processing.,By sorting its LBA-ordered queues and sending requests in batch in LBA order.,By only servicing requests older than a certain age.,By maintaining a real-time queue.,C,The deadline scheduler's 'Queues sorted in LBA order (implements C-SCAN). All I/O requests sent in batch in LBA order.'
How does the Linux `deadline` scheduler prevent long-term starvation of FCFS requests?,It immediately processes all FCFS requests before any LBA-sorted requests.,It checks if FCFS requests are older than a configured age and prioritizes them for the next batch.,It merges FCFS and LBA queues into a single optimized queue.,It assigns higher priority to FCFS requests by default.,It only allows a fixed number of LBA-sorted requests per batch.,B,The deadline scheduler 'Checks after each batch: FCFS requests older than configured age (default 500 ms)? If so: LBA queue (read/write) with old request selected for next batch.'
For which type of systems and storage is the NOOP I/O scheduler preferred in Linux?,Systems with high rotational latency HDDs.,CPU-bound systems using fast storage like NVM devices.,Servers primarily handling large sequential write operations.,Desktop systems requiring low power consumption.,Systems with a mix of many small random I/O requests.,B,The NOOP scheduler is 'preferred for CPU-bound systems using fast storage (NVM devices).'
The Completely Fair Queueing (CFQ) scheduler is the default for what type of drives in Linux?,SCSI drives.,NVMe drives.,SATA drives.,USB flash drives.,Network drives.,C,CFQ is the 'default for SATA drives.'
What are the three types of queues maintained by the Completely Fair Queueing (CFQ) scheduler?,"Read, Write, and System.","Input, Output, and Error.","Real time, Best effort, and Idle.","High priority, Medium priority, and Low priority.","FCFS, SCAN, and C-SCAN.",C,"CFQ maintains three queues: 'real time, best effort (default), idle.'"
"What is the priority order among CFQ's queues, and what is a potential issue despite this system?",Idle > Best effort > Real time; starvation is impossible.,Best effort > Real time > Idle; starvation is impossible.,Real time > Best effort > Idle; starvation is possible.,Idle > Real time > Best effort; starvation is possible.,All queues have equal priority; no starvation.,C,CFQ's queues have 'exclusive priority (real time > best effort > idle); starvation possible.'
How does CFQ attempt to minimize seek time by leveraging historical data?,It always processes requests from the queue with the most pending requests.,"It anticipates if a process will issue more I/O and idles, ignoring other requests, assuming locality of reference.",It reorders all requests globally based on LBA before processing.,It only allows one process to issue I/O at a time.,It uses a complex mathematical model to predict future requests.,B,"CFQ 'Uses historical data: anticipates if process will issue more I/O requests soon. If so: idles waiting for new I/O, ignores other queued requests (minimizes seek time, assumes locality of reference per process).'"
What is the primary goal of disk-scheduling algorithms for Hard Disk Drives (HDDs)?,Maximize data transfer rates,Minimize disk head movement,Prioritize write requests,Ensure FCFS for all requests,Balance read and write operations,B,Disk-scheduling algorithms for HDDs are designed to minimize disk head movement to improve performance.
Which scheduling policy is commonly employed by Non-Volatile Memory (NVM) devices?,Shortest Seek Time First (SSTF),SCAN algorithm,"First-Come, First-Served (FCFS)",Circular SCAN (C-SCAN),Elevator algorithm,C,"NVM devices, having no moving disk heads, commonly use a simple FCFS policy."
Describe the characteristics of the Linux NOOP scheduler.,It prioritizes read requests over write requests.,It uses an FCFS policy and merges adjacent requests.,It employs a complex algorithm to minimize wear.,It is designed specifically for HDDs with moving heads.,It reorders requests based on data type.,B,"The Linux NOOP scheduler applies an FCFS policy and merges adjacent requests, primarily used for NVM devices."
How do read and write service times typically behave on NVM devices?,Both read and write service times are uniform.,Both read and write service times are non-uniform.,"Read service time is uniform, while write service time is not uniform.","Write service time is uniform, while read service time is not uniform.",Service times depend entirely on the file system in use.,C,"NVM devices exhibit uniform read service times, but non-uniform write service times due to flash memory properties."
What is a characteristic behavior of some SSD schedulers regarding I/O requests?,They merge all adjacent requests.,They prioritize write requests over read requests.,They merge only adjacent write requests and service all read requests in FCFS order.,They service all requests in a strict FCFS order.,They attempt to minimize logical block address (LBA) jumps for all operations.,C,Some SSD schedulers specifically merge only adjacent write requests while servicing all read requests in FCFS order.
How is random-access I/O performance typically measured?,Gigabytes per second (GB/s),Milliseconds (ms) latency,Input/output operations per second (IOPS),Megabytes per second (MB/s) throughput,Cycles per instruction (CPI),C,Random-access I/O performance is measured in Input/Output Operations Per Second (IOPS).
What is a direct consequence of random access I/O on Hard Disk Drives (HDDs)?,Reduced platter spin speed,Increased cache hit rates,Significant disk head movement,Uniform write service times,Decreased power consumption,C,"Random access I/O causes considerable disk head movement on HDDs, impacting performance."
How do the IOPS capabilities of NVM devices compare to HDDs for random access I/O?,"HDDs achieve hundreds of thousands IOPS, while NVMs achieve hundreds.",NVMs achieve similar IOPS to HDDs.,"NVMs are much faster, achieving hundreds of thousands IOPS compared to HDDs' hundreds.","HDDs are much faster, achieving hundreds of thousands IOPS compared to NVMs' hundreds.",IOPS is not a relevant metric for NVM devices.,C,"NVM devices are much faster for random access I/O, achieving hundreds of thousands of IOPS, whereas HDDs only achieve hundreds."
"Regarding raw sequential throughput, what is the advantage of NVM devices compared to HDDs?",NVM devices show a significant advantage.,NVM devices show a slight advantage.,NVM devices show less advantage because HDD head seeks are minimized for sequential I/O.,HDDs are significantly better for sequential throughput.,There is no difference in sequential throughput.,C,"NVM devices show less advantage for raw sequential throughput because HDD head seeks are already minimized for sequential access, narrowing the performance gap."
"For sequential reads, what is the approximate performance advantage of NVM over HDD?",Equivalent performance,2x advantage,5x advantage,10x advantage,NVM is slower,D,Sequential reads on NVM devices offer a performance equivalent to a 10x advantage over HDDs.
How does writing to NVM compare to reading from NVM in terms of speed?,Writing is generally faster.,Writing is generally slower.,Writing and reading speeds are always equivalent.,Writing speed is much faster only on new devices.,Writing speed is only slower when the device is empty.,B,"Writing to NVM is generally slower than reading, which decreases its overall performance advantage."
Which type of storage device typically maintains more consistent write performance throughout its lifespan?,NVM devices,Hybrid SSD/HDD drives,Hard Disk Drives (HDDs),Optical drives,Magnetic tapes,C,"HDD write performance remains consistent throughout the device's life, unlike NVM, which varies."
What factors can cause NVM write performance to vary?,Platter rotation speed and read/write head wear,"Device fullness (garbage collection, over-provisioning) and ""wear""",Temperature fluctuations and power supply stability,"The type of interface cable used (SATA, NVMe)",The number of active applications concurrently accessing the drive,B,"NVM write performance varies based on device fullness (influenced by garbage collection and over-provisioning) and the extent of ""wear""."
How does wear affect the performance of an NVM device?,A worn NVM device performs better than a new one due to optimization.,Wear has no significant impact on NVM performance.,A worn NVM device performs much worse than a new device.,"Wear primarily affects read performance, not write performance.","Wear only impacts the device's lifespan, not its performance.",C,A worn NVM device exhibits much worse performance compared to a new device.
"How can the lifespan and performance of an NVM device be improved, particularly regarding deleted files?",By performing regular defragmentation.,By increasing the over-provisioning space manually.,"By ensuring the file system informs the device when files are deleted, allowing it to erase blocks.",By regularly reformatting the device.,By always filling the device to maximum capacity.,C,"Informing the NVM device when files are deleted allows it to proactively erase blocks, improving lifespan and performance by optimizing garbage collection."
What is the primary purpose of Garbage Collection (GC) in NVM devices?,To compress data for storage efficiency.,To minimize disk head movement.,To reclaim space from invalid data.,To encrypt data for security.,To manage the file system directory structure.,C,Garbage collection must occur to reclaim space from invalid data on NVM devices.
Under what circumstances is Garbage Collection (GC) likely to occur on an NVM device?,Only during system shutdown.,When the device is completely empty.,"When the device is under random read/write load, is full but still has free space.",Only when new firmware is installed.,When sequential reads are being performed exclusively.,C,GC is triggered when an NVM device is under random read/write load and is full but still has free space (requiring invalid data to be cleaned up).
"When a single write request to an NVM device triggers garbage collection, which sequence of operations best describes the typical process?",Page write (data) only.,Page read (data) + page write (data).,Page write (data) + one or more page reads (by GC) + one or more page writes (good data from GC blocks).,Block erase (invalid data) + page write (new data).,"Only block erases occur, with no data movement.",C,"A single write request, when combined with garbage collection, involves the initial page write of new data, followed by one or more page reads (to collect valid data from blocks to be erased), and one or more page writes (to move that valid data to overprovisioning space)."
"What is ""write amplification"" in the context of NVM devices?",The process of increasing data redundancy for fault tolerance.,The creation of additional I/O requests by the NVM device itself for tasks like garbage collection and space management.,The rate at which data can be written to the device in MB/s.,The effect of a single write operation spreading across multiple logical blocks.,The process of optimizing write performance by merging sequential writes.,B,"Write amplification refers to the creation of I/O requests by NVM devices (e.g., for garbage collection and space management), which can impact write performance."
What is the main impact of write amplification on NVM device performance?,It primarily enhances read speeds.,It has no significant impact on performance.,It can greatly impact write performance by triggering extra I/Os.,It reduces the lifespan of the device without affecting performance.,It only affects sequential write operations.,C,Write amplification can greatly impact write performance by triggering several extra I/Os with each write request.
What happens in the worst-case scenario due to write amplification?,The device fails immediately.,Read performance degrades to zero.,Several extra I/Os are triggered with each write request.,Data becomes corrupted.,The device automatically switches to read-only mode.,C,"In the worst-case scenario, write amplification can lead to several extra I/Os being triggered with each write request, severely degrading write performance."
"According to the provided text, what is the definition of ""NOOP""?",A measure of random access I/O performance.,A Linux NVM scheduling algorithm that uses FCFS with adjacent requests merged.,The process of reclaiming space from invalid data on an NVM device.,The creation of I/O requests by NVM devices for space management.,A type of non-volatile memory that has no moving parts.,B,"""NOOP"" is defined as a Linux NVM scheduling algorithm that uses FCFS with adjacent requests merged."
"What does ""input/output operations per second"" measure?",The speed of sequential data transfer.,The latency of a single I/O request.,The number of inputs + outputs per second for random access I/O performance.,The power consumption of an I/O device.,The capacity of a storage device.,C,"""Input/output operations per second"" measures random access I/O performance as the number of inputs + outputs per second."
"Which processes are responsible for the creation of additional I/O requests leading to ""write amplification"" in NVM devices?",Only user application write requests.,Only sequential read requests.,Garbage collection and space management.,Disk defragmentation.,Error correction and data redundancy.,C,Write amplification is caused by the creation of I/O requests by NVM devices for garbage collection and space management.
Error detection and correction are considered fundamental to which of the following system components?,"Memory, networking, and storage",Only CPUs and GPUs,User interface and input/output devices,Application software and operating systems,Cloud services and virtual machines,A,"The text states that error detection and correction are fundamental to memory, networking, and storage."
What is the primary purpose of 'error detection'?,To automatically repair corrupted data without intervention,"To determine if a problem, such as data corruption, has occurred",To prevent any errors from ever occurring in a system,To increase the speed of data transfer by skipping error checks,To encrypt data for security purposes,B,"Error detection is defined as determining if a problem has occurred, such as a bit change in DRAM, network packet change, or data block change."
"Upon detecting an issue, what actions can a system take?",Automatically correct the error and proceed as normal without notification,"Halt operation, report the error, or warn of a failing/failed device",Increase processing power to ignore the error,Reformat the entire storage device immediately,Only log the error silently without any user notification,B,"The text states that 'By detecting issue: system can halt operation, report error, warn of failing/failed device.'"
"Which method do memory systems commonly use to detect errors, according to the text?",Cyclic Redundancy Checks (CRCs),Error-Correction Codes (ECC),Parity bits,Hash functions,Modular arithmetic calculations,C,The text explicitly states: 'Memory systems: detect errors using parity bits.'
How many extra bits of memory are required per byte when using parity bits for error detection?,Four,Two,One,Eight,"Zero, it's a software-only solution",C,The text states that parity 'Requires extra bit of memory per byte.'
How is a single-bit error detected using parity bits?,The system retries the operation multiple times until it works,The parity changes and does not match the stored parity,The data block checksum indicates a large discrepancy,A dedicated error correction algorithm fixes the bit automatically,The memory controller halts immediately without further analysis,B,"The text explains: 'Single-bit error: parity changes, does not match stored parity ⇒ detected.'"
Which type of error might go undetected when using only parity bits?,Single-bit errors,Double-bit errors,Triple-bit errors,All multi-bit errors,Errors due to power fluctuations,B,The text explicitly states: 'Double-bit error: might go undetected.' It also notes 'All single-bit errors detected.'
How is parity typically calculated?,By adding up all the bits and dividing by two,By performing a bitwise AND operation on the data,By XORing the bits,By using a complex cryptographic algorithm,By counting the number of zero bits,C,The text states: 'Parity: calculated by XORing bits.'
Parity is described as one form of what general error checking mechanism?,Encryption,Compression,Checksums,Hashing algorithms for security,Data fragmentation,C,The text states: 'Parity: one form of checksums.'
"According to the provided definition, what is a 'checksum'?",A method for encrypting data to prevent unauthorized access,A general term for an error detection and correction code,A specific algorithm used only for network packet verification,A system's total memory capacity,A technique to compress data before storage,B,The definition table defines 'checksum' as: 'General term for an error detection and correction code.'
"How do checksums compute, store, and compare values?",By using cryptographic keys for secure verification,By employing modular arithmetic on fixed-length words,By converting data into a graphical representation,By performing a byte-by-byte comparison with a master copy,By physically checking the integrity of storage media,B,"The text states: 'Checksums: use modular arithmetic to compute, store, compare values on fixed-length words.'"
What is a 'cyclic redundancy check (CRC)'?,A method for encrypting data streams,An error-detection method using a hash function to detect multiple-bit errors,A technique to recover lost data from damaged storage drives,A process to optimize disk space by eliminating redundant files,A software utility for checking system file versions,B,The definition table defines 'cyclic redundancy check (CRCs)' as: 'Error-detection method using a hash function to detect multiple-bit errors.'
Which type of errors are CRCs particularly effective at detecting?,Only single-bit errors,Errors caused by physical shock to the device,Multiple-bit errors,Software bugs in the operating system,User input mistakes,C,The text states: 'CRCs: use hash function to detect multiple-bit errors.'
What is the primary function of an 'Error-Correction Code (ECC)'?,"To detect errors only, without the ability to correct them",To encrypt sensitive data for secure transmission,To compress data before storing it on a drive,To detect and correct problems in data,To perform routine system diagnostics,D,The text defines ECC as detecting and correcting problems. The definition table also states 'can correct errors.'
How does ECC achieve its error correction capability?,By requiring manual user intervention for every error,By simply re-reading the data until it's correct,By using algorithms and extra storage to identify and calculate correct values,By comparing the data with a remote server's copy,By disabling the faulty memory or storage sector entirely,C,The text states: 'Correction: uses algorithms and extra storage.' It further explains that ECC 'contains enough info to identify changed bits and calculate correct values (if few bits corrupted).'
What factors cause Error-Correction Code (ECC) codes to vary?,The color of the storage device and its manufacturer,The operating system version and the amount of available RAM,The extra storage needed and the number of errors correctable,The current power consumption and ambient temperature,The user's preference settings and security protocols,C,"The text states: 'Codes vary: based on extra storage needed, number of errors correctable.'"
On which type of storage device do disk drives typically use ECC?,Per-file ECC,Per-block ECC,Per-sector ECC,Per-track ECC,Per-platter ECC,C,The text specifies: 'Disk drives: use per-sector ECC.'
On which type of storage device do flash drives typically use ECC?,Per-sector ECC,Per-cluster ECC,Per-block ECC,Per-page ECC,Per-chip ECC,D,The text specifies: 'Flash drives: use per-page ECC.'
What happens during a read operation if the recalculated ECC value does not match the stored ECC value?,The system automatically encrypts the data,"The data is confirmed as corrupted, and the storage media may be bad",The read operation is immediately aborted without further action,The stored ECC value is updated to match the recalculated one,The data is compressed to save space,B,"The text states: 'Sector/page read: ECC recalculated, compared with stored value. Mismatch: data corrupted, storage media may be bad.'"
What is a 'soft error' in the context of ECC?,An error that leads to permanent data loss,An error that can be recovered by retrying the operation,"An error in software logic, not data integrity",An error that requires manual user intervention to fix,An error that corrupts the operating system files,B,The definition table defines 'soft error' as: 'Recoverable error by retrying the operation.' The text also says ECC 'Reports recoverable soft error.'
What is signaled when an ECC system encounters too many changes to correct?,A recoverable soft error,A non-correctable hard error,A parity mismatch error,A cyclic redundancy check failure,A checksum validation success,B,"The text states: 'Too many changes, ECC cannot correct: non-correctable hard error signaled.'"
What is a 'hard error' according to the provided text?,An error that can be fixed by simply refreshing the memory,An error that is easily recoverable by software algorithms,"An unrecoverable error, possibly resulting in data loss","An error that only affects system performance, not data integrity","An error related to a faulty power supply, not data",C,"The definition table defines 'hard error' as: 'Unrecoverable error, possibly resulting in data loss.'"
Who or what automatically performs ECC processing during read/write operations?,The user through a diagnostic tool,The operating system software,The storage device controller,The CPU's arithmetic logic unit,Network interface card,C,The text states: 'Controller automatically performs ECC processing on read/write.'
Error detection and correction capabilities are frequently differentiators between which product categories?,Desktop and laptop computers,Consumer and enterprise products,Hardware and software components,Open-source and proprietary systems,Analog and digital devices,B,The text mentions: 'Error detection/correction: frequently differentiators between consumer and enterprise products.'
"Beyond storage drives, ECC is also used in some systems for what other purposes?",User interface responsiveness and graphical rendering,Network routing and packet prioritization,DRAM error correction and data path protection,CPU clock speed regulation and power management,Sound card audio processing,C,The text notes: 'ECC: used in some systems for DRAM error correction and data path protection.'
Which of the following are key responsibilities of the Operating System (OS) regarding storage device management?,"Drive initialization, network protocol configuration, and user account management.","Booting from drive, bad-block recovery, and application software development.","Drive initialization, booting from drive, and bad-block recovery.","CPU scheduling, memory allocation, and drive initialization.","File system backup, driver signing, and power management.",C,"The OS is responsible for drive initialization, booting from the drive, and bad-block recovery, among other aspects of storage device management."
"What is the initial state of a new storage device, such as a magnetic platter or uninitialized semiconductor cells, before data can be stored?",Partially formatted with a basic file system.,Pre-configured with a master boot record.,"A blank slate, requiring specific preparation.",Ready for direct data storage without any prior steps.,Filled with default operating system files.,C,A new storage device is described as a 'blank slate' that needs to be prepared before data can be stored.
"What is the process called that divides a storage device into sectors or initializes NVM pages and creates an FTL, preparing it for data storage?",Logical formatting.,Partitioning.,Volume creation.,Low-level formatting.,File system mounting.,D,The process of dividing a device into sectors or initializing NVM pages and creating an FTL is called low-level formatting (or physical formatting).
What specialized data structure does low-level formatting fill each storage location (sector/page) with?,User data and application headers.,File system allocation tables.,"A header, a data area, and a trailer.",Directory entries and metadata.,Volume labels and partition tables.,C,"Low-level formatting fills the device with a special data structure for each storage location, consisting of a header, data area, and trailer."
What kind of information is typically contained within the header and trailer of a storage sector or NVM page?,User data checksums and file names.,"Controller information, such as sector/page number and error detection/correction code.",Operating system boot code and partition details.,Application-specific metadata and timestamps.,File permissions and access control lists.,B,Headers and trailers contain controller information like sector/page number and error detection/correction code.
At what stage are most storage drives typically low-level formatted?,During operating system installation.,When a new file system is created by the user.,At the factory during the manufacturing process.,Immediately before the first data write operation.,Only when bad blocks are detected after extensive use.,C,Most drives are low-level formatted at the factory as part of the manufacturing process.
What is a key benefit for the manufacturer of low-level formatting drives at the factory?,It allows the end-user to select custom sector sizes.,It enables testing the device and initializing the mapping from logical block numbers to defect-free sectors/pages.,It pre-installs the operating system's bootloader.,It encrypts the entire drive for security.,It prepares the drive for immediate raw I/O access.,B,Factory low-level formatting enables the manufacturer to test the device and initialize the mapping from logical block numbers to defect-free sectors/pages.
Which of the following are the two common sector sizes mentioned for storage devices?,256 bytes and 2KB.,512 bytes and 4KB.,1KB and 8KB.,2KB and 16KB.,512 bytes and 1MB.,B,The common sector sizes mentioned are 512 bytes and 4KB.
What is an advantage of using a larger sector size on a storage device?,It always allows for more efficient random access.,"It requires more headers/trailers, increasing redundancy.","It results in fewer sectors per track, fewer headers/trailers, and more space for user data.",It simplifies the low-level formatting process significantly.,It enables the OS to handle multiple sector sizes simultaneously.,C,"A larger sector size leads to fewer sectors per track, fewer headers/trailers, and thus more space available for user data."
Which of the following describes the first step an OS takes to record its own data structures on a storage device?,Logical formatting.,Volume creation.,Partitioning.,Boot block writing.,Low-level formatting.,C,"The first step is Partitioning, where the device is divided into one or more groups of blocks/pages."
What is the primary way an OS treats each partition created on a storage device?,"As a single, continuous array of raw data.","As a separate, independent device.",As a temporary cache for system files.,As a redundant backup for other partitions.,As a network-attached storage unit.,B,"The OS can treat each partition as a separate device, allowing for different uses or file systems on each."
Which Linux command is specifically used for managing partitions on a storage device?,mkfs,mount,fdisk,lsblk,df,C,The Linux `fdisk` command is used to manage partitions.
"After reading partition information, what does the OS typically create in locations like `/dev` in Linux?",Backup copies of the partition table.,Hidden system files for each partition.,Device entries corresponding to the partitions.,Temporary swap files.,User home directories.,C,"Once partition information is read, the OS creates device entries (e.g., `/dev` in Linux) for each partition."
What is the purpose of the `/etc/fstab` configuration file in Linux?,To store user passwords and permissions.,To manage network interface settings.,To tell the OS to mount each partition containing a file system at a specified location with mount options.,To log system errors and warnings.,To configure hardware device drivers.,C,The `/etc/fstab` file is used to configure how and where file systems should be mounted automatically by the OS.
Which of the following best defines 'mounting' a file system?,Erasing all data on a partition.,Making a file system available for use by logically attaching it to the root file system.,Creating a new partition on a storage device.,Performing a low-level format on a volume.,Encrypting the contents of a storage volume.,B,"Mounting a file system means making it available for use, typically by logically attaching it to the root file system hierarchy."
"When a file system is placed directly within a partition, what is the resulting entity ready to be mounted implicitly referred to as?",A raw disk.,A cluster.,A volume.,A boot block.,A sector.,C,"When a file system is placed directly within a partition, it is implicitly considered a volume ready to be mounted."
Which of the following is an example of explicit volume creation and management?,Formatting a single partition with a file system.,Using multiple partitions or devices as a RAID set with one or more file systems spread across them.,Directly accessing a disk as a raw array of blocks.,Mounting a CD image file to a directory.,Creating an empty directory on an existing file system.,B,"Explicit volume creation includes scenarios like using multiple partitions/devices as a RAID set, allowing file systems to spread across them."
Which of the following is an example of a system where volume management and file system are integrated?,LVM2 in Linux.,Traditional Windows NTFS.,ZFS.,FAT32 on older systems.,Standard UNIX file systems.,C,ZFS is noted for its integrated volume management and file system capabilities.
"What is the third step an OS takes to record its own data structures on a device, following partitioning and volume creation?",Bad-block recovery.,Device driver installation.,Logical formatting (creation of a file system).,Firmware update.,Master Boot Record (MBR) writing.,C,"The third step is logical formatting, which involves the creation of a file system by storing initial file-system data structures."
What initial data structures does the OS store on a device during logical formatting?,User files and application data.,Operating system executable code.,Maps of free/allocated space and an initial empty directory.,Hardware configuration settings.,Network access logs.,C,"During logical formatting, the OS stores initial file-system data structures such as maps of free/allocated space and an initial empty directory."
How does a partition labeled for boot contribute to the overall computer's file system structure?,It solely contains user data and applications.,It serves as a temporary storage for system logs.,It is used to establish the root of the file system.,It acts as a backup for other partitions.,It bypasses the need for any other partitions.,C,"The partition labeled for boot is crucial as it is used to establish the root of the file system, from which all other mounted volumes are accessed."
What constitutes a computer's 'file system' once all necessary volumes are mounted?,Only the physical storage devices themselves.,"A single, unchangeable system partition.","All mounted volumes, collectively forming the accessible storage hierarchy.",Only the swap space partition.,A network-attached storage device.,C,"The computer's 'file system' consists of all mounted volumes, providing a unified view of the storage."
How do Windows and Linux typically differ in how they name or structure access to separately mounted file systems?,Windows uses a single tree structure; Linux uses separately named letters.,"Windows separately names them via letters (e.g., C:, D:); Linux mounts them within a tree structure.","Both use only a single, unified tree structure.",Both use only separately named drive letters.,Windows has no concept of mounted file systems; Linux treats everything as raw disk.,B,"Windows typically uses separate drive letters (C:, D:, etc.), while Linux mounts file systems into a single hierarchical tree structure, usually rooted at '/'. "
"In the context of storage management, what is a 'cluster'?",A single physical sector on a disk.,A logical group of CPU cores.,"In Windows storage, a power-of-2 number of disk sectors collected for I/O optimization.",A type of network storage device.,A specialized partition for swap space.,C,"A cluster is defined as a power-of-2 number of disk sectors collected for I/O optimization, particularly in Windows storage."
What is the primary difference in unit of I/O between device I/O and file system I/O?,"Device I/O uses bytes, file system I/O uses bits.","Device I/O uses sectors, file system I/O uses pages.","Device I/O uses blocks, file system I/O uses clusters.","Device I/O uses tracks, file system I/O uses cylinders.","Device I/O uses files, file system I/O uses directories.",C,"Device I/O operates via blocks, while file system I/O operates via clusters, which are larger chunks of blocks."
What is the benefit of file systems grouping blocks into larger chunks called clusters?,It increases the number of random-access operations.,It assures more sequential-access and fewer random-access characteristics.,It reduces the overall storage capacity of the disk.,It makes disk defragmentation impossible.,It removes the need for error detection codes.,B,"Grouping blocks into clusters assures more sequential-access and fewer random-access characteristics, improving I/O efficiency."
How do file systems typically optimize for reducing HDD head seeks?,By distributing file contents randomly across the disk.,By grouping file contents near their metadata.,By always placing new files at the very beginning of the disk.,By using smaller sector sizes for all operations.,By never using a buffer cache.,B,"File systems group file contents near their metadata to reduce HDD head seeks, improving performance."
"Which term describes direct access to a secondary storage device as an array of logical blocks, bypassing file-system data structures?",Mounted volume.,Logical partition.,Raw disk.,Bootable media.,Clustered storage.,C,"A 'raw disk' refers to using a partition as a large sequential array of logical blocks, without file-system data structures."
For what purposes are raw disk access and 'raw I/O' typically used?,Standard user file storage and retrieval.,Operating system kernel loading.,Swap space and some database systems that control exact record location.,Buffering and caching of frequently accessed data.,Network file sharing.,C,"Raw I/O is used for specific purposes like swap space or by certain database systems that require precise control over record placement, bypassing file system services."
Which of the following file-system services are bypassed when an application uses raw I/O?,Hardware error detection and correction.,Physical sector allocation by the controller.,"Buffer cache, file locking, and space allocation.",Basic read and write operations.,Power management and device initialization.,C,"Raw I/O bypasses file-system services such as buffer cache, file locking, prefetching, space allocation, file names, and directories."
"How can applications in Linux achieve functionality similar to raw I/O, even though direct raw I/O is generally not allowed?",By using the `chmod` command to change device permissions.,By mounting the device with the `noatime` option.,By utilizing the `DIRECT` flag with the `open()` system call.,By enabling transparent huge pages.,By disabling the kernel's virtual memory subsystem.,C,"In Linux, similar access to raw I/O can be achieved using the `DIRECT` flag with the `open()` system call."
What is the initial program a computer must run when it starts (powered up or rebooted)?,The user interface.,An application program.,An initial bootstrap loader.,The full operating system kernel.,A network discovery tool.,C,"When a computer starts, it must have an initial program to run, which is typically a simple bootstrap loader."
"Where is the initial, simple bootstrap loader typically stored?",On the hard disk's user data partition.,In the system's RAM.,"In NVM flash memory firmware on the motherboard, mapped to a known memory location.",On a connected USB drive.,In the CPU's registers.,C,"The initial bootstrap loader is stored in NVM flash memory firmware on the motherboard, mapped to a known memory location."
What are the primary functions of the tiny initial bootstrap loader during the computer startup process?,Loading device drivers and launching user applications.,"Initializing CPU registers, device controllers, and main memory contents.",Connecting to the internet and updating software.,Formatting the hard drive and creating partitions.,Performing a comprehensive diagnostic check of all hardware.,B,"The tiny initial bootstrap loader initializes CPU registers, device controllers, and main memory contents."
"After the initial bootstrap loader, where is the full bootstrap program typically stored?",In a temporary file on the root file system.,In 'boot blocks' at a fixed location on secondary storage.,In the CPU's L1 cache.,On a remote server accessed via network.,Within the operating system's kernel itself.,B,"The tiny bootstrap loader brings in the full bootstrap program, which is stored in 'boot blocks' at a fixed location on secondary storage."
What is a 'boot disk' or 'system disk'?,Any storage device used for data backup.,A storage device with a boot partition and kernel to load for booting.,A removable storage device like a USB drive.,A disk used exclusively for swap space.,A disk that only contains application software.,B,A 'boot disk' or 'system disk' is a device with a boot partition containing the OS kernel to load for booting.
What action does the Bootstrap NVM code perform early in the boot process?,It loads the entire operating system directly into RAM.,It instructs the storage controller to read boot blocks into memory and then executes that code.,It initializes all network interfaces and connects to the internet.,It performs a full scan for bad blocks on the hard drive.,It launches the graphical user interface.,B,The Bootstrap NVM code instructs the storage controller to read the full bootstrap program from boot blocks into memory and then executes that code.
How is the full bootstrap program more sophisticated than the initial bootstrap loader?,It can format an entire disk from scratch.,It includes a complete web browser.,It loads the entire OS from a non-fixed location and starts the OS.,It can repair hardware failures.,It bypasses the need for any device drivers.,C,The full bootstrap program is more sophisticated because it can load the entire operating system from a non-fixed location on disk and then start the OS.
"In the Windows boot process, what is the 'Master Boot Record (MBR)'?",The main directory of the user's files.,The primary configuration file for all hardware.,"The Windows boot code, stored in the first logical block/page of the hard disk/NVM device.",A log file of all boot attempts.,A utility for disk defragmentation.,C,"The Master Boot Record (MBR) is the Windows boot code, typically found in the first logical block or page of the boot device."
What information does the Master Boot Record (MBR) contain in the Windows boot process?,Detailed user preferences and network settings.,Only the version number of the operating system.,"Boot code, a table listing partitions, and a flag for the boot partition.",A complete copy of the operating system kernel.,Device drivers for all peripherals.,C,"The MBR contains boot code, a table listing the partitions, and a flag indicating which partition is the boot partition."
What is the 'boot sector' in the Windows boot process?,Any sector that holds a user file.,The last sector on a hard drive.,"The first sector/page of the identified boot partition, which directs to the kernel.",A sector specifically for storing error logs.,A sector containing only random data for security.,C,"The boot sector is the first sector/page of the boot partition, containing code that directs the system to the kernel."
Which type of disk failure is more frequent than a complete disk failure?,Power supply unit malfunction.,Entire system memory corruption.,One or more sectors becoming defective (bad blocks).,Processor overheating.,Network interface card failure.,C,It is more frequent for one or more sectors to become defective (bad blocks) than for a complete disk failure.
What is true about bad blocks regarding new disks from the factory?,New disks are always completely free of bad blocks.,Most disks come from the factory with some bad blocks.,Bad blocks only develop after extensive use.,Factory bad blocks are immediately visible to the OS.,Only refurbished disks have factory bad blocks.,B,The text states that 'Most disks come from factory with bad blocks.'
"How were bad blocks typically handled on older disks (e.g., those with IDE controllers)?","Automatically remapped by the disk controller, invisible to the OS.","Through sector slipping, shifting all subsequent logical blocks.","Manually, often by scanning the disk during formatting and flagging blocks as unusable, or running special programs.","By duplicating all data on a separate, fault-tolerant partition.","They were simply ignored, leading to data corruption.",C,"Older disks required manual handling of bad blocks, often involving scanning during formatting or running special utilities like Linux `badblocks` to flag them."
What happens to data located on a bad block that goes bad during operation on older disk systems?,It is automatically moved to a spare sector.,It is usually lost.,It is immediately restored from a cloud backup.,The file system reconstructs it from parity information.,The OS reboots and attempts a full disk recovery.,B,"On older systems, data on bad blocks that occur during operation is usually lost, requiring manual recovery from backups if available."
Which mechanism do more sophisticated disks use for bad-block recovery?,Requiring manual user intervention for every bad sector.,A simple list of bad blocks maintained by the OS.,The controller maintaining a list of bad blocks and replacing them logically with spare sectors (sector sparing/forwarding).,Completely ignoring bad blocks to maximize storage space.,Shredding the entire disk when a single bad block is found.,C,"More sophisticated disks use sector sparing (or forwarding), where the controller maintains a list of bad blocks and logically replaces them with spare sectors."
What is a potential downside of the controller-based redirection (sector sparing) for bad blocks?,It significantly increases the drive's storage capacity.,It can invalidate OS disk-scheduling optimizations.,It requires the user to manually intervene for every bad block.,It prevents the disk from being partitioned.,It makes the disk incompatible with older operating systems.,B,"Redirection by the controller, while effective for bad block recovery, could invalidate OS disk-scheduling optimizations because the physical location of data is no longer predictable by the OS scheduler."
"When a bad block is remapped using sector sparing, where does the controller ideally try to find a spare sector?",From a remote network location.,From the very end of the disk.,From the same cylinder as the bad block.,From a different physical drive.,From the operating system's RAM.,C,"Most disks are formatted with few spare sectors in each cylinder, and the controller tries to use a spare sector from the same cylinder if possible, to minimize head movement."
"Which bad-block recovery scheme involves remapping all sectors from a defective one onwards, moving them down one spot, to free up space for the bad sector to be mapped to?",Sector sparing.,Logical formatting.,Sector slipping.,Physical formatting.,Raw I/O.,C,"Sector slipping is the alternative scheme where sectors are remapped (shifted down) to avoid a bad sector, effectively 'slipping' the bad sector out of the sequence."
What action might recoverable soft errors on a disk trigger?,Immediate shutdown of the system.,Manual replacement of the entire disk.,Device activity like copying block data or sparing/slipping the block.,Automatic deletion of all files on the affected sector.,A full low-level format of the entire drive.,C,Recoverable soft errors may trigger device activity such as copying block data or initiating sector sparing/slipping to preserve data integrity.
What is the consequence of an unrecoverable hard error on a storage device?,The data on the affected block is automatically restored from the spare area.,The disk can continue operating normally without any intervention.,"The data on the affected block is lost, and the file using it must be repaired, often from a backup, requiring manual intervention.",The operating system automatically fixes the error and notifies the user.,"Only temporary system files are affected, and they are regenerated.",C,"An unrecoverable hard error means the data on the block is lost, and the file must be repaired, typically from a backup, and requires manual intervention."
Why is bad block management simpler for NVM (Non-Volatile Memory) devices compared to HDDs?,NVM devices are immune to bad blocks.,NVM devices do not store user data.,There is no seek time performance loss in NVM devices due to bad block remapping.,NVM devices use a different type of magnetic recording.,NVM devices rely entirely on software for bad block handling.,C,"NVM bad block management is simpler because there is no mechanical seek time performance loss when remapping bad pages, unlike HDDs."
How do NVM devices manage bad pages?,They rely on the operating system to manage a list of bad pages.,They physically remove the bad pages from the device.,"The controller maintains a table of bad pages and never sets them as available to write to, using multiple pages as replacement locations or space from over-provisioning.",They use sector sparing and sector slipping mechanisms identical to HDDs.,They halt all operations until the bad pages are manually repaired.,C,"NVM device controllers maintain a table of bad pages, preventing them from being used, and utilize spare pages or over-provisioned space for replacements."
What is the definition of 'low-level formatting' or 'physical formatting'?,The process of organizing files into directories.,The creation of a file system in a volume.,The initialization of a storage medium for computer storage.,The logical segregation of storage space into multiple areas.,The process of defragmenting a hard drive.,C,"Low-level formatting (or physical formatting) is the initialization of a storage medium for computer storage, preparing it for data by defining sectors/pages."
What is the definition of a 'partition' in storage management?,A physical division within a single hard drive platter.,A logical segregation of storage space into multiple areas.,A temporary storage area for volatile data.,A network drive for shared access.,A dedicated space for virtual memory.,B,"A partition is the logical segregation of storage space into multiple distinct areas, which the OS can treat as separate devices."
"Which term describes a container of storage, often a device with a mountable file system?",Sector.,Block.,Volume.,Cluster.,Partition table.,C,"A 'volume' is defined as a container of storage, often a device that has a mountable file system."
What is 'logical formatting'?,The process of physically dividing a disk into sectors.,The initial setup of a new hard drive at the factory.,The creation of a file system in a volume to ready it for use.,The manual flagging of bad blocks on a disk.,The process of merging multiple partitions into one.,C,"Logical formatting is the creation of a file system within a volume, which involves writing the initial file-system data structures to make it ready for use."
What does the term 'bootstrap' refer to in computing?,A method of securing network connections.,Steps taken at computer power-on to bring the system to full operation.,A type of software development framework.,The process of backing up system files.,A tool for recovering lost data.,B,"Bootstrap refers to the sequence of steps taken when a computer is powered on to bring the system to full operational status, starting with a small initial loader."
What is a 'boot partition'?,A partition exclusively for storing user documents.,A storage device partition containing an executable operating system.,A temporary partition used during software installation.,A partition that stores hardware drivers only.,A partition reserved for disk diagnostics.,B,"A 'boot partition' is a storage device partition that contains an executable operating system, allowing the computer to boot from it."
Which term describes the replacement of an unusable HDD sector with another sector elsewhere on the device?,Sector slipping.,Logical formatting.,Partitioning.,Sector sparing.,Volume creation.,D,"Sector sparing (or forwarding) is the process of replacing a bad sector with a spare sector from another location on the drive, managed by the controller."
What is 'sector slipping'?,A method for securely erasing data on a disk.,The process of making sectors available for use after a low-level format.,"The renaming of sectors to avoid using a bad sector, shifting subsequent sectors.",A technique to increase the density of data storage on a disk.,The automatic detection and reporting of bad sectors to the OS.,C,Sector slipping is a bad-block recovery method that involves renaming or remapping sectors to effectively bypass a bad sector by shifting the logical addresses of subsequent sectors.
"What is the primary definition of 'swapping' in the context of operating systems, as initially described?",Moving individual data blocks between cache and main memory.,Transferring entire processes between secondary storage and main memory.,Copying executable code from a file system into memory.,Reallocating physical memory pages within a process's address space.,Managing the flow of network packets between devices.,B,The text defines swapping as 'moving entire processes between secondary storage and main memory'.
Under what specific condition does swapping typically occur to free up main memory?,When a new process is created.,When physical memory is critically low.,During regular system startup.,When a user requests a file transfer.,At the end of a process's execution.,B,"Swapping 'occurs when physical memory critically low, processes moved to swap space to free memory'."
How do modern operating systems typically implement 'swapping' compared to its original definition?,They no longer use swapping at all.,They solely rely on transferring entire processes.,"They combine it with virtual memory, swapping individual pages rather than entire processes.",They exclusively use main memory for all process storage.,They use network-attached storage as the primary swap space.,C,"Modern OS 'combine swapping with virtual memory, swap pages, not entire processes'."
"In modern operating system terminology, how are the terms 'swapping' and 'paging' generally used?",They describe entirely distinct and unrelated operations.,"'Swapping' refers to disk-to-memory transfers, while 'paging' refers to memory-to-disk.",They are often used interchangeably.,"'Paging' is an older term, replaced by 'swapping'.",Only 'paging' is relevant in virtual memory systems.,C,"The text states, 'Terms ""swapping"" and ""paging"" used interchangeably'."
What is the nature of 'swap-space management' as a task within an operating system?,A high-level application programming interface (API).,A user-level utility for managing disk partitions.,A low-level OS task.,A network protocol for distributed memory.,A process scheduling algorithm.,C,The text explicitly defines 'swap-space management' as a 'low-level OS task'.
How does virtual memory utilize secondary storage?,As a temporary backup for critical system files.,As a permanent storage location for all user data.,As an extension of main memory.,Solely for storing hibernation files.,To accelerate CPU processing.,C,Virtual memory 'uses secondary storage as extension of main memory'.
What is the primary impact of using swap space on overall system performance?,It significantly increases system performance.,It has no measurable effect on performance.,It significantly decreases system performance due to slower drive access.,It only impacts performance when the system is idle.,It improves CPU utilization but degrades I/O.,C,The text indicates 'Drive access much slower than memory access implies swap space significantly decreases system performance'.
What is the main design and implementation goal for swap space in a virtual memory system?,To minimize the amount of secondary storage used.,To maximize data redundancy.,To provide the best throughput for the virtual memory system.,To ensure maximum storage efficiency.,To primarily store application installers.,C,The 'main goal for swap space design/implementation' is to 'provide best throughput for virtual memory system'.
How does the use of swap space differ between traditional 'swapping systems' and 'paging systems'?,"Swapping systems store individual pages, while paging systems store entire process images.","Swapping systems hold entire process images, while paging systems store pages pushed out of main memory.",Both systems store only anonymous memory pages.,"Swapping systems use it for temporary files, paging systems for permanent data.",Paging systems do not use swap space; only swapping systems do.,B,"Swapping systems 'may hold entire process image', while paging systems 'store pages pushed out of main memory'."
Which of the following factors does NOT directly influence the amount of swap space needed by an operating system?,Physical memory available.,Virtual memory backing requirements.,Virtual memory usage patterns.,The number of CPU cores.,All of the above directly influence it.,D,"The text lists 'physical memory, virtual memory backing, virtual memory usage' as factors influencing swap space amount, but not the number of CPU cores."
What is the potential consequence of underestimating the amount of swap space needed?,Increased system performance.,Wasted secondary storage space.,Aborting processes or system crash.,Reduced power consumption.,Slower boot times.,C,The text warns that 'Running out of swap space: may abort processes or crash'.
What is the primary negative consequence of overestimating the amount of swap space required?,System crashes.,Aborted processes.,Wasted secondary storage space.,Decreased physical memory availability.,Increased internal fragmentation in main memory.,C,The text states that 'Overestimation: wastes secondary storage space (no other harm)'.
How did Solaris typically determine the recommended amount of swap space?,It was fixed at double the physical memory.,It matched the total size of the hard drive.,It was equal to the virtual memory exceeding pageable physical memory.,It was always a static 1GB.,It depended solely on the number of active user sessions.,C,Solaris recommended 'swap space = virtual memory exceeding pageable physical memory'.
"Why do some operating systems, like Linux, allow for multiple swap spaces, often on separate storage devices?",To provide redundancy in case one swap area fails.,To simplify disk partitioning.,To consolidate all I/O onto a single device.,To spread the I/O load from paging/swapping over the system's I/O bandwidth.,To allow different file systems to use different swap policies.,D,The purpose of multiple swap spaces is to 'spread I/O load from paging/swapping over system's I/O bandwidth'.
What are the two primary locations where swap space can reside on a storage device?,In CPU cache or GPU memory.,In RAM disk or network share.,Carved out of a normal file system (as a large file) or in a separate raw partition.,Within the kernel's protected memory or user space.,In ROM or on a floppy disk.,C,Swap space can reside in 'Carved out of normal file system (large file)' or 'In a separate raw partition'.
What is a 'raw partition' in the context of swap space?,A partition primarily used for user data storage.,A partition that contains a dedicated file system for swap files.,A partition within a storage device that does not contain a file system.,A partition accessible only by the system administrator.,A virtual partition created in main memory.,C,A 'raw partition' is defined as a 'Partition within a storage device not containing a file system'.
"When swap space is located in a raw partition, what entity is responsible for allocating and deallocating blocks?",The standard file system routines.,The user application managing the process.,A separate swap-space storage manager.,The hardware disk controller directly.,The network file system daemon.,C,"For raw partitions, a 'separate swap-space storage manager allocates/deallocates blocks'."
What is the primary optimization goal for the algorithms used by a swap-space storage manager for raw partitions?,Storage efficiency.,Data integrity and backup.,Speed.,Minimizing internal fragmentation over the long term.,User accessibility.,C,The manager uses algorithms 'optimized for speed (not storage efficiency)'.
How is internal fragmentation typically viewed in the context of swap space on a raw partition?,It is entirely eliminated due to optimized algorithms.,It is a major long-term problem that must be avoided.,"It may increase, but is considered an acceptable trade-off due to short data life and reinitialization at boot.",It only occurs if the system experiences a power failure.,"It is managed by the file system, not the swap manager.",C,"Internal fragmentation 'may increase (acceptable trade-off, data life shorter)', and is 'short-lived (reinitialized at boot time)'."
"If an operating system relies on raw partitions for swap space, how is adding more swap space typically accomplished?",By simply creating a new large file in the existing file system.,Automatically by the OS as needed.,Through repartitioning the device or adding another swap space elsewhere.,"By connecting a new USB drive, which is then automatically configured.",It is not possible to add more once set up.,C,Adding more raw partition swap space requires 'repartitioning device (moving/destroying other partitions) or adding another swap space elsewhere'.
What is the primary trade-off when choosing between using a file system or a raw partition for swap space?,Security vs. compatibility.,Cost vs. reliability.,Convenience of file system allocation/management vs. performance of raw partitions.,Disk space usage vs. CPU utilization.,Network bandwidth vs. local storage.,C,The trade-off is 'convenience of file system allocation/management vs. performance of raw partitions'.
"In Solaris 1 (SunOS), how were text-segment pages (code) typically handled if selected for pageout?",They were always written to swap space.,They were automatically encrypted and compressed.,"They were immediately reread from the file system, avoiding swap write.","They were thrown away, with the system relying on rereading them from the file system when needed.",They were duplicated to another memory region.,D,"Text-segment pages were 'thrown away if selected for pageout', as it was 'more efficient to reread page from file system than write to swap and reread'."
What is 'anonymous memory' in the context of swap space usage?,Memory explicitly shared between multiple processes.,Memory backed by a specific file on the file system.,"Memory not associated with any file, such as stack, heap, or uninitialized data.",Memory used by system kernel processes only.,Read-only memory segments.,C,"Anonymous memory is defined as 'not backed by any file (stack, heap, uninitialized data of process)'."
What significant change was introduced in later Solaris versions regarding swap space allocation?,Swap space was entirely eliminated.,Swap space was allocated for every virtual memory page creation.,Swap space was allocated only when a page was forced out of physical memory.,"Swap space was only used for system files, not process data.",Swap space was dynamically resized based on CPU load.,C,Later Solaris versions 'allocates swap space only when page forced out of physical memory (not when virtual memory page first created)'.
What is the primary benefit of the later Solaris swap space allocation scheme (allocating only when a page is forced out)?,"It uses more swap space, thus improving reliability.",It simplifies disk partitioning.,It provides better performance on modern computers with more physical memory.,It allows for more efficient file system journaling.,It reduces the need for CPU context switching.,C,"This scheme provides 'better performance on modern computers (more physical memory, page less)'."
How does Linux's approach to swap space usage compare to Solaris?,"Linux uses swap space for all process memory, unlike Solaris.","Linux avoids swap space entirely, unlike Solaris.","Linux is similar to Solaris, using swap space primarily for anonymous memory.","Linux uses swap space only for kernel modules, unlike Solaris.","Linux restricts swap space to raw partitions only, unlike Solaris.",C,"Linux is 'similar to Solaris, swap space only for anonymous memory'."
What are the fundamental units that make up a Linux swap area?,Entire process images.,Logical volumes.,Series of 4-KB page slots.,File system directories.,CPU registers.,C,Each Linux swap area consists of a 'series of 4-KB page slots'.
"In Linux swap-space management, what is a 'swap map'?",A graphical representation of disk usage.,A list of all active processes using swap.,An array of integer counters associated with page slots.,A configuration file for swap area settings.,A kernel module that handles I/O requests.,C,A 'swap map' is an 'array of integer counters' where each counter 'corresponds to a page slot'.
"In a Linux swap map, what does a counter value greater than 0 for a page slot indicate?",The page slot is available for use.,The page slot is corrupted.,"The page slot is occupied by a swapped page, and the value indicates the number of mappings to that page.",The page slot is reserved for future use.,The page slot is part of the file system.,C,A 'Counter value > 0: page slot occupied by swapped page. Counter value: indicates number of mappings to swapped page'.
"Which of the following is NOT one of the three primary ways computers access secondary storage, as described in the text?",Host-attached storage,Network-attached storage,Cloud storage,Direct-to-CPU storage,All of the above are listed ways.,D,"The text states that computers access secondary storage in three ways: host-attached, network-attached, and cloud storage. Direct-to-CPU storage is not mentioned as a primary access method."
How is host-attached storage primarily accessed?,Through a remote-procedure-call (RPC) interface over a network,Via an API over the Internet,Through local I/O ports,Using a private storage-area network (SAN),By sending logical blocks across a general IP network,C,Host-attached storage is defined as being 'accessed through local I/O ports'.
What is the most common local I/O port mentioned for host-attached storage?,USB,FireWire,Thunderbolt,SATA,Ethernet,D,The text states that 'most common: SATA' when referring to local I/O ports for host-attached storage.
"Which of the following is a high-speed serial architecture, often using optical fiber or copper cable, beneficial for high-end workstations and servers needing more or shared host-attached storage?",Ethernet,Fibre Channel (FC),SATA Express,USB 3.0,InfiniBand (IB),B,Fibre Channel (FC) is described as a 'high-speed serial architecture (optical fiber or copper cable)' used when high-end workstations/servers need more/shared storage.
What is a benefit of Fibre Channel (FC) as described in the text?,It operates exclusively over wireless connections for maximum flexibility.,"It is a low-cost, low-speed solution ideal for personal computers.","It provides a large address space and switched communication, allowing multiple hosts/storage devices to attach to the fabric.",It primarily focuses on providing file-level access using standard IP protocols.,It uses a general IP network to carry SCSI protocol for simplicity.,C,"The text lists FC benefits as: 'large address space, switched communication, multiple hosts/storage devices attach to fabric (flexibility in I/O communication)'."
Which statement accurately describes how I/O commands are directed for host-attached storage?,They send parts of files across a network using RPC interfaces.,"They are API-based, designed for high latency WAN connections.",They are reads/writes of logical data blocks directed to specifically identified storage units.,"They utilize a private network with storage protocols, bypassing the data network.","They only support traditional HDD devices, not NVM.",C,The text states: 'I/O commands for host-attached storage: reads/writes of logical data blocks directed to specifically identified storage units (bus ID or target logical unit)'.
What defines Network-attached storage (NAS)?,Storage accessed through local I/O ports on the host computer.,Storage accessed via an API over the Internet to a remote data center.,Storage accessed from a computer over a network.,A private network connecting servers and storage units using storage protocols.,A dedicated high-speed bus architecture for direct server-to-storage links.,C,Network-attached storage (NAS) is defined as providing 'access to storage across a network'.
"Which of the following are common remote-procedure-call (RPC) interfaces used by clients to access NAS, as mentioned in the text?",HTTP and FTP,SSH and Telnet,NFS (UNIX/Linux) and CIFS (Windows),SMTP and POP3,FC and iSCSI,C,"Clients access NAS via RPC interface: 'NFS (UNIX/Linux), CIFS (Windows)'."
What is a major downside of Network-attached storage (NAS) compared to some direct-attached storage?,It lacks file sharing capabilities between hosts.,"It requires complex, custom hardware for implementation.",It is generally less efficient and offers lower performance.,It can only be accessed by a single client at a time.,It relies exclusively on Fibre Channel for connectivity.,C,"The text mentions a 'Downside: less efficient, lower performance than some direct-attached storage'."
Which network-attached storage protocol uses an IP network to carry the SCSI protocol?,NFS,CIFS,iSCSI,Fibre Channel (FC),InfiniBand (IB),C,The text identifies 'iSCSI' as the 'latest network-attached storage protocol' that 'Uses IP network protocol to carry SCSI protocol'.
How does iSCSI differ from NFS/CIFS in terms of how it presents storage and transfers data?,"iSCSI presents a file system and sends parts of files, while NFS/CIFS sends logical blocks.","iSCSI sends logical blocks across the network, while NFS/CIFS presents a file system and sends parts of files.","iSCSI uses optical fiber, while NFS/CIFS uses copper cables.","iSCSI is only for local networks, while NFS/CIFS is for wide-area networks.","iSCSI is API-based, while NFS/CIFS uses traditional file sharing protocols.",B,"The text states: 'NFS/CIFS: present file system, send parts of files. iSCSI: sends logical blocks across network, client uses blocks directly or creates file system'."
What is a key distinction of Cloud storage compared to Network-attached storage (NAS)?,"Cloud storage is accessed via local I/O ports, while NAS uses network protocols.","Cloud storage is accessed over the Internet/WAN to a remote data center, unlike NAS which is usually over a LAN.","Cloud storage typically uses NFS/CIFS for access, while NAS is API-based.","Cloud storage primarily focuses on block-level access, while NAS is file-level.",Cloud storage does not require any network connectivity.,B,The text states: 'Unlike NAS: accessed over Internet/WAN to remote data center (storage for fee/free)' for cloud storage.
"How is cloud storage typically accessed by programs, contrasting with how NAS is accessed?","Cloud storage uses traditional file system protocols like NFS/CIFS, while NAS uses proprietary APIs.","Cloud storage is accessed as another file system or raw block device, while NAS is API-based.","Cloud storage uses standard TCP/UDP over IP, while NAS uses specialized storage protocols.","Cloud storage is API based; programs use APIs to access, while NAS integrates existing protocols (CIFS/NFS or iSCSI).","Cloud storage bypasses the OS for direct hardware access, unlike NAS.",D,The text clarifies: 'NAS: accessed as another file system (CIFS/NFS) or raw block device (iSCSI). OS integrates these protocols. Cloud storage: API based; programs use APIs to access.'
What is the primary reason given for why cloud storage uses APIs instead of existing LAN protocols like NFS/CIFS?,APIs are less secure than traditional LAN protocols.,Existing protocols are too complex for cloud environments.,WAN latency and failure scenarios make LAN protocols unsuitable.,"APIs allow for direct integration with hardware, bypassing software layers.",Cloud providers prefer to lock users into their proprietary systems.,C,The text states: 'Reason for APIs vs. existing protocols: WAN latency and failure scenarios'.
What is a significant drawback of Network-attached storage (NAS) that Storage-area networks (SANs) aim to address?,NAS devices are typically more expensive than SANs.,"NAS requires proprietary hardware, while SAN uses off-the-shelf components.","NAS storage I/O consumes data network bandwidth, increasing network communication latency.",NAS offers limited flexibility in allocating storage to hosts.,NAS cannot be shared by multiple hosts simultaneously.,C,"The text explicitly states: 'Drawback of NAS: storage I/O consumes data network bandwidth, increases network communication latency.' and SANs are introduced as a solution for large installations where 'server-client communication competes with server-storage communication'."
Which of the following best defines a Storage-area network (SAN)?,A general-purpose IP network used for both data and storage traffic.,"A private network connecting servers and storage units using storage protocols, not networking protocols.",A cloud-based service that provides storage via APIs over the Internet.,A system where storage is directly attached to individual host computers via local I/O ports.,A device that provides file-level access to storage across a local area network.,B,"A SAN is defined as a 'private network (storage protocols, not networking protocols) connecting servers and storage units'."
What is a primary 'power' or benefit of a Storage-area network (SAN)?,Its low cost and ease of deployment for small businesses.,Its ability to eliminate the need for storage controllers.,Its exclusive reliance on wireless communication for storage access.,"Its flexibility, allowing multiple hosts/storage arrays to attach and storage to be dynamically allocated.",Its capacity to store data only on solid-state drives (SSDs).,D,The text highlights 'SAN power: flexibility' and details that 'Multiple hosts/storage arrays: attach to same SAN. Storage: dynamically allocated to hosts'.
What does 'JBOD' stand for in the context of storage arrays?,Just Before Out of Data,Jumbo Block on Demand,Joined Backup of Disks,Just a Bunch of Disks,Java Based Object Drive,D,The acronym 'JBOD' is defined as 'Just a Bunch of Disks' referring to unprotected drives in a storage array.
What is the role of a SAN switch?,To convert IP network traffic to Fibre Channel traffic.,To manage client access to files stored on a NAS device.,To allow or prohibit access between hosts and storage units.,To perform data deduplication and compression for a storage array.,To connect individual drives within a storage array to the controller.,C,The text states: 'SAN switch: allows/prohibits access between hosts and storage'.
Which statement accurately compares SANs and NAS concerning connected hosts?,SANs can typically connect more hosts than NAS.,NAS can have more connected hosts than SAN.,Both SANs and NAS are limited to a single connected host.,"SANs primarily connect to individual workstations, while NAS connects to servers.",The number of connected hosts is irrelevant for both SAN and NAS.,B,The text mentions: 'NAS can have more connected hosts than SAN'.
What is a Storage Array described as?,A general-purpose server running file-sharing software.,A collection of individual hard drives directly connected to a single host.,A purpose-built device including drives to store data and controller(s) to manage storage/access.,A software-defined storage solution that runs entirely on virtual machines.,A network switch specifically designed for connecting client computers to the internet.,C,"A storage array is described as a 'purpose-built device (includes SAN/network ports or both). Contains: drives to store data, controller(s) to manage storage/access'."
Which of the following functions are typically implemented by the controllers within a storage array?,Email server functionalities and web hosting.,"Network protocols, UIs, RAID, snapshots, replication, compression, deduplication, and encryption.",Operating system booting and application execution for client computers.,Connecting to the Internet via Wi-Fi and managing wireless access points.,Providing power backup and uninterruptible power supply (UPS).,B,"Controllers are described as having 'CPUs, memory, software (implement array features: network protocols, UIs, RAID, snapshots, replication, compression, deduplication, encryption)'."
What is Fibre Channel (FC) identified as in the context of Storage-area networks (SANs)?,The most common networking protocol for NAS.,A type of cloud storage API.,The most common SAN interconnect.,A method for directly attaching storage to a host's CPU.,A protocol for accessing storage over a wireless network.,C,The text states: 'FC: most common SAN interconnect'.
"Besides Fibre Channel (FC) and iSCSI, what other SAN interconnect is mentioned?",Ethernet,USB,InfiniBand (IB),FireWire,SATA,C,The text states: 'Another SAN interconnect: InfiniBand (IB)'.
What distinguishes InfiniBand (IB) as described in the text?,It is a general-purpose networking protocol for client-server communication.,"It is a low-speed, cost-effective solution for small office environments.",It is a special-purpose bus architecture with hardware/software support for high-speed interconnection networks.,It is primarily used for host-attached storage with individual drives.,It uses standard IP networks to carry file-level protocols like NFS and CIFS.,C,"InfiniBand (IB) is described as a 'special-purpose bus architecture, hardware/software support for high-speed interconnection networks (servers, storage units)'."
What does the acronym RAID primarily stand for in its current usage?,Rapid Arrays of Inexpensive Disks,Redundant Arrays of Independent Disks,Reliable Access to Integrated Data,Replicated Archives for Integrated Devices,Restored Algorithms for Indexed Data,B,The text states that 'I' in RAID now stands for 'independent' and the full term is 'Redundant arrays of independent disks'.
Which of the following best describes the modern purpose of Redundant Arrays of Independent Disks (RAIDs)?,"To provide a cost-effective alternative to large, expensive single disks.",To decrease the overall data storage capacity of a system.,To achieve higher reliability and improved data-transfer rates.,To simplify disk management by reducing the number of drives.,To eliminate the need for data backups entirely.,C,"The text states, 'Today: RAIDs used for higher reliability and data-transfer rate.' While they were a cost-effective alternative in the past, their primary modern purpose is higher reliability and data-transfer rate."
What is the statistical mean time a device is expected to work correctly before failing?,Mean time to repair (MTTR),Mean time to data loss (MTTDL),Mean time between failures (MTBF),Mean operating time (MOT),Estimated lifespan (ELS),C,The definition provided is 'mean time between failures (MTBF) Statistical mean time a device is expected to work correctly before failing.'
"According to the text, how does the chance of some disk failing in an N-disk array compare to the chance of a single disk failing?",It is exactly the same.,It is always lower.,It is always higher.,"It depends on the type of disk, but generally lower.","It depends on the RAID level, but generally higher.",C,The text explicitly states: 'Chance of some disk failing in N disks > chance of single disk failing.'
What is the primary function of 'redundancy' in a storage system?,To increase the overall speed of data access.,To reduce the physical space required for data storage.,"To store extra information not normally needed, used to rebuild lost information.",To encrypt data for enhanced security.,To consolidate multiple smaller files into larger blocks.,C,"The text defines redundancy as 'store extra info not normally needed, used to rebuild lost info.'"
"Which RAID technique involves duplicating every drive, so a logical disk consists of two physical drives with every write operation performed on both?",Data striping,Bit-level striping,Block-level striping,Mirroring,Parity checking,D,The text describes mirroring as 'Simplest (most expensive) redundancy: duplicate every drive (mirroring). Mirroring: logical disk = two physical drives; every write on both drives.'
A 'mirrored volume' loses data only if which of the following conditions is met?,The primary drive fails during a read operation.,The second drive fails before the first is replaced after its failure.,Both drives experience a power failure simultaneously.,A software bug corrupts the file system on both drives.,A single drive fails and is not replaced within 24 hours.,B,The text states for mirroring: 'One drive fails: data read from other. Data lost only if second drive fails before first replaced.'
What two factors determine the Mean Time to Data Loss (MTTDL) of a mirrored volume?,The total storage capacity and the number of logical drives.,The individual drive's RPM and the transfer rate.,The Mean Time Between Failures (MTBF) of individual drives and the Mean Time To Repair (MTTR).,The power supply's reliability and the network latency.,The cost of the drives and the age of the system.,C,The text states: 'MTBF of mirrored volume (failure = data loss) depends on: MTBF of individual drives. mean time to repair: average time to replace failed drive and restore data.'
Which term describes the average time to replace a failed drive and restore its data?,Mean time between failures (MTBF),Mean time to data loss (MTTDL),Mean time to repair (MTTR),Recovery point objective (RPO),Recovery time objective (RTO),C,The definition provided is 'mean time to repair: average time to replace failed drive and restore data.'
"Which of the following is NOT a type of correlated failure that can affect mirrored-drive systems, as mentioned in the text?",Power failures,Natural disasters,Manufacturing defects,Simultaneous independent errors,Drive aging affecting failure probability,D,"The text lists 'power failures, natural disasters, manufacturing defects' and 'drives age' as causes for correlated failures, implying they are not independent. Simultaneous independent errors are not a cause of *correlated* failure."
What is a common solution to protect against inconsistent states during write operations in mirrored systems caused by power failures?,Using only solid-state drives (SSDs).,Implementing a write-back cache protected by ECC or mirroring.,Performing all writes synchronously to the network.,Ensuring all drives are from different manufacturers.,Disabling the write cache on all drives.,B,"The text suggests solutions for power failures during writes: 'write one copy first, then next; or add solid-state nonvolatile cache to RAID array. Write-back cache: protected from data loss during power failures (write considered complete). Assumes cache has error protection/correction (ECC, mirroring).'"
How does mirroring affect the read performance of a RAID system?,It doubles the transfer rate per read for individual requests.,It halves the overall read request rate.,It doubles the read request rate by sending reads to either drive.,It significantly increases latency for all read operations.,It has no impact on read performance.,C,The text states: 'Mirroring: read request rate doubled (reads sent to either drive). Transfer rate per read: same as single drive; but reads per unit time doubled.'
What technique involves splitting data across multiple drives to improve transfer rates?,Data mirroring,Parity generation,Data striping,Volume caching,Drive pooling,C,The text defines 'data striping' as 'Splitting of data across multiple devices.'
"In an 8-drive array using bit-level striping, how is the array logically perceived?","As 8 separate drives, each with independent access.",As a single drive with 8 times the normal sector size and 8 times the access rate.,As a single drive with the same sector size but 8 times the latency.,"As 4 mirrored pairs, each with double the sector size.",As a drive that only performs sequential reads.,B,"For bit-level striping, the text gives the example: 'Array of 8 drives: treated as single drive with 8x normal sector size, 8x access rate.'"
Which statement accurately describes the goals of parallelism in a storage system via striping?,To maximize storage capacity and minimize hardware cost.,To increase throughput of multiple small accesses and reduce response time of large accesses.,To provide data redundancy and simplify backup procedures.,To ensure data encryption and secure network communication.,To reduce power consumption and increase drive longevity.,B,The text lists the goals as: '1. Increase throughput of multiple small accesses (page accesses) by load balancing. 2. Reduce response time of large accesses.'
Which RAID level utilizes block-level striping but provides no data redundancy?,RAID level 0,RAID level 1,RAID level 4,RAID level 5,RAID level 6,A,"The text defines 'RAID level 0' as 'drive arrays with block-level striping, no redundancy.'"
What is RAID level 1 primarily known for?,Block-level striping with distributed parity.,Using memory-style ECC organization.,Implementing drive mirroring.,P + Q redundancy for two drive failures.,Providing no redundancy but high performance.,C,The text states 'RAID level 1: drive mirroring.'
"In RAID level 4, where are the error-correcting code (ECC) calculations for striped data blocks typically stored?",Distributed evenly across all data drives.,On a dedicated parity drive.,Within the same data block on the same drive.,In the system's main memory (RAM).,"On a separate, external backup device.",B,"For RAID 4, the text explains: 'ECC calculation result stored on drive N+1 (error-correction block).'"
What is a 'read-modify-write cycle' in the context of RAID 4 operations?,"The process of reading all data blocks, modifying them, and writing them back to increase performance.","A necessary operation when the operating system writes data smaller than a full block, requiring the old block to be read, modified, and written back, along with an updated parity block.","A method to verify data integrity by reading a block, calculating its checksum, and rewriting it if necessary.",The procedure for migrating data from a failing drive to a hot spare.,The initial setup process where all drives are formatted and synchronized.,B,"The text defines 'read-modify-write cycle' as a situation where 'Write of data smaller than block requires entire block to be read, modified, and written back.' It elaborates that 'OS write smaller than block: requires read-modify-write cycle. ... block read, modified, written back; parity block updated.'"
"What is a significant advantage of RAID 4 over RAID 1, assuming equal data protection?",RAID 4 offers higher individual drive throughput.,RAID 4 simplifies the hot-swapping of failed drives.,RAID 4 significantly reduces storage overhead by using one parity drive for several data drives.,RAID 4 eliminates the need for any form of parity calculation.,RAID 4 provides better protection against multiple drive failures.,C,The text states 'RAID 4 advantages over RAID 1 (equal data protection): Storage overhead reduced: one parity drive for several regular drives (vs. one mirror for every drive).'
What is the primary distinguishing feature of RAID level 5 compared to RAID level 4?,"RAID 5 uses a dedicated parity drive, while RAID 4 distributes parity.","RAID 5 provides more redundancy for multiple drive failures, while RAID 4 only handles one.","RAID 5 distributes parity blocks across all drives, avoiding a single point of congestion, while RAID 4 uses a dedicated parity drive.",RAID 5 offers higher read performance due to bit-level striping.,RAID 5 does not require a read-modify-write cycle for small writes.,C,The text describes RAID 5 as 'Spreads data and parity among all N+1 drives (not parity in one drive)' and states 'Spreading parity: avoids overuse of single parity drive (RAID 4 problem).'
Which statement about RAID level 5's parity block placement is true?,"The parity block for a set of data blocks can be stored on any drive, including one that contains data from that same set.","The parity block for a set of data blocks must be stored on a dedicated drive, separate from all data drives.",The parity block for a set of data blocks cannot store parity for blocks in the same drive to prevent data + parity loss upon failure.,"Parity blocks are only stored in memory, not on physical drives.",RAID 5 does not use parity blocks.,C,The text explicitly states for RAID 5: 'Parity block cannot store parity for blocks in same drive (failure \u21d2 data + parity loss).'
RAID level 6 is distinguished by its ability to tolerate how many drive failures?,Zero,One,Two,Three,An unlimited number,C,The text states for RAID level 6: 'System tolerates two drive failures.'
What mathematical concept is mentioned as being used to calculate the 'Q' parity in RAID level 6?,Euclidean Geometry,Boolean Algebra,Galois Field Math,Linear Regression,Calculus,C,"The text states for RAID 6: 'Uses error-correcting codes (e.g., Galois field math) to calculate Q.'"
Which of the following accurately describes a theoretical advantage of RAID 1+0 over RAID 0+1 in a single drive failure scenario?,"RAID 0+1 ensures that only the affected drive becomes unavailable, while the rest of the array remains functional.","RAID 1+0 ensures that if one drive fails, its mirror is still available, and the rest of the striped drives remain accessible.",RAID 0+1 can recover data much faster due to distributed parity.,RAID 1+0 uses less storage overhead compared to RAID 0+1.,RAID 0+1 is more cost-effective for enterprise solutions.,B,"The text explains: 'RAID 0 + 1 single drive failure: entire stripe inaccessible, only other stripe left. RAID 1 + 0 single drive failure: single drive unavailable, but mirror available, rest of drives available.'"
"Which level of RAID implementation offers the most flexibility, allowing RAID sets to be created and sliced into volumes, with the OS only implementing the file system on these volumes, and potentially supporting multiple connections or part of a SAN?",Volume-management software,HBA hardware,Storage array hardware,SAN interconnect layer,Operating system file system,C,"The text states: 'Storage array hardware: creates RAID sets, slices into volumes. OS only implements file system on volumes. Arrays can have multiple connections/part of SAN.' This describes the highest level of flexibility and integration among the choices."
What is a 'snapshot' in the context of file systems?,"A full, bit-for-bit copy of an entire volume for backup.",A read-only view of a file system at a particular point in time.,A real-time synchronization of data between two active servers.,A process to defragment and optimize disk space.,A method for automatically repairing corrupted files.,B,"The definition provided is 'snapshot: In file systems, a read-only view of a file system at a particular point in time.'"
"Which term describes the automatic duplication of writes between separate sites, either synchronously or asynchronously, for redundancy and disaster recovery?",Striping,Mirroring,Caching,Replication,Snapshotting,D,"The definition provided is 'replication: In file systems, duplication and synchronization of data over network to another system.'"
What is the primary characteristic of a 'hot spare' drive?,It is actively used for data storage but can be quickly repurposed for backup.,It is an unused storage device configured to automatically replace a failed drive in a RAID set.,It is a drive that operates at a higher temperature to improve performance.,It is a temporary drive used only during initial RAID setup.,It is a drive dedicated to storing log files and system metadata.,B,"The definition provided is 'hot spare: Unused storage device ready to be used to recover data (e.g., in RAID set).'"
"Which RAID level is often chosen for high-performance applications where data loss is not critical, such as scientific computing?",RAID level 1,RAID level 5,RAID level 0,RAID level 6,RAID level 0 + 1,C,"The text states: 'RAID level 0: used in high-performance apps where data loss not critical (e.g., scientific computing).'"
What distinguishes the InServ storage array's approach to RAID configuration?,"It requires all drives to be configured at a single, fixed RAID level.","It applies RAID at the 'chunklet' level, allowing drives to participate in multiple RAID levels simultaneously.",It only supports RAID level 1 (mirroring) for maximum reliability.,It uses a unique form of bit-level striping not found in other arrays.,It completely eliminates the concept of RAID in favor of object storage.,B,The text says: 'InServ storage array (HP 3Par): Does not require drives configured at specific RAID level. Each drive broken into 256-MB 'chunklets'. RAID applied at chunklet level. Drive participates in multiple/various RAID levels (chunklets used for multiple volumes).'
"Which InServ feature allows a host to be provided with a large logical storage space, but initially only a small amount of physical storage, with more physical storage allocated as needed?",Data striping,Mirroring,Utility storage,Block-level caching,Volume mirroring,C,"The text defines 'utility storage' as an 'InServ feature: storage space can be increased as needed' and describes how 'Administrator configures InServ: provide host with large logical storage, initially small physical storage. Host uses storage: unused drives allocated up to original logical level.'"
Which of the following problems is NOT protected against by traditional RAID implementations?,Physical media errors on a single drive.,Failure of a single disk drive.,Incomplete writes ('torn writes').,Corruption of data due to a disk controller failure.,Loss of a parity drive in a RAID 5 array.,C,"The text lists problems not protected by RAID: 'wrong file pointer, wrong pointers within file structure, incomplete writes (""torn writes""), accidental overwrite of file system structures.'"
What is the core innovation of the Solaris ZFS file system regarding data integrity?,It uses advanced encryption to prevent unauthorized data access.,"It maintains internal checksums of all data and metadata blocks, stored with pointers to the blocks.",It implements a triple-mirroring scheme for ultimate redundancy.,It automatically compresses all data to save storage space.,It virtualizes storage to present a single large volume to the OS.,B,"The text states: 'ZFS: maintains internal checksums of all blocks (data, metadata). Checksums: not kept with checksummed block, stored with pointer to block.'"
"In ZFS, if mirrored data blocks have one correct checksum and one incorrect checksum, what action does ZFS automatically take?",It flags the entire mirror as corrupted and requires manual intervention.,It discards both blocks and requests a rewrite of the data.,It automatically updates the bad block with the good one.,It sends an alert to the administrator but takes no action.,It calculates a new checksum based on the incorrect block.,C,"The text states: 'Data mirrored, one block correct checksum, one incorrect: ZFS automatically updates bad block with good one.'"
What is a 'pool' in the context of Solaris ZFS?,A logical volume created by striping data across multiple physical drives.,A temporary cache for frequently accessed data.,"A collection of drives, partitions, or RAID sets that can contain one or more ZFS file systems, with free space available to all.",A network-attached storage device dedicated to backups.,A shared memory area used for inter-process communication.,C,"The definition provided is 'pool: In ZFS, drives, partitions, or RAID sets that can contain one or more file systems.' The text further clarifies 'Entire pool's free space: available to all file systems within that pool.'"
How does object storage typically differ from traditional file systems in terms of finding data?,Object storage uses a hierarchical directory structure.,Object storage allows navigation of the pool to find objects directly.,"Object storage accesses objects primarily via an object ID, without a navigational structure.",Object storage relies on a central database to map file names to physical locations.,"Object storage is only accessible through command-line interfaces, not graphical ones.",C,The text states about object storage: 'Differs from file systems: no way to navigate pool and find objects' and describes the sequence 'Access object via object ID.'
Which of the following is an example of object storage management software mentioned in the text?,Microsoft NTFS,Solaris ZFS,Linux EXT4,Hadoop file system (HDFS),Apple HFS+,D,The text lists 'Hadoop file system (HDFS)' and 'Ceph' as examples of object storage management software.
What is the primary advantage of object storage in terms of scaling capacity?,It allows scaling capacity by simply replacing existing drives with larger ones.,"It enables vertical scalability, maximizing the capacity of a single server.","It provides horizontal scalability, allowing capacity to be added by integrating more computers with disks into the pool.","It automatically compresses all stored data, effectively increasing capacity.",It relies on cloud providers to automatically manage storage capacity without user intervention.,C,The text states: 'Advantage: horizontal scalability. ... Object store: add capacity by adding more computers with internal/external disks to pool.'
What is 'content-addressable storage' another term for?,Network-attached storage (NAS),Storage area network (SAN),Object storage,Direct-attached storage (DAS),Solid-state drive (SSD) caching,C,The definition provided is 'content-addressable storage: Another term for object storage; objects retrieved based on their contents.'
"What type of data is characterized by not having a fixed format and being free-form, commonly found in object stores?",Relational data,Structured data,Transactional data,Unstructured data,Encrypted data,D,The definition provided is 'unstructured data: Data not in a fixed format but rather free-form.'
When does RAID 4 have high transfer rates for both reads and writes?,"Only for small, random access operations.",When accessing individual drives independently.,"During large, sequential read and write operations that utilize all disks in parallel.",During the read-modify-write cycle for small writes.,When the system is idle and no data is being accessed.,C,The text states for RAID 4: 'Large reads: high transfer rates (all disks read in parallel). Large writes: high transfer rates (data and parity written in parallel).'
Which of the following are identified as major secondary storage I/O units?,RAM and Cache,HDDs and CPUs,SSDs and GPUs,Hard disk drives (HDDs) and Nonvolatile memory (NVM) devices,USB drives and Optical drives,D,The text states that 'Hard disk drives (HDDs) and nonvolatile memory (NVM) devices' are major secondary storage I/O units.
How is modern secondary storage typically structured?,As a hierarchical tree of directories,As a complex network of interconnected nodes,As large one-dimensional arrays of logical blocks,As a two-dimensional grid for faster access,As a stack-based memory allocation system,C,Modern secondary storage is described as 'structured as large one-dimensional arrays of logical blocks'.
Which of the following is NOT a way drives can be attached to a computer according to the text?,Through local I/O ports on the host,Directly connected to motherboards,Through a communications network or storage network connection,Via a dedicated wireless Bluetooth connection,All of the above are listed ways,D,"The text lists three ways: 'Through local I/O ports on host', 'Directly connected to motherboards', and 'Through communications network or storage network connection'. A dedicated wireless Bluetooth connection is not mentioned."
Which two system components are responsible for generating requests for secondary storage I/O?,The CPU and GPU,The File System and Virtual Memory System,The Network Interface Card and BIOS,The Power Supply Unit and Cooling System,The Keyboard and Mouse input drivers,B,The text states that 'Requests for secondary storage I/O: generated by file system and virtual memory system'.
How is the device address specified in each secondary storage I/O request?,As a file name and path,"As a physical cylinder, track, and sector address",As a unique device serial number,As a logical block number,As an IP address for network storage,D,Each request 'specifies device address as logical block number'.
What is the primary purpose of disk-scheduling algorithms for Hard Disk Drives (HDDs)?,To reduce power consumption during idle periods,"To improve effective bandwidth, average response time, and variance in response time",To reformat the disk automatically when errors occur,To encrypt data for security purposes,To physically align disk platters for better performance,B,"Disk-scheduling algorithms are designed to 'improve HDD effective bandwidth, average response time, variance in response time'."
How do algorithms like SCAN and C-SCAN improve disk performance for HDDs?,By reducing the rotational latency of the disk,By employing advanced error correction codes,Via disk-queue ordering strategies,By increasing the spindle speed of the drive,By parallelizing read/write operations across multiple heads,C,"Algorithms (SCAN, C-SCAN) 'improve via disk-queue ordering strategies'."
What is true about the performance of Solid-State Disks (SSDs) in relation to scheduling algorithms?,"SSDs have moving parts, so their performance varies greatly among algorithms.",SSDs primarily use algorithms to minimize seek time.,"SSDs have no moving parts, and their performance varies little among algorithms.",SSDs always require complex scheduling algorithms for optimal performance.,SSDs prioritize algorithms that reduce rotational delay.,C,"The text states: 'Solid-state disks (SSDs): no moving parts, performance varies little among algorithms'."
Which scheduling strategy is often used by Solid-State Disks (SSDs)?,SCAN,C-SCAN,Shortest Seek Time First (SSTF),"First-Come, First-Served (FCFS)",Look,D,SSDs 'often use simple FCFS strategy' due to their lack of moving parts.
What is the primary goal of error detection in data storage and transmission?,To automatically repair all corrupted data without user intervention.,To increase the data transfer rate by skipping problematic sections.,"To spot problems, alert the system for corrective action, and avoid error propagation.",To compress data before transmission to minimize errors.,To ensure data privacy and security during storage.,C,"Error detection is defined as attempting to 'spot problems, alert system for corrective action, avoid error propagation'."
"What does error correction involve, and what does its effectiveness depend on?",Detecting and preventing future errors; depends on system uptime.,Detecting and repairing problems; depends on correction data and corruption amount.,Simply alerting the user to errors; depends on user response time.,Logging errors for later analysis; depends on log file size.,Ignoring minor errors to maintain performance; depends on error tolerance thresholds.,B,"Error correction 'detects and repairs problems (depends on correction data, corruption amount)'."
How are storage devices typically organized into chunks of space?,They are dynamically resized based on current usage.,They are partitioned into one or more chunks of space.,"They use a single, monolithic block for all data.",They are divided into equal-sized segments by the CPU.,"They are organized as a complex, multi-level cache system.",B,Storage devices are 'partitioned into one or more chunks of space'.
What can each partition on a storage device hold?,Only the operating system's kernel.,A single application program's data.,A volume or be part of a multidevice volume.,Only temporary swap files.,The entire contents of the system's RAM.,C,Each partition 'can hold a volume or be part of a multidevice volume'.
Where are file systems created?,Directly on raw disk sectors.,Within the CPU's registers.,In volumes.,On network attached storage (NAS) devices only.,Inside the boot block of the device.,C,File systems are 'created in volumes'.
What is typically true about new storage devices when they are acquired?,They are completely blank and require manual low-level formatting.,They are typically pre-formatted.,They come with a pre-installed operating system.,They are ready to use immediately without any setup.,They must first be connected to a network storage server.,B,New devices are 'typically pre-formatted'.
What is the purpose of boot blocks if a storage device contains an operating system?,To store user application data.,To house the entire operating system kernel.,To store the system's bootstrap program.,To act as a cache for frequently accessed files.,To perform error correction on the entire disk.,C,Boot blocks are 'allocated to store system's bootstrap program (if device contains OS)'.
What action must the system take if a block or page becomes corrupted?,It automatically reboots the entire system.,It attempts to recover data from the cloud.,It must lock out or logically replace with a spare.,It notifies the user to manually replace the storage device.,It compresses the corrupted block to save space.,C,"If a block/page is corrupted, the 'system must lock out or logically replace with spare'."
What is critical for good performance in some systems regarding swap space?,Minimizing its size to save disk space.,Ensuring it is encrypted for security.,Having efficient swap space.,Storing it on a network drive.,Avoiding its use altogether.,C,Efficient swap space is described as 'key to good performance in some systems'.
How do different systems handle the implementation of swap space?,All systems dedicate a raw partition for swap space.,All systems use a file within the file system for swap space.,"Some dedicate a raw partition, others use a file within the file system, and some provide both options.",Swap space is only used in virtualized environments.,"Swap space is managed entirely by hardware, not the OS.",C,The text outlines the variety: 'Some systems: dedicate raw partition to swap space. Others: use file within file system. Still others: provide both options (user/admin decision).'
"For large systems storage, why are secondary storage devices frequently made redundant via RAID algorithms?",To simplify device partitioning and volume creation.,To improve power efficiency of the storage system.,"To allow more than one drive for operation, and enable continued operation/automatic recovery from drive failure.",To automatically detect and correct all data transmission errors.,To exclusively store boot blocks and system bootstrap programs.,C,"RAID algorithms 'allow more than one drive for operation, allow continued operation/automatic recovery from drive failure'."
What is the primary characteristic of RAID algorithms regarding their organization and benefits?,"They are a single, universal algorithm providing maximum speed.","They are organized into different levels, each providing a combination of reliability and high transfer rates.","They are only used for backup purposes, not active storage.",They exclusively focus on data compression for storage efficiency.,They require manual intervention for any drive failure recovery.,B,RAID algorithms are 'organized into different levels (each provides reliability/high transfer rates combination)'.
Which type of storage is described as being used for big data problems like Internet indexing and cloud photo storage?,Direct Attached Storage (DAS),Network Attached Storage (NAS),Storage Area Network (SAN),Object storage,Solid-state drives (SSDs),D,"Object storage is specified as being 'used for big data problems (e.g., Internet indexing, cloud photo storage)'."
How are objects in object storage primarily addressed?,By a traditional file name and directory path.,By their physical disk location.,By a logical block number.,By an object ID.,By an IP address.,D,Objects are 'addressed by object ID (not file name)'.
What method does object storage typically use for data protection?,RAID striping with parity.,Regular incremental backups to tape.,Replication.,Error correction codes on single copies.,Checksum verification upon retrieval only.,C,Object storage 'Typically uses replication for data protection'.
What is a key characteristic of object storage related to its scalability?,"It is vertically scalable, meaning it can be expanded by adding more powerful single machines.",It is horizontally scalable for vast capacity and easy expansion.,It is limited to a fixed capacity and cannot be expanded.,Its scalability depends entirely on the underlying HDD technology.,It scales by reducing the size of individual objects.,B,Object storage is described as 'Horizontally scalable: for vast capacity and easy expansion'.
What is a characteristic of objects themselves within object storage systems?,They are stored as raw binary data without any metadata.,They are self-defining collections of data.,"They must conform to a rigid, predefined schema.","They are primarily used for small, frequently changing files.",They can only be accessed sequentially from beginning to end.,B,Objects are described as 'self-defining collections of data'.
"In many common computer uses, which function is often considered primary, with computing being incidental?",Data processing,System administration,I/O operations,Software development,Memory management,C,"The text mentions that 'Often, I/O is primary, computing incidental (e.g., browsing, editing)'."
What is the primary role of the Operating System (OS) concerning I/O?,To design new I/O devices,To manage and control I/O operations and devices,To optimize I/O device manufacturing,To provide power to I/O devices,To monitor network traffic only,B,The text states the 'OS role in I/O: manage and control I/O operations and devices'.
Which of the following is NOT explicitly mentioned as a topic covered regarding I/O?,I/O hardware basics and their constraints,OS I/O services and application I/O interface,The UNIX System V STREAMS mechanism,Historical evolution of I/O interfaces,I/O performance and OS design principles for improvement,D,"The text lists specific topics covered, and 'Historical evolution of I/O interfaces' is not among them."
What is identified as a major operating system design concern related to devices?,Device manufacturing cost,Device aesthetics,Device control,Device marketing strategies,Device power consumption,C,The text clearly states: 'Device control is major OS design concern'.
"Why do I/O devices like a mouse, hard disk, and tape robot require varied control methods?",Because they are manufactured by different companies.,Due to their wide variation in function and speed.,To ensure device security.,To comply with international standards.,To minimize power usage.,B,"The text explains that 'Wide variation in I/O device function/speed (mouse, hard disk, flash drive, tape robot) requires varied control methods'."
What is the primary purpose of the kernel's I/O subsystem?,To provide a user interface for I/O operations.,To manage network connections.,To separate the kernel from device management complexities.,To regulate the speed of I/O devices.,To analyze I/O data for security threats.,C,"The text states that control methods 'form kernel's I/O subsystem, separating kernel from device management complexities'."
Which I/O device technology trend helps in incorporating new device generations?,Decreasing cost of hardware components.,Increasing standardization of software/hardware interfaces.,Growing variety of I/O devices.,Miniaturization of devices.,Enhanced power efficiency.,B,The text notes: 'Increasing standardization of software/hardware interfaces: helps incorporate new device generations'.
What challenge do trends in I/O device technology present regarding device incorporation?,Decreasing demand for I/O devices.,Increasing difficulty in designing device drivers.,"Increasingly broad variety of I/O devices, making it challenging to incorporate new, unlike ones.",The rapid obsolescence of current I/O technologies.,Lack of sufficient power supply for new devices.,C,"The text identifies: 'Increasingly broad variety of I/O devices: challenge to incorporate new, unlike devices'."
Which basic I/O hardware elements are cited as capable of accommodating diverse devices?,"CPUs, RAM, and GPUs","Monitors, keyboards, and printers","Ports, buses, and device controllers","Hard drives, SSDs, and optical drives","Network cards, modems, and routers",C,"The text specifies: 'Basic I/O hardware elements (ports, buses, device controllers) accommodate diverse devices'."
How is the kernel structured to address the complexities of diverse I/O devices?,"By using a single, monolithic device management module.",By relying solely on hardware-level management.,By structuring with device-driver modules to encapsulate device details.,By requiring users to manually configure each device.,By limiting the number of supported I/O devices.,C,The text states: 'Kernel structured with device-driver modules to encapsulate device details'.
What uniform interface do device drivers provide to the I/O subsystem?,A programming language interface.,A physical connection interface.,A device-access interface.,A network protocol interface.,A graphical user interface.,C,"The text clarifies that 'Device drivers provide uniform device-access interface to I/O subsystem, similar to system calls for applications'."
"What is the definition of a ""device driver"" as provided in the glossary?",A hardware component that physically controls a device.,A software application used for device troubleshooting.,An OS component providing uniform access and managing I/O to various devices.,A protocol for device-to-device communication.,A utility for measuring device performance.,C,The glossary explicitly defines 'device driver' as 'OS component providing uniform access and managing I/O to various devices'.
"Which specific mechanism is mentioned as a topic covered, involving dynamic driver code pipelines?",Windows Plug and Play,Linux Kernel Modules,UNIX System V STREAMS,macOS I/O Kit,Android Hardware Abstraction Layer,C,The text lists 'UNIX System V STREAMS mechanism: dynamic driver code pipelines' as a covered topic.
"In the context of computer hardware, what is a 'port'?",A type of internal memory for fast data access.,A software interface for network communication protocols.,A connection point for devices to attach to computers.,A specialized processing unit within the CPU.,A temporary storage area for system logs.,C,The text defines a 'port' as a 'Connection point for devices to attach to computers'.
Which statement best describes a 'bus' in computer systems?,A software program that manages device drivers.,A protocol for network communication over wireless links.,"A communication system connecting computer components (CPU, I/O devices) for data and command transfer.",A specific type of input device like a scanner.,A method for cooling internal computer components.,C,"The text defines a 'bus' as a 'Communication system connecting computer components (CPU, I/O devices) for data/command transfer' and a 'set of wires, rigidly defined protocol'."
What is a 'daisy chain' in the context of device communication?,A secure encryption method for data transfer.,"Devices connected in a string (A to B, B to C), usually operating as a bus.",A network topology where all devices connect to a central hub.,A method for distributing power to multiple peripheral devices.,A series of parallel data lines for high-speed transfers.,B,"The text defines 'daisy chain' as 'devices connected in string (A to B, B to C), usually operates as a bus'."
Which bus type typically connects the processor-memory subsystem to fast devices in a PC bus structure?,Expansion bus,USB bus,Serial-attached SCSI (SAS) bus,PCIe bus,Parallel ATA (PATA) bus,D,The text states 'PCIe bus: connects processor-memory subsystem to fast devices'.
"In a typical PC bus structure, what is the primary function of the 'expansion bus'?",Connecting high-speed graphics cards directly to the CPU.,Providing direct access to main memory for the processor.,"Connecting slow devices such as keyboards, serial ports, and USB ports.",Managing network traffic between the computer and external networks.,Interfacing with external storage arrays in a data center.,C,"The text specifies the 'Expansion bus' 'connects slow devices (keyboard, serial, USB ports)'."
"In PCIe technology, what does a 'lane' consist of?",A single wire for unidirectional data flow.,Two signaling pairs (receive/transmit) enabling a full-duplex byte stream.,A shared channel for multiple devices to transmit data sequentially.,A dedicated power line for peripheral components.,A software-defined pathway for data packets.,B,"The text states a 'Lane: two signaling pairs (receive/transmit), full-duplex byte stream'."
"According to the text, what does 'PCIe gen3 x8' signify about the bus's capabilities?","It is the 3rd generation of PCIe, operating at 8 MB/s throughput.","It is the 3rd generation of PCIe, utilizing 8 lanes for a total throughput of 8 GB/s.","It is the 3rd generation of PCIe, with an 8-bit data packet format.","It is the 3rd generation of PCIe, offering 8 Gigabits per second per lane.","It is the 3rd generation of PCIe, with a maximum of 8 physical links.",B,"The text provides the example 'PCIe gen3 x8: 8 GB/s throughput', indicating 'x8' refers to 8 lanes."
What is a 'controller' in the context of I/O hardware?,A software driver that manages device operations.,"An electronic component that operates a port, bus, or device.",A physical cable connecting two devices in a daisy chain.,A memory unit used for buffering I/O data temporarily.,A diagnostic tool for identifying I/O hardware faults.,B,"The text defines 'Controller' as 'electronics operating a port, bus, or device'."
What is a 'host bus adapter (HBA)' primarily used for?,Boosting network signal strength for wireless devices.,Connecting a CPU directly to high-speed memory modules.,Acting as a complex device controller installed in a host bus port for device connection.,Regulating power supply to internal computer components.,Optimizing CPU scheduling for I/O-bound tasks.,C,"The text describes an HBA as a 'Device controller installed in host bus port for device connection', often complex like a Fibre Channel bus controller."
Which of the following tasks is specifically mentioned as being implemented by a disk controller?,Managing operating system boot processes.,Implementing network routing protocols for internet access.,"Bad-sector mapping, prefetching, buffering, and caching.",Allocating CPU time to various user applications.,Translating virtual memory addresses to physical addresses.,C,"The text states, 'Disk controller: implements disk-side protocol (SAS, SATA), microcode, processor for tasks (bad-sector mapping, prefetching, buffering, caching)'."
What is 'memory-mapped I/O'?,A technique where I/O devices have their own dedicated processor cores.,A method where device-control registers are mapped into the processor's address space.,A system where all I/O operations are handled exclusively by special I/O instructions.,A type of memory optimized solely for I/O operations.,A process of caching I/O data in dedicated buffer memory.,B,The text defines 'memory-mapped I/O' as a 'Device I/O method where device-control registers map into processor address space'.
"What is a key advantage of 'memory-mapped I/O', especially for tasks like updating graphics memory?",It reduces the complexity of device driver development.,"It allows the CPU to use standard data-transfer instructions, making large transfers faster.",It provides better security by isolating I/O devices from main memory.,It eliminates the need for any dedicated I/O controllers.,It guarantees real-time performance for all I/O operations.,B,The text notes that 'CPU uses standard data-transfer instructions to read/write registers at mapped locations' and 'Writing millions of bytes to graphics memory faster than millions of I/O instructions'.
Which of the following is NOT typically one of the four registers found in an I/O device controller?,Status register,Control register,Data-in register,Data-out register,Program Counter register,E,"The text lists 'Typically four registers: status, control, data-in, data-out'."
What is the primary function of the 'data-in register' in an I/O device controller?,To send output data from the host to the device.,To indicate the device's current operational state.,To receive input data from the device to be read by the host.,To initiate a command on the device.,To store temporary configuration settings.,C,"The text states, 'Data-in register: read by host for input'."
A host writes to which I/O device control register to start a command or change a device's mode?,Data-in register,Data-out register,Status register,Control register,Error register,D,"The text states, 'Control register: written by host to start command or change device mode'."
What information can typically be read from an I/O device's 'status register'?,Data to be sent to the device.,Commands to change device operating mode.,"States such as command complete, byte available, or error.",The device's unique serial number.,The amount of free memory on the device.,C,"The text states, 'Status register: bits read by host, indicate states (command complete, byte available, error)'."
What is the purpose of FIFO chips sometimes found in I/O controllers?,To prioritize commands from the host.,To implement flow control protocols for network communication.,"To hold several bytes, expanding capacity and buffering data bursts.",To generate interrupt signals to the CPU.,To store firmware for the controller's operation.,C,"The text states, 'Some controllers have FIFO chips: hold several bytes, expand capacity, buffer data bursts'."
What does 'polling' (or busy-waiting) involve in the context of host-controller interaction for I/O?,The host sends an interrupt request to the controller for every data transfer.,"The host continuously reads the status register of a device, waiting for I/O completion.",The controller sends data directly to main memory without CPU intervention.,The host dedicates a separate processor core to handle all I/O operations.,The controller queues up multiple I/O requests for batch processing.,B,The text defines 'polling' as an 'I/O loop where I/O thread continuously reads status waiting for I/O completion' and states 'host is busy-waiting or polling (repeatedly reading status register)'.
Under what condition is 'polling' an efficient method for host-controller interaction?,When the wait for the device to be ready is long.,When there are many other CPU tasks pending concurrently.,"When the controller and device are very fast, and the wait is short.",When data loss due to buffer overflow is a significant concern.,When interrupts are disabled by the CPU for critical sections.,C,"The text states, 'Polling efficient if controller/device fast'."
"What is a significant disadvantage of 'polling' for I/O, especially when the device is slow?",It increases the complexity of device driver development.,It leads to higher power consumption for the peripheral device.,"It is inefficient if the wait is long and other CPU tasks are pending, wasting CPU cycles.",It requires more dedicated hardware components for the I/O operation.,"It cannot detect errors during data transfer, leading to corrupted data.",C,"The text notes, 'Inefficient if wait is long and other CPU tasks pending'."
What is an 'interrupt' in computer systems?,A software function call to a kernel routine for service.,A hardware mechanism for a device to notify the CPU that it needs attention.,A signal indicating a power failure or system shutdown.,A type of memory error that leads to system instability.,A system for debugging application code execution.,B,The text defines 'interrupt' as a 'Hardware mechanism for device to notify CPU it needs attention'.
What action does the CPU take immediately after sensing an asserted 'interrupt-request line'?,It continues executing the current instruction until completion.,It saves its current state and jumps to a predefined interrupt-handler routine.,It clears the interrupt signal and waits for the next instruction cycle.,It starts polling the device's status register repeatedly.,It waits for the device to send data directly to a buffer.,B,"The text states, 'CPU senses interrupt-request line after each instruction... CPU saves state, jumps to interrupt-handler routine at fixed address'."
What type of errors are typically handled by a 'nonmaskable interrupt'?,User program errors like division by zero.,Routine device I/O completions.,"Unrecoverable hardware errors, such as memory errors.",Software requests for operating system services.,Network connection timeouts and disconnections.,C,"The text states 'Nonmaskable interrupt: for unrecoverable errors (e.g., memory errors)'."
Which statement is true about 'maskable interrupts'?,They are exclusively used for critical system failures that cannot be ignored.,They cannot be turned off or delayed by the CPU under any circumstances.,They are primarily used by device controllers and can be temporarily turned off by the CPU.,They always have a higher priority level than nonmaskable interrupts.,"They are only generated by software, not hardware devices.",C,"The text states 'Maskable: can be turned off by CPU for critical sequences, used by device controllers'."
What is the purpose of an 'interrupt vector'?,To store all pending interrupt requests in a queue.,To manage the priority levels of different interrupt sources.,To provide a table of memory addresses of specialized interrupt handlers.,To encrypt interrupt signals for enhanced system security.,To buffer data from I/O devices before it is processed by the CPU.,C,The text defines 'interrupt vector' as a 'table of memory addresses of specialized handlers'.
When is 'interrupt chaining' used in interrupt handling?,When there are fewer devices than available interrupt vector elements.,"To ensure all interrupts are handled by a single, monolithic handler.","When more devices exist than available interrupt vector elements, requiring a list of handlers per vector element.",To combine multiple low-priority interrupts into one high-priority interrupt.,To prevent any interrupt handler from servicing a request out of order.,C,"The text states 'More devices than vector elements: interrupt chaining', where 'each vector element points to head of handler list'."
"In the context of interrupts, what is an 'exception'?",A rare hardware fault that does not trigger an interrupt.,A software-generated interrupt caused by an error or a user program's request for OS service.,A high-priority external interrupt from a critical device.,A signal indicating successful completion of an I/O operation.,A general term for any event that temporarily halts CPU execution.,B,The glossary defines 'exception' as 'Software-generated interrupt by error or user program request for OS service'.
What is the primary role of the 'first-level interrupt handler (FLIH)' compared to the 'second-level interrupt handler (SLIH)'?,"FLIH performs the actual handling of the interrupt, while SLIH handles context switching and queuing.","FLIH is responsible for context switching, state storage, and queuing, while SLIH performs the actual interrupt handling.","FLIH is designed for maskable interrupts, while SLIH is for nonmaskable interrupts.","FLIH runs in user mode, while SLIH always runs in kernel mode.","FLIH manages interrupt priorities, while SLIH interacts directly with the device controller.",B,"The text states, 'First-level interrupt handler (FLIH): context switch, state storage, queuing', and 'Second-level interrupt handler (SLIH): performs actual handling'."
"Which mechanism saves user state, switches to kernel mode, and dispatches to a kernel routine, often triggered by a system call?",Polling,Direct Memory Access (DMA),A hardware interrupt from a device,A trap (software interrupt),Busy-waiting,D,"The text describes a 'Trap' as saving 'user state, switches to kernel mode, dispatches to kernel routine' and notes 'System calls: ... execute software interrupt or trap'."
How does the priority of a 'trap' (software interrupt) typically compare to that of device interrupts?,Traps always have higher priority because they initiate OS services.,Traps always have an equal priority to device interrupts.,"Traps typically have lower priority compared to device interrupts, as they are less urgent.",Trap priority is dynamically assigned based on current system load.,Traps are only handled when no device interrupts are active.,C,"The text states, 'Trap priority: low compared to device interrupts (less urgent)'."
"In a threaded kernel architecture (e.g., Solaris), how are interrupt handlers often implemented?",As independent user-level processes with fixed priorities.,"As kernel threads with high scheduling priorities, allowing preemption and concurrent execution on multiprocessors.","As part of the device driver, running in a continuous polling loop.",As hardware-level logic that bypasses the kernel entirely.,"As single-threaded, non-preemptable routines to ensure atomicity.",B,"The text states, 'Threaded kernel architecture (e.g., Solaris): interrupt handlers as kernel threads, high scheduling priorities, preemption, concurrent execution on multiprocessor'."
"Why is 'programmed I/O (PIO)' considered wasteful for large data transfers, such as from a disk?",It requires too much dedicated memory for buffering.,"It involves the CPU watching status bits and feeding data byte-by-byte, consuming significant CPU time.",It generates an excessive number of interrupt requests.,It can only transfer data in one direction at a time.,It requires specialized I/O instructions that are not efficient on modern CPUs.,B,"The text states, 'For large transfers (e.g., disk), programmed I/O (PIO) (CPU watching status bits, feeding data byte-by-byte) is wasteful'."
What is 'direct memory access (DMA)'?,A CPU feature that allows direct access to graphics memory without a controller.,An operation allowing device controllers to transfer large data directly to/from main memory without main CPU involvement.,A method for the CPU to directly access I/O ports using special instructions.,A caching technique for frequently accessed I/O data in the CPU's L1 cache.,A protocol for network communication between devices on a local area network.,B,The text defines 'direct memory access (DMA)' as an 'Operation allowing device controllers to transfer large data directly to/from main memory'.
How does a host typically initiate a DMA transfer?,By sending a series of I/O instructions for each byte to be transferred.,By continuously polling the DMA controller's status register.,"By writing a DMA command block (including source, destination, and byte count) to memory.",By raising a nonmaskable interrupt to alert the DMA controller.,By directly accessing the device's data-out register with the first byte.,C,"The text states, 'Initiate DMA: host writes DMA command block to memory (source, destination, byte count)'."
What does 'scatter-gather' allow in the context of DMA?,Transferring data to multiple devices simultaneously from a single source.,Specifying a list of non-contiguous sources/destinations in one DMA command.,Collecting multiple small I/O requests into a single large one for efficiency.,Distributing the load across multiple DMA controllers in a system.,Grouping related I/O devices on a single bus to reduce complexity.,B,The text describes a DMA command block that can be 'complex: list of non-contiguous sources/destinations (scatter-gather)'.
How does a DMA controller perform data transfers after being initiated by the CPU?,It requests the CPU to move data one byte at a time.,It uses the CPU's general-purpose registers to temporarily store data.,"It operates the memory bus directly, performing transfers without the main CPU's intervention.",It sends an interrupt to the CPU for each byte transferred to confirm reception.,It copies data from a dedicated cache memory within the CPU.,C,"The text states, 'DMA controller operates memory bus directly, performs transfers without main CPU'."
What is 'double buffering' often used for when transferring data from a device via DMA to a user space target?,To increase the speed of data transfer by using two DMA controllers in parallel.,"To copy data twice (e.g., device to kernel, then kernel to user memory) to manage risks of user space modification.",To store the same data in two different memory locations for redundancy.,To allow simultaneous read and write operations on the same data set.,To avoid 'cycle stealing' from the CPU by pre-fetching data.,B,"The text notes, 'Data to user space: second copy operation (double buffering) from kernel to user memory, inefficient', indicating its use due to modification risk."
What is 'cycle stealing' in the context of DMA?,The CPU gaining unauthorized access to device memory.,"A device (e.g., DMA controller) temporarily seizing the memory bus, preventing CPU access.",A technique to reduce CPU power consumption during idle periods.,The process of optimizing CPU clock cycles for faster execution.,A security vulnerability allowing unauthorized data exfiltration.,B,The text defines 'cycle stealing' as 'DMA controller seizing bus: CPU momentarily prevented from main memory access'.
What is a key characteristic or advantage of 'direct virtual memory access (DVMA)'?,"It allows direct memory access only to physical addresses, without translation.",It requires CPU intervention for every data transfer operation.,It enables transfers between memory-mapped devices using virtual addresses without CPU or main memory involvement.,It bypasses the need for any memory translation units (MMUs).,"It is an older, deprecated technology no longer used in modern computing.",C,"The text states 'DVMA: uses virtual addresses, translates to physical. Can transfer between memory-mapped devices without CPU/main memory'."
"Which bus type is commonly used in data centers to connect computers to storage arrays, as specified in the glossary?",PCIe bus,Expansion bus,Fibre Channel (FC) bus,USB bus,Parallel SCSI bus,C,The glossary defines 'Fibre channel (FC)' as a 'Storage I/O bus used in data centers to connect computers to storage arrays'.
What does the term 'PHY' refer to in the context of networking components mentioned in the glossary?,"A physical cable type, such as Ethernet or Fibre Optic.",The Physical hardware component connecting to a network (OSI layer 1).,A software layer for network protocol processing.,A power supply unit for network devices.,A diagnostic tool for network connectivity issues.,B,The glossary defines 'PHY' as 'Physical hardware component connecting to a network (OSI layer 1)'.
"What does the acronym SAS stand for, and what type of component is it, according to the glossary?",System Area Server; a type of network server.,Secure Access System; a security protocol for data encryption.,Serial-attached SCSI; a common type of I/O bus.,Software Allocation System; a memory management scheme.,Storage Area Subsystem; a component in data storage architectures.,C,The glossary defines 'SAS' as 'Serial-attached SCSI (SAS) Common type of I/O bus'.
"Which data transfer method involves the CPU transferring data one byte at a time, described as wasteful for large transfers?",Direct Memory Access (DMA),Interrupt-driven I/O,Programmed I/O (PIO),Memory-mapped I/O,Scatter-gather I/O,C,The glossary defines 'programmed I/O (PIO)' as 'Data transfer method where CPU transfers data one byte at a time'.
Which term describes a thread or process continuously using the CPU while waiting for an I/O operation to complete by repeatedly reading a status register?,Interrupt handling,Direct memory access,Busy waiting (polling),Context switching,Paging,C,The glossary defines 'busy waiting' as 'Thread/process continuously uses CPU while waiting; I/O loop reading status' and equates it with 'polling'.
Which of the following is explicitly mentioned as a key concept of I/O hardware in the summary?,Virtual memory paging,Application programming interfaces (APIs),Handshaking (host and device controller),Network routing protocols,CPU instruction pipelining,C,The 'I/O hardware summary' explicitly lists 'Handshaking (host and device controller)' as a key concept.
"What is the general term for device-control registers mapped into the processor's address space, allowing the CPU to use standard data-transfer instructions?",Special I/O instructions,Programmed I/O (PIO),Memory-mapped I/O,Direct Memory Access (DMA),Register-based communication,C,The text states 'Memory-mapped I/O: device-control registers mapped into processor address space. CPU uses standard data-transfer instructions to read/write registers at mapped locations'.
"Which register is read by the host to determine the current state of an I/O device, such as whether a command is complete or an error has occurred?",Data-in register,Data-out register,Control register,Status register,Interrupt register,D,"The text states, 'Status register: bits read by host, indicate states (command complete, byte available, error)'."
What is the primary goal of the application I/O interface structuring techniques?,To maximize the speed of I/O operations by bypassing the OS kernel.,"To ensure that all I/O devices have unique, non-standardized interfaces for security.","To treat I/O devices uniformly, abstracting away differences and simplifying OS development.","To allow applications direct, unmediated access to hardware device controllers.",To force hardware manufacturers to use only one specific type of I/O device.,C,The text states the overarching goal is to 'treat I/O devices uniformly' which simplifies OS development and allows for abstracting away device differences.
Which of the following approaches are explicitly mentioned as being utilized to achieve uniform treatment of I/O devices?,"Direct hardware manipulation, unmediated access, and polling.","Abstraction, encapsulation, and software layering.",Custom system calls for every device type and dedicated hardware.,"Batch processing, interrupt masking, and memory-mapped registers.","Kernel-level device drivers, but without standard interfaces.",B,"The text explicitly lists 'Approach: abstraction, encapsulation, software layering' as the methods used."
What is the primary purpose of the device-driver layer within the kernel I/O subsystem?,To directly manage application-level requests for I/O without kernel intervention.,To export non-standardized interfaces unique to each device controller.,To hide differences among device controllers from the kernel I/O subsystem.,To provide a direct interface for applications to bypass the kernel entirely for I/O.,To perform complex computations unrelated to device communication.,C,"The text states, 'Purpose of device-driver layer: hide differences among device controllers from kernel I/O subsystem.'"
A device that transfers bytes one by one is classified as what type of device?,Block device,Random-access device,Character-stream device,Synchronous device,Sharable device,C,The text defines 'Character-stream: transfers bytes one by one.'
What is the distinguishing characteristic of a 'block' device compared to a 'character-stream' device?,"Block devices transfer data asynchronously, while character-stream devices transfer synchronously.","Block devices transfer a block of bytes as a unit, while character-stream devices transfer bytes one by one.","Block devices are always random-access, while character-stream devices are always sequential.","Block devices are typically used for printers, while character-stream devices are for disks.","Block devices are dedicated, while character-stream devices are sharable.",B,The text defines 'Block: transfers a block of bytes as a unit' and 'Character-stream: transfers bytes one by one.'
Which characteristic describes an I/O device where the user can seek to any storage location?,Sequential access,Random-access,Character-stream,Synchronous,Dedicated,B,The text defines 'Random-access: user seeks to any storage location.'
What is the key difference between synchronous and asynchronous I/O devices regarding their response times?,"Synchronous devices have irregular/unpredictable response times, while asynchronous devices have predictable response times.",Synchronous devices are always faster than asynchronous devices.,"Synchronous devices have predictable response times and are coordinated, while asynchronous devices have irregular/unpredictable response times and are not coordinated.","Asynchronous devices require direct polling, while synchronous devices use interrupts.","Synchronous devices are dedicated, while asynchronous devices are sharable.",C,"The text states, 'Synchronous: predictable response times, coordinated. Asynchronous: irregular/unpredictable response times, not coordinated.'"
Which type of device can be used concurrently by several processes or threads?,Dedicated,Sequential,Read only,Sharable,Write once,D,The text defines 'Sharable: used concurrently by several processes/threads.'
Which of the following is NOT listed as a major access convention by which the OS groups devices?,Block I/O,Character-stream I/O,Direct Memory Access (DMA),Memory-mapped file access,Network sockets,C,"The text lists Block I/O, Character-stream I/O, Memory-mapped file access, and Network sockets as major access conventions. DMA is a hardware feature, not an access convention."
What is the purpose of an 'escape' or 'back door' mechanism in an OS I/O interface?,To allow the OS to directly control all network traffic.,To provide a standardized way to access all device functionalities.,To transparently pass arbitrary commands to a device driver when the standard interface lacks a specific method.,To restrict application access to sensitive device functions.,To implement file system caching for block devices.,C,The glossary defines 'escape' and 'back door' as a 'Method of passing arbitrary commands when interface lacks standard method.'
"In UNIX, what is the `ioctl()` system call primarily used for?",Performing standard file `read()` and `write()` operations.,Creating new system calls dynamically.,Enabling an application to access any driver functionality without requiring a new system call.,Managing memory allocation for device buffers.,Synchronizing clocks across multiple devices.,C,The text states `ioctl()` 'Enables application to access any driver functionality without new system call.'
"When using `ioctl()` in UNIX, what does the 'major number' of the device identifier typically represent?","The specific instance of a device (e.g., the second hard drive).",The total number of I/O operations performed on the device.,"The device type, which routes I/O requests to the appropriate driver.",The memory address where the device is mapped.,The speed of the I/O device.,C,"The text specifies, 'Major number: device type, routes I/O requests to driver.'"
What does 'raw I/O' imply when accessing secondary storage?,Accessing data through a standard file system interface with full buffering.,"Direct access to secondary storage as a linear array of blocks, bypassing the file system.",Only read-only access to storage devices.,Accessing data via memory-mapped files exclusively.,Transferring data byte by byte without block aggregation.,B,"The text defines 'raw I/O' as 'Direct access to secondary storage as array of blocks, no file system.'"
What is a key benefit of using raw I/O for applications like database management systems (DBMS)?,It allows for standard file system permissions to be enforced more strictly.,It avoids extra buffering and redundant locking provided by the OS file system.,It automatically encrypts data for secure storage.,It enables byte-by-byte data transfer for maximum granularity.,It simplifies the programming interface by eliminating the need for `seek()` commands.,B,"The text notes, 'Raw I/O avoids extra buffering and redundant locking.'"
How does memory-mapped file access typically operate?,It transfers entire files into main memory at once before any access.,"It accesses disk storage via a byte array in main memory, with data transferred only when needed (demand-paged).","It bypasses main memory entirely, transferring data directly between disk and CPU registers.",It requires applications to manage their own disk block allocation.,"It is exclusively used for character-stream devices, not block devices.",B,"The text states, 'Access disk storage via byte array in main memory. Data transfers only when needed (demand-paged virtual memory access), efficient.'"
Which of the following devices typically uses a character-stream interface?,Disk drives,Solid State Drives (SSDs),Keyboards,USB flash drives,CD-ROM drives,C,"The text lists 'keyboards, mice, modems, printers, audio boards' as examples for character-stream interface."
What is the common interface used for network I/O in operating systems like UNIX and Windows?,`read()`/`write()`/`seek()` interface,Character-stream interface,Network socket,Raw I/O interface,Memory-mapped file access,C,"The text states, 'Common interface: network socket (UNIX, Windows).'"
What is the primary function of the `select()` system call in the context of network sockets?,To establish a new network connection.,To send a packet over a network.,To manage a set of sockets and return information on which ones are ready for I/O.,To close an existing network socket.,To convert network addresses between IPv4 and IPv6.,C,"The text describes `select()` as managing 'set of sockets, returns info on ready sockets (packet waiting, room to send).'"
"How does the `select()` system call improve network I/O efficiency, particularly regarding polling?",It introduces additional busy-waiting to ensure data integrity.,It allows applications to poll constantly without performance impact.,It eliminates polling/busy waiting for network I/O by blocking until I/O is possible.,It forces the kernel to ignore I/O requests for a specified duration.,"It streams data directly to the CPU registers, bypassing kernel buffers.",C,"The text states, '`select()` eliminates polling/busy waiting for network I/O.'"
Which of the following is a key function provided by hardware clocks and timers to the OS and applications?,Managing file system permissions.,Performing context switching between processes.,"Providing current time, elapsed time, and setting timers for future operations.",Encrypting network traffic.,Resolving domain names to IP addresses.,C,"The text states, 'Hardware clocks/timers provide: current time, elapsed time, set timer for operation X at time T.'"
What is the primary mechanism by which a programmable interval timer helps the OS with operations like process preemption or cache flushing?,It directly executes the preemption logic.,"It generates an interrupt after a set period, allowing the OS to react.",It provides a continuous stream of time data for passive monitoring.,It stores user application data temporarily.,It synchronizes external network devices.,B,"The text states it can be 'Set to wait, then generate interrupt (once or periodically)' and is 'Used by scheduler (preempt process), disk I/O (flush dirty cache), network (cancel slow operations).'"
How does an OS typically manage more timer requests from user processes than the available hardware timer channels?,By rejecting any requests beyond the hardware limit.,By assigning a dedicated hardware timer to each user process.,"By simulating virtual clocks, maintaining a sorted list of wanted interrupts, and setting the hardware timer for the earliest.",By distributing timer requests across multiple CPUs without coordination.,By requiring user processes to implement their own timing logic.,C,"The text states, 'Supports more timer requests than hardware channels by simulating virtual clocks. Kernel maintains sorted list of wanted interrupts, sets timer for earliest.'"
Which protocol is specifically mentioned for correcting system clock drift?,TCP/IP,HTTP,Network Time Protocol (NTP),FTP,SMTP,C,"The text explicitly mentions, 'System clock drift corrected by protocols (e.g., network time protocol (NTP)).'"
What defines a 'blocking' system call in the context of I/O?,The calling thread continues execution immediately without waiting for I/O.,The system call returns with partial data if the full request cannot be met immediately.,The calling thread is suspended and moved to a wait queue until the I/O operation completes.,"The I/O operation is performed entirely in hardware, bypassing the OS.",It indicates that the I/O operation has failed.,C,"The text defines 'Blocking system call: calling thread suspended, moved to wait queue. Resumes after completion.'"
What is the main difference in behavior between a 'nonblocking' I/O system call and an 'asynchronous' I/O system call regarding data transfer?,"Nonblocking calls always transfer the full amount of data requested, while asynchronous calls never do.","Nonblocking calls suspend the calling thread, while asynchronous calls return immediately.","Nonblocking `read()` returns immediately with available data (full, fewer, or none), while asynchronous `read()` requests a full transfer to complete later, returning immediately.","Asynchronous calls require manual polling, while nonblocking calls use interrupts.","Nonblocking calls are only for network I/O, while asynchronous calls are for disk I/O.",C,"The text states, 'Difference: nonblocking `read()` returns available data immediately (full, fewer, none); asynchronous `read()` requests full transfer to complete later.' Both return immediately, but their immediate *result* is different regarding data readiness."
How is I/O completion typically communicated to a thread that initiated an asynchronous system call?,The thread remains blocked until completion.,By a return value indicating success or failure immediately.,"Via variable setting, a signal/software interrupt, or a callback.",Through continuous polling by the calling thread.,By logging the completion status to a system file.,C,"The text states, 'I/O completion communicated via variable setting, signal/software interrupt, or callback.'"
"What is the primary benefit of 'Vectored I/O,' also known as 'scatter-gather'?",It allows I/O operations to bypass the kernel entirely.,"It forces data to be transferred to a single, contiguous buffer first.","It performs multiple I/O operations involving multiple locations with one system call, avoiding context-switching and system-call overhead.",It encrypts all data transfers automatically.,It is exclusively used for character-stream devices.,C,The text defines 'Vectored I/O' as 'one system call performs multiple I/O operations involving multiple locations' and lists benefits including 'Avoids context-switching and system-call overhead.'
What is `direct I/O` in UNIX?,"A method for applications to access I/O devices directly, bypassing all OS layers.",A mode on a file that disables OS block features like buffering and locking.,A system call that guarantees immediate data transfer to the CPU registers.,An interface used only for network communication.,A special type of memory mapping for graphical displays.,B,"The text states, 'OS allows mode on file that disables buffering/locking (UNIX: direct I/O).' The glossary further defines it as 'Block I/O bypassing OS block features (buffering, locking).'"
Why do modern operating systems buffer I/O requests and often return to the application before the request fully completes?,To slow down I/O operations and conserve power.,To ensure data corruption by introducing delays.,To optimize performance by allowing the application to continue execution while I/O completes later.,To force applications to implement their own buffering mechanisms.,To prevent multiple applications from accessing the same device.,C,"The text says, 'OS buffers I/O, returns to application, completes request later (optimizes performance).'"
What is a High-Performance Event Timer (HPET) typically used for in modern PCs?,Managing network packet routing.,Storing user passwords.,Providing high-resolution time intervals and triggering interrupts when a value matches.,Controlling the graphical display resolution.,Simulating virtual memory for applications.,C,The text states HPET is a 'Hardware timer provided by some CPUs' and that 'Comparators trigger interrupts when value matches HPET.' It's also mentioned that a hardware clock (like HPET) offers 'accurate time intervals.'
Which of the following is NOT typically provided as a service by the kernel's I/O subsystem?,Scheduling I/O requests,Buffering data transfers,User application logic execution,Device reservation,Error handling for I/O operations,C,"The kernel's I/O subsystem provides services such as scheduling, buffering, caching, spooling, device reservation, and error handling. User application logic execution is the responsibility of the application itself, not the I/O subsystem."
What is a primary responsibility of the kernel's I/O subsystem regarding system integrity?,Managing user interface graphics,Protecting itself from errant processes and malicious users,Optimizing network packet routing,Compiling user-level code,Performing hardware diagnostics at boot time,B,The I/O subsystem is responsible for protecting itself from errant processes and malicious users to maintain system integrity.
What is the main purpose of I/O scheduling within the kernel's I/O subsystem?,To ensure applications always get their I/O requests processed in the exact order they were made.,To determine a good execution order for I/O requests to improve overall system performance and fairness.,To prevent any application from monopolizing an I/O device.,To simply maintain a queue of I/O requests without reordering.,To delegate all I/O processing directly to hardware controllers.,B,"I/O scheduling aims to determine a good execution order for requests, as the application system call order is rarely the best. This reordering improves overall system performance, ensures fair device access, and reduces average waiting time."
Which of the following is a benefit of I/O scheduling?,Increased power consumption of I/O devices.,Decreased responsiveness for delay-sensitive requests.,Reduction in average waiting time for I/O operations.,Elimination of the need for device queues.,Direct user-level access to I/O instructions.,C,"Benefits of I/O scheduling include improving overall system performance, ensuring fair device access, and reducing average waiting time."
How does the OS typically implement I/O scheduling?,By allowing applications to directly control device access order.,By maintaining a wait queue for each device and rearranging it for efficiency.,By immediately fulfilling every I/O request in FIFO order.,By offloading all scheduling decisions to the device hardware.,By only processing one I/O request at a time system-wide.,B,The OS implements I/O scheduling by maintaining a wait queue for each device and rearranging it to optimize for efficiency and average response time.
The kernel's I/O scheduler may prioritize certain requests. Which type of request is explicitly mentioned as potentially receiving higher priority?,Requests from general user applications.,Requests involving network communication only.,"Delay-sensitive requests, such as those from the virtual memory subsystem.",Requests for printing documents.,Requests that involve sequential disk reads.,C,"The OS may prioritize delay-sensitive requests, such as those originating from the virtual memory subsystem, over requests from standard applications."
What is a `device-status table` in the context of the kernel I/O subsystem?,A table used by applications to check device compatibility.,A kernel data structure that tracks the status and queues of operations for I/O devices.,A hardware component that stores device driver code.,A log of all I/O errors that have occurred since system boot.,A user-level interface for configuring new devices.,B,"A `device-status table` is a kernel data structure that tracks the status and queues of operations for each I/O device, particularly for asynchronous I/O."
What information is typically stored in an entry of the `device-status table` for an I/O device?,Only the device's unique serial number.,"The device's physical address, current state, and the type/parameters of any pending requests if busy.","The manufacturer, warranty information, and last service date of the device.",A list of all applications that have ever used the device.,The total amount of data ever transferred by the device.,B,"An entry in the `device-status table` typically includes the device type, its address, and its state (e.g., not functioning, idle, or busy). If the device is busy, the request type and parameters are also stored in its table entry."
What is the definition of a `buffer` in the context of the kernel I/O subsystem?,A specialized hardware component used for accelerating network traffic.,A memory area used for temporary storage of data being transferred between devices or between a device and an application.,A mechanism for encrypting data before it is written to disk.,A type of physical disk sector reserved for system files.,A software utility for compressing data.,B,A `buffer` is defined as a memory area that stores data being transferred between two devices or between a device and an application.
One key reason for using buffering in I/O operations is to cope with:,Insufficient main memory for applications.,Speed mismatch between the producer and consumer of data.,Excessive CPU utilization during I/O operations.,Hardware failures in I/O devices.,Security vulnerabilities in data transfer protocols.,B,"Buffering is primarily used to cope with the speed mismatch between the producer and consumer of data, allowing data to accumulate at one speed and be processed at another."
What is `double buffering`?,Using a single buffer that is twice the normal size.,"A technique where two buffers are used in alternation to decouple producer and consumer operations, relaxing timing constraints.",A method of writing data to two separate disks simultaneously for redundancy.,A process of verifying data integrity by reading it back into a second buffer.,Storing data both in a buffer and a cache at the same time.,B,"`Double buffering` involves using two buffers to decouple the producer and consumer, allowing one buffer to be filled while the other is being emptied, thereby relaxing timing requirements."
What does `copy semantics` for application I/O guarantee?,That the application's buffer is always a direct copy of the kernel's buffer.,"That data written to disk is the version that existed at the time of the system call, independent of subsequent application buffer changes.",That the OS never copies data between kernel and application space.,That all I/O operations are handled through a copy-on-write mechanism.,That data is never duplicated in memory.,B,"`Copy semantics` ensures that the version of data written to disk is exactly what was in the application's buffer at the moment the system call was made, even if the application modifies its buffer immediately thereafter."
How does an Operating System typically guarantee `copy semantics` for a `write()` system call?,By forcing the application to wait until the disk write is complete.,By marking the application's buffer as read-only after the call.,"By copying the application data to a kernel buffer before returning from the system call, and then writing from the kernel buffer to disk.",By directly writing from the application's buffer to the disk hardware.,By using a special hardware register to store the data temporarily.,C,"To guarantee `copy semantics`, the OS typically copies the application data into a kernel buffer immediately after the `write()` system call is made (and before returning to the application). The actual disk write then occurs from this kernel buffer."
What is the primary characteristic of a `cache`?,It is a permanent storage location for frequently accessed files.,"It is a region of fast memory that holds temporary copies of data, making access more efficient.",It is a dedicated processing unit for I/O operations.,It is a mechanism for encrypting data during transfer.,It is a component that converts data formats between different devices.,B,"A `cache` is defined as a region of fast memory that holds copies of data, which are typically found elsewhere, to improve performance by providing quicker access to those copies."
What is the key difference between a `buffer` and a `cache`?,"Buffers are always in main memory, while caches are always in CPU memory.","A buffer may hold the *only* copy of data during transfer, while a cache holds a *copy* of data that also resides elsewhere.","Caches are used for write operations, while buffers are used for read operations.","Buffers are managed by hardware, while caches are managed by the operating system.",There is no functional difference; the terms are interchangeable.,B,"The text states: 'buffer may hold only copy, cache holds copy of item residing elsewhere.' This is the key distinguishing characteristic, even though a memory region can serve both roles simultaneously."
How can main memory buffers used by the OS for disk data also act as a cache?,By converting the data format to a cached version.,By simply being a memory area; they don't serve as a cache.,By accumulating disk writes for efficient schedules and allowing the kernel to check them for file I/O requests to avoid/defer physical disk I/O.,By holding encrypted versions of the data.,By transferring data directly to the CPU's internal cache.,C,"OS uses main memory buffers for disk data (for copy semantics and efficient scheduling). These buffers also act as a cache by accumulating disk writes for seconds (allowing efficient write schedules) and by allowing the kernel to check them for file I/O requests, avoiding or deferring physical disk I/O if the data is already present."
What is a `spool` in the context of the kernel I/O subsystem?,A special type of high-speed network connection.,"A buffer that holds output for a device, like a printer, that cannot accept interleaved data streams.",A utility for defragmenting disk space.,A security feature that prevents unauthorized access to I/O devices.,A program that monitors CPU temperature.,B,"A `spool` is defined as a buffer holding output for a device (e.g., printer) that cannot accept interleaved data streams, allowing multiple applications to 'print' concurrently."
Why is spooling used for devices like printers?,To increase the printing speed of a single job.,To allow multiple applications to send output concurrently to a device that can only serve one job at a time.,To compress print jobs before sending them to the printer.,To provide real-time feedback on printer status to applications.,To ensure that print jobs are always processed in the order they were submitted by applications.,B,"Spooling addresses the problem that devices like printers can only serve one job at a time, while multiple applications may attempt to print concurrently. It coordinates this concurrent output by buffering jobs."
How does the spooling system typically manage concurrent print jobs?,It directly interweaves output from multiple applications to the printer.,It rejects print requests if the printer is currently busy.,"It intercepts each application's output, spools it to a separate file on secondary storage, queues these files, and then copies them to the printer one at a time.",It requires applications to wait until the printer is free before sending any output.,It sends a small part of each job to the printer in a round-robin fashion.,C,"The OS intercepts printer output, spools each application's output to a separate secondary storage file. Once an application finishes, the spooling system queues its spool file for printer output and copies these queued files to the printer one at a time."
"For which type of devices is explicit coordination and exclusive device access typically required, as they cannot multiplex I/O requests?",Solid State Drives (SSDs).,Network Interface Cards (NICs).,Graphics Processing Units (GPUs).,Tape drives and printers.,Keyboards and mice.,D,"The text explicitly states that 'Some devices (tape drives, printers) cannot multiplex I/O requests' and thus require explicit coordination facilities like exclusive device access."
"When an OS provides functions for processes to coordinate exclusive device access, what is the application's responsibility?",To handle all hardware-level device communication directly.,To avoid deadlock scenarios among competing requests for the device.,To perform its own I/O scheduling.,To manage the device's power state.,To ensure that its data is buffered by the OS.,B,"The text states, 'Applications responsible for avoiding deadlock' when using explicit coordination facilities for exclusive device access."
What is a primary goal of error handling within a protected memory Operating System?,To allow applications to directly recover from all hardware failures.,To ensure that minor malfunctions or errant applications do not cause system failure.,To permanently disable faulty hardware components.,To notify users of every single error event immediately.,To revert the system to a previous state every time an error occurs.,B,A protected memory OS guards against hardware/application errors to prevent system failure from minor malfunctions. It aims to compensate for transient failures and provide error information.
How does the OS typically handle transient I/O failures compared to permanent I/O failures?,It attempts to recover from both transient and permanent failures with equal success.,It ignores transient failures and attempts to recover only from permanent ones.,"It compensates for transient failures (e.g., retries) but is unlikely to recover from permanent failures of important components.",It immediately crashes the system upon detecting any type of I/O failure.,It passes all error recovery responsibility directly to the application for both types of failures.,C,"The OS compensates for transient failures (e.g., disk `read()` retry, network `send()` resend) but is unlikely to recover from permanent failures of important components."
"In UNIX systems, how does an I/O system call typically report error codes to an application?",By displaying a pop-up error message on the screen.,By writing a detailed error log to a file that the application must parse.,"By returning a success/failure bit, and if failure, setting an integer variable named `errno` with a specific error code.",By sending a network packet containing the error information.,By causing the application to terminate immediately.,C,"UNIX systems typically report I/O errors by returning a success/failure bit from the system call, and if it's a failure, setting the `errno` integer variable with a specific error code."
"According to the SCSI protocol error reporting, what does the `sense key` indicate?",The specific data block that caused the error.,"The general nature of the failure, such as a hardware error or an illegal request.",The exact parameter within a command that was incorrect.,The current temperature of the SCSI device.,The time and date the error occurred.,B,"The `sense key` in SCSI protocol error reporting indicates the general nature of the failure, such as a hardware error or an illegal request."
"In SCSI protocol error reporting, what level of detail does the `additional sense-code qualifier` provide?","It indicates the broad category of the failure, like a bad command parameter.",It specifies if the error is transient or permanent.,"It offers more specific detail about the failure, such as which parameter was bad or which subsystem failed.",It provides the contact information for technical support.,It identifies the firmware version of the SCSI device.,C,"The `additional sense-code qualifier` provides more specific detail about the failure, for example, indicating which parameter was bad or which subsystem failed."
How does the OS primarily prevent user processes from executing illegal I/O instructions directly?,By requiring all I/O instructions to be written in a special programming language.,"By making all I/O instructions privileged, requiring user programs to use system calls that transition to monitor mode.",By having user processes simulate I/O instructions without actually executing them.,By physically disconnecting I/O devices when a user process is running.,By encrypting all I/O instructions.,B,"All I/O instructions are privileged. User programs cannot issue them directly but must use system calls. The OS then performs the I/O in monitor (kernel) mode after checking its validity, returning control to the user."
How are memory-mapped I/O locations and I/O ports typically protected from user process access?,Through a special password system required for each access.,By physically isolating the memory regions from the CPU.,"By the memory-protection system, which restricts user access to these specific memory areas.",By requiring users to physically unplug and replug the I/O device.,They are not protected; any user process can access them freely.,C,"Memory-mapped and I/O port memory locations are protected from user access by the memory-protection system, which is typically part of the operating system's memory management unit."
"Why might a kernel provide a mechanism for controlled direct user access to specific I/O memory, even though most I/O is privileged?",To simplify the kernel's internal I/O management.,To increase system security by offloading responsibility to user processes.,"Because certain applications, like graphics games, require high-performance direct access to memory-mapped graphics memory.",To reduce the overall memory footprint of the operating system.,To allow users to develop their own device drivers easily.,C,"The kernel cannot deny all user access. For example, graphics games need direct, high-performance access to memory-mapped graphics memory for rendering. In such cases, the kernel might provide a locking mechanism to allocate a specific section to one process at a time."
What is the primary purpose of kernel maintaining in-kernel data structures for I/O components?,To store user application data.,"To keep state information on I/O components, such as device status, open files, and network connections.",To store device driver executable code.,To record all user login attempts.,To perform arithmetic calculations for the CPU.,B,"The kernel keeps state information on I/O components via in-kernel data structures, tracking things like open files, network connections, and character-device communications."
"How does UNIX typically handle the differing `read()` semantics for various entities like user files, raw devices, and process address spaces?",It requires each application to implement its own `read()` logic for each entity.,It uses a separate kernel system call for each distinct entity type.,"It encapsulates these differences using an object-oriented technique, where an open-file record contains a dispatch table with pointers to appropriate routines based on file type.",It converts all entities to a common data format before `read()` operations.,It delegates the handling of `read()` semantics entirely to the device drivers.,C,UNIX encapsulates the differences in `read()` semantics for various entities using an object-oriented technique. An open-file record contains a dispatch table with pointers to the appropriate routines based on the file type.
Which operating system is mentioned as using a message-passing approach for I/O requests?,Linux,macOS,UNIX,Windows,Android,D,"The text states: 'Some OS use message-passing for I/O (e.g., Windows).'"
"What is a key benefit of using a message-passing approach for I/O, despite potential overhead compared to procedural techniques?",It guarantees faster I/O completion times for all requests.,"It simplifies the I/O system structure and design, adding flexibility.",It eliminates the need for device drivers.,It inherently provides copy semantics without extra steps.,It significantly reduces the kernel's memory footprint.,B,"Despite potential overhead, message-passing for I/O offers benefits such as simplifying the I/O system structure and design, and adding flexibility."
"In data centers, what is a major driving factor for the operating system's role in power management?",Maximizing individual CPU core performance.,Increasing data storage capacity.,"Minimizing power costs, greenhouse gas emissions, and heat generation.",Improving network bandwidth.,Ensuring continuous uptime regardless of power consumption.,C,"The text highlights that in data centers, power costs, greenhouse gas emissions, and heat generation (and associated cooling costs) are significant concerns, making OS power management crucial."
Which of the following best describes Android's 'power collapse' state?,A full system shutdown requiring a manual restart.,"A deep sleep state that uses marginally more power than off, but responds to external stimuli and wakes quickly.","A state where only the screen is turned off, with all other components fully active.",A mode where the CPU runs at maximum frequency for performance.,"A state used for data backup, where I/O is paused indefinitely.",B,"Android's 'power collapse' is described as a deep sleep state that uses marginally more power than off, but can respond to external stimuli and allows for quick wake-up."
How does Android's component-level power management typically determine when to enter 'power collapse'?,It relies solely on a user-defined timer for sleep mode.,It enters power collapse only when the battery level is critically low.,"It tracks component usage via device drivers; if a component is unused, it's turned off, and if all components in the device tree are unused, the system enters power collapse.",It requires applications to explicitly notify the OS when they are idle.,It analyzes network activity and enters power collapse only when there is no incoming traffic.,C,"Android builds a device tree and associates components with device drivers that track usage. If a component is unused, it's turned off. If all components on a bus are unused, the bus is off. If all components in the device tree are unused, the system enters power collapse."
What is the primary purpose of 'wakelocks' in Android's power management?,To lock the device screen to prevent accidental touches.,To temporarily prevent the system from entering a low-power 'power collapse' state when an application needs to perform work.,To ensure that only one application can access a device at a time.,To encrypt data before the device enters a sleep state.,To measure the power consumption of individual applications.,B,"`Wakelocks` allow applications to temporarily prevent the system from entering a low-power 'power collapse' state when they need to keep the CPU or certain components active, for example, during an application update."
What is ACPI (Advanced Configuration and Power Interface)?,A type of network protocol for data centers.,A proprietary Android power management component.,An industry standard firmware that provides callable routines for the kernel to manage hardware aspects like device state and power.,A software library for graphics rendering.,A physical connector for external I/O devices.,C,"ACPI (Advanced Configuration and Power Interface) is an industry standard firmware that provides callable routines for the kernel, facilitating device state discovery, management, error management, and power management."
What is the primary role of ACPI firmware in modern computers regarding power management?,It directly powers off individual CPU cores without OS involvement.,It serves as an interface through which the kernel can call routines to manage device power states and other hardware aspects.,It is responsible for compiling the device tree at boot time.,It exclusively handles power delivery to the motherboard components.,It logs power consumption data for user analysis.,B,"ACPI provides callable routines for the kernel to interact with and manage various hardware aspects, including device state discovery, error management, and power management. The kernel calls device drivers, which in turn call ACPI routines to communicate with the hardware."
"Which of the following aspects is supervised by the kernel's I/O subsystem, as stated in the summary?",User application code debugging.,Financial transaction processing.,Device-driver configuration and initialization.,Web browser rendering.,Predictive analytics for system failures.,C,"The kernel I/O subsystem supervises several aspects, including device-driver configuration and initialization, alongside management of namespaces, access control, buffering, scheduling, and error handling."
How do upper levels of the I/O subsystem typically access devices?,Through direct memory access by applications.,Via a uniform interface provided by device drivers.,By sending raw commands directly to hardware controllers.,Only through message-passing mechanisms.,By user-defined callback functions.,B,"Upper levels of the I/O subsystem access devices via a uniform interface from device drivers, abstracting away hardware specificities."
"How does an operating system typically enable an application to access data stored on a disk, starting from the application's perspective?",By directly mapping the application's memory to disk sectors.,"Through a file name, which the file system then maps to physical disk blocks.",By using hardware-specific addresses provided directly by the application.,Applications send raw I/O commands to the disk controller.,The application manually translates file names into inode numbers.,B,"The text states, 'Application refers to data by file name. File system maps file name through directories to space allocation.'"
"In MS-DOS using the FAT file system, how does a file name primarily relate to the physical storage on disk?",The file name directly indicates the memory-mapped register of the disk controller.,"The file name is mapped to an inode number, which contains space allocation information.","The file name, via a number, indicates an entry in a file-access table that tells which disk blocks are allocated.","The first part of the file name points to a device driver, and the rest to an offset.",It uses a mount table to associate the file name with a device.,C,"The text explains, 'MS-DOS for FAT: name maps to number, indicates entry in file-access table, tells which disk blocks allocated.'"
How does the UNIX operating system typically map a file name to its physical storage allocation information?,The file name is directly converted into a major and minor device number.,"The file name is mapped to an inode number, which then contains the space-allocation information.","The file name specifies a port address, which is then looked up in a device table.",A mount table entry directly provides the disk block addresses for the file.,It relies on a separate device name space to identify the disk location.,B,"The text states, 'UNIX: name maps to inode number, inode contains space-allocation info.'"
Which characteristic describes the device naming convention in MS-DOS for FAT file systems?,Device names are fully integrated into the regular file-system name space.,Device names are resolved using a mount table lookup for path prefixes.,The first part of the file name (before a colon) identifies the hardware device.,Devices are identified by major and minor device numbers within the file name.,It uses a device table that maps file names directly to port addresses.,C,"The text specifies, 'MS-DOS for FAT: first part of file name (before colon) identifies hardware device (e.g., `C:` for primary hard disk).'"
What is a key difference in how MS-DOS (FAT) and UNIX handle their device name spaces relative to the file-system name space?,"MS-DOS integrates device names into the file system, while UNIX keeps them separate.","MS-DOS uses major/minor device numbers for all device naming, unlike UNIX.","MS-DOS has a separate device name space (due to colon separator), while UNIX incorporates it into the regular file-system name space.","UNIX uses a mount table for device naming, whereas MS-DOS relies on direct port addresses in file names.","Both systems fully integrate device names, but use different lookup mechanisms.",C,"MS-DOS has a 'Device name space separate from file-system name space (due to colon separator),' while 'UNIX: device name space incorporated in regular file-system name space.'"
"In UNIX, what is the primary function of the `mount table`?",To store inode numbers for all files on the system.,To map file names directly to physical disk block addresses.,To associate path name prefixes with specific device names.,To provide a list of all currently open file descriptors for processes.,To convert major device numbers into minor device numbers.,C,"The text states, 'UNIX uses mount table: associates path name prefixes with specific device names.' The glossary further defines it as tracking file systems and access for mounted volumes."
"When resolving a path name to a device in UNIX, what is typically found after looking up the device name itself?",The inode number for the device.,The direct memory-mapped address of the device controller.,"A `<major, minor>` device number.",The file-access table entry.,The corresponding file descriptor.,C,"The text explains, 'Lookup device name: finds `<major, minor>` device number, not inode.'"
What is the purpose of the major device number in UNIX-like systems?,"To identify the specific instance of a device (e.g., the second hard drive).",To indicate the size of the device's buffer cache.,To identify the device driver responsible for handling I/O for that device type.,To specify the hardware port address of the device controller.,To signify whether the device is block-oriented or character-oriented.,C,"The text explicitly states, 'Major device number: identifies device driver to handle I/O.'"
What is the role of the minor device number in UNIX-like systems?,"To identify the generic type of device, such as 'disk' or 'printer'.",To specify the base address in memory where the device's data is stored.,To be passed to the device driver to index into a device table for a specific device instance.,To determine the interrupt request line (IRQ) used by the device.,To indicate whether the device requires direct memory access (DMA).,C,"The text says, 'Minor device number: passed to device driver to index into device table.'"
"After obtaining the major and minor device numbers in UNIX, what information does the device-table entry typically provide?","The file system type (e.g., ext4, FAT32).",The number of disk blocks allocated to the device.,The path to the device driver's executable file.,The port address or memory-mapped address of the device controller.,"The current status of the device (e.g., busy, idle).",D,"The text states, 'Device-table entry: gives port address or memory-mapped address of device controller.'"
What flexibility do modern operating systems often provide regarding device drivers?,They require kernel recompilation for every new device added.,They load all possible device drivers at boot time regardless of connected hardware.,They allow new devices and drivers to be introduced without kernel recompilation and can load drivers on demand.,Device drivers are exclusively managed by user-space applications.,They prevent any dynamic changes to device drivers after boot.,C,The text highlights that 'New devices/drivers can be introduced without kernel recompilation' and 'Some OS load device drivers on demand.'
How might an operating system detect and load a driver for a device added *after* boot time?,By waiting for manual user input to install the driver.,The kernel inspects the device upon detecting an error related to it and loads the driver dynamically.,By performing a full system reboot to rescan all buses.,It requires the application to explicitly call a driver loading function.,"Drivers for such devices must be pre-loaded, or they cannot be used.",B,"The text states, 'Devices added after boot: detected by error, kernel inspects, loads driver dynamically.'"
What is the very first step in the life cycle of a blocking `read()` system call for an opened file?,The device driver immediately allocates kernel buffer space.,The kernel system-call code checks parameters and looks for data in the buffer cache.,The process is directly placed on a wait queue for the device.,The I/O subsystem sends a request to the device controller.,The DMA controller begins transferring data.,B,"The process begins by issuing the `read()` call, and then 'Kernel system-call code checks parameters. If data in buffer cache, data returned, I/O completed.'"
"If a physical I/O operation is required for a blocking `read()` request because the data is not in the buffer cache, what happens to the requesting process?","It continues executing, polling the device for completion.",It is terminated to free up resources.,It is removed from the run queue and placed on a wait queue for the device.,It is immediately moved to the ready queue to await CPU assignment.,It spawns a new thread to handle the I/O asynchronously.,C,"The text explains, 'Else, physical I/O performed. Process removed from run queue, placed on wait queue for device. I/O request scheduled.'"
"After receiving an I/O request from the kernel's I/O subsystem, what is a key action performed by the device driver in the context of a blocking read?",It directly transfers data to the application's address space.,It unblocks the requesting process by moving it to the ready queue.,It allocates kernel buffer space and sends commands to the device controller by writing to its registers.,It receives interrupts from the DMA controller and stores data in the buffer cache.,It updates the mount table with new device information.,C,"Step 5 states, 'Device driver allocates kernel buffer space, schedules I/O. Sends commands to device controller by writing to device-control registers.'"
"In the lifecycle of a blocking read request, what is the primary responsibility of the device controller?",To manage the system's buffer cache.,To operate the device hardware for data transfer.,To determine which device driver to load.,To place the requesting process on the wait queue.,To handle interrupts from other devices.,B,"Step 6 indicates, 'Device controller operates device hardware for data transfer.'"
"How is the completion of a data transfer from a device to kernel memory often signaled, particularly when Direct Memory Access (DMA) is used?",The device driver continuously polls the device status.,The application process checks a shared memory flag.,The DMA controller generates an interrupt upon transfer completion.,The kernel periodically checks a global status variable.,The device controller sends a direct signal to the requesting process.,C,"Step 7 mentions, 'DMA controller generates interrupt on transfer completion.'"
"Upon receiving an interrupt from a DMA controller, what is a key action taken by the correct interrupt handler?",It immediately unblocks the requesting process.,It re-schedules the I/O request for later.,"It stores the transferred data, signals the device driver, and then returns.",It reconfigures the device controller.,It modifies the major and minor device numbers.,C,"Step 8 details, 'Correct interrupt handler receives interrupt via interrupt-vector table, stores data, signals device driver, returns.'"
"After the device driver signals the kernel I/O subsystem that an I/O request has completed, what are the kernel's final steps to complete the blocking `read()`?",It terminates the process and cleans up resources.,It only updates the device's status in the device table.,It transfers data/return codes to the requesting process's address space and moves the process from the wait queue to the ready queue.,It reloads the device driver to ensure readiness for the next request.,It modifies the file system's allocation table.,C,"Step 10 outlines, 'Kernel transfers data/return codes to requesting process's address space. Moves process from wait queue to ready queue.'"
"According to the glossary, what is a `mount table`?",A physical table where hard drives are installed.,A list of all active processes and their file descriptors.,"An in-memory data structure with info about each mounted volume, tracks file systems and access.",A temporary buffer for I/O requests before they are sent to the device controller.,A table that maps interrupt requests to their respective handlers.,C,"The glossary defines 'mount table' as 'In-memory data structure with info about each mounted volume, tracks file systems and access.'"
What is the primary purpose of the STREAMS mechanism in UNIX System V?,To provide a secure encryption layer for inter-process communication.,To enable dynamic assembly of driver code pipelines.,To manage virtual memory allocation for user processes.,To facilitate distributed file system access across networks.,To optimize CPU scheduling algorithms for real-time applications.,B,"The STREAMS mechanism enables dynamic assembly of driver code pipelines, as stated in the text and glossary definition."
A 'stream' in the STREAMS mechanism represents a full-duplex connection between which two components?,A user process and a network interface card.,A device driver and a user-level process.,Kernel space and user space.,Two distinct device drivers within the kernel.,Two user-level processes for inter-process communication.,B,The text defines a stream as a 'full-duplex connection between device driver and user-level process'.
Which of the following are the fundamental architectural components of a STREAMS connection?,"Stream head, device files, and read/write system calls.","Stream head, stream modules, and driver end.","User process, kernel, and physical device.","Read queue, write queue, and message buffer.","Input buffer, output buffer, and control logic.",B,"The components listed are 'Stream head', 'Driver end', and 'Zero or more stream modules'."
What is the primary function of the 'stream head' component in a STREAMS mechanism?,Controlling the physical hardware device.,Providing modular processing functionality for data.,Interfacing directly with the user-level process.,Responding to hardware interrupts from the device.,Managing the flow of data between modules.,C,The text states the 'Stream head: interfaces with user process'.
What is the main responsibility of the 'driver end' in a STREAMS connection?,To provide a user-level interface for application programs.,To encapsulate processing logic for data transformation.,To control the actual hardware device.,To manage message exchange between adjacent queues.,To handle asynchronous I/O operations from user processes.,C,The text specifies that the 'Driver end: controls the device'.
How is data primarily transferred between components within a STREAMS connection?,Through direct memory access (DMA) operations.,Using shared memory segments between components.,Via message passing between queues.,By direct function calls between kernel modules.,Through atomic operations on shared variables.,C,The text states 'Data transfer: message passing between queues'.
How are stream modules typically added or 'pushed' onto a stream by a user process?,By modifying kernel boot parameters.,Using the `open()` system call.,Via the `ioctl()` system call.,Through an `exec()` call with specific arguments.,Automatically by the operating system upon device access.,C,Modules are 'pushed' onto a stream using `ioctl()` system call.
What is the main purpose of 'flow control' in STREAMS modules?,To ensure messages are delivered in a specific order.,To prevent queue overflow by regulating data flow.,To encrypt data messages for secure communication.,To compress data before sending it to the device.,To log all data transfers for debugging purposes.,B,The text states flow control is used 'To prevent queue overflow'.
"If a STREAMS queue does NOT support flow control, how does it typically behave when receiving messages?",It buffers messages until sufficient space becomes available.,It immediately drops any incoming messages without processing.,It accepts all messages and immediately sends them to the adjacent queue without buffering.,It signals the sending queue to pause its data transmission.,It redirects overflow messages to a system-wide error log.,C,"Without flow control, a queue 'accepts all messages, immediately sends to adjacent queue without buffering'."
How does a STREAMS queue behave when it *does* support flow control?,It accepts all messages regardless of its current buffer space.,It immediately drops messages if the adjacent queue is busy.,It buffers messages and does not accept new messages without sufficient buffer space.,It signals the user process directly to halt data transmission.,It converts all incoming messages into an unstructured byte stream before processing.,C,"With flow control, a queue 'buffers messages, does not accept messages without sufficient buffer space'."
Which system call allows a user process to write raw data directly to a STREAMS connection?,`getmsg()`,`read()`,`ioctl()`,`putmsg()`,`write()`,E,`write()`: writes raw data to stream.
"A user process wants to send a specific structured message, not just raw data, to a device via STREAMS. Which system call should it use?",`read()`,`getmsg()`,`putmsg()`,`write()`,`poll()`,C,`putmsg()`: allows user to specify a message.
"When a user process reads data from a STREAMS connection using the `read()` system call, what is typically returned by the stream head?",A structured message with header and data fields.,"An ordinary, unstructured byte stream.",A control message indicating the stream's status.,An error code if no complete message is available.,The entire content of the read queue as a single block.,B,"`read()`: stream head gets message, returns ordinary data (unstructured byte stream)."
"Which system call is used by a user process to retrieve a complete message, rather than an unstructured byte stream, from a STREAMS connection?",`write()`,`putmsg()`,`getmsg()`,`ioctl()`,`select()`,C,`getmsg()`: message returned to process.
How is STREAMS I/O generally characterized regarding its blocking behavior?,It is always synchronous and blocking for all operations.,It is always asynchronous and nonblocking.,It is asynchronous (nonblocking) except when communicating with the stream head.,"It blocks only during read operations, never during writes.","It blocks only during write operations, never during reads.",C,STREAMS I/O is asynchronous (or nonblocking) except when communicating with stream head.
Under what condition might a user process block when writing data to a STREAMS connection?,If the device driver is not yet initialized.,If the stream head has no room for the message to be copied.,If the next queue in the stream uses flow control and has no room to copy the message.,If the `write()` call is used instead of `putmsg()`.,If another user process is simultaneously reading from the same stream.,C,Writing to stream: user process blocks (if next queue uses flow control) until room to copy message.
"How does the driver end typically handle incoming data compared to the stream head, concerning flow control and blocking?",Both the driver end and stream head are designed to block if their buffers are full.,"The driver end must handle all incoming data, unlike the stream head which may block.","The stream head must handle all incoming data, unlike the driver end which may block.",Neither can block; they both always drop data if full.,"Only the stream head supports flow control, the driver end does not.",B,"Unlike stream head (may block), driver end must handle all incoming data. Drivers must support flow control."
What commonly happens if a device's internal buffer becomes full when processing data from a STREAMS connection?,The device driver automatically expands its buffer size.,The device typically drops the incoming messages.,"The stream immediately terminates, requiring re-initialization.",The user process is notified to re-send the data at a later time.,Flow control mechanisms are temporarily disabled to force data through.,B,"If device buffer full: device typically drops incoming messages (e.g., network card)."
What is cited as a major benefit of the STREAMS mechanism in UNIX systems?,It provides a robust security layer for all kernel-level I/O operations.,It eliminates the need for physical device drivers in the kernel.,"It offers a framework for modular, incremental device drivers and network protocols.",It guarantees hard real-time performance for all connected devices.,"It simplifies memory management, reducing memory footprint for I/O.",C,"Benefit of STREAMS: framework for modular, incremental device drivers and network protocols."
A key advantage of STREAMS modules is their reusability. What does this mean in practice?,Modules can be dynamically loaded and unloaded without system reboot.,A single module can be used by different streams or for different devices.,"Modules can be written in any programming language, independent of the kernel language.",Modules can automatically adapt to new hardware without modification.,"Modules can be combined into a single, monolithic driver for performance.",B,"Modules reusable by different streams/devices (e.g., networking module for Ethernet and 802.11 wireless)."
"Beyond just unstructured byte streams, what additional information does STREAMS support transfer between modules?",File permissions and ownership details.,CPU utilization metrics for each module.,Message boundaries and control information.,Process IDs and user credentials for security.,Detailed kernel stack trace data for debugging.,C,STREAMS 'Supports message boundaries and control info between modules (not just unstructured byte stream)'.
Which well-known mechanism is mentioned as being implemented using STREAMS in System V UNIX and Solaris?,Virtual memory paging.,Process scheduling algorithms.,File system journaling.,The socket mechanism.,Inter-process communication (IPC) shared memory.,D,Example: System V UNIX and Solaris implement socket mechanism using STREAMS.
Which of the following is identified as a major factor significantly impacting overall system performance?,CPU clock speed,Memory bus bandwidth,I/O operations,GPU processing power,Disk storage capacity,C,The text explicitly states: 'I/O: major factor in system performance.'
Heavy demands on the CPU due to I/O primarily involve which two activities?,Executing user application code and managing virtual memory,Performing floating-point calculations and handling network protocols,Executing device-driver code and scheduling processes (block/unblock),Managing file system access and caching frequently used data,Initializing hardware components and monitoring system temperature,C,"The text states that heavy I/O demands on the CPU include 'execute device-driver code, schedule processes (block/unblock).'"
What is the primary effect of context switches on system resources?,They reduce memory consumption by flushing caches.,They stress the CPU and hardware caches.,They eliminate the need for interrupt handling.,They always improve system throughput.,They increase network latency.,B,The text indicates: 'Context switches: stress CPU and hardware caches.'
How do data copies between controllers/physical memory and kernel buffers/application space impact the system?,They reduce the overall system latency.,They offload work from the main CPU.,They improve the efficiency of interrupt handling.,They load the memory bus.,They directly increase CPU clock speed.,D,"The text states: 'Loads memory bus: data copies between controllers/physical memory, and kernel buffers/application space.'"
Which statement accurately describes the cost of interrupt handling?,It is generally inexpensive and simple.,It involves only executing the handler code.,"It is relatively expensive due to state changes, handler execution, and state restoration.",It is more efficient than Programmed I/O (PIO) in all scenarios.,It primarily stresses the network interface card.,C,"The text explicitly mentions: 'Interrupt handling: relatively expensive (state change, execute handler, restore state).'"
Under what condition can Programmed I/O (PIO) be more efficient than interrupt-driven I/O?,When the system is experiencing low I/O demands.,If busy waiting is completely eliminated.,When the amount of data transferred is very large.,If busy waiting is minimized.,When the CPU is dedicated solely to I/O tasks.,D,The text states: 'Programmed I/O (PIO) can be more efficient than interrupt-driven I/O if busy waiting minimized.'
What is a direct consequence of an I/O completion unblocking a process?,Reduced CPU utilization.,Elimination of context switches.,An increase in memory bus bandwidth.,A full context switch overhead.,Improved cache hit rates.,D,The text notes: 'I/O completion unblocks process: leads to full context switch overhead.'
Which of the following activities is known to cause a high context-switch rate?,Intensive CPU computation,Large file transfers via local disk,Network traffic,Memory defragmentation,System boot-up sequence,C,The text states: 'Network traffic: high context-switch rate.'
"In the remote login character example, which component is responsible for receiving the typed character from the keyboard and generating an interrupt?",The user process,The kernel's network layers,The keyboard itself (or its controller),The network device driver,The interrupt handler,C,"The sequence starts with 'character typed -> keyboard interrupt', implying the keyboard hardware/controller generates the initial interrupt."
"During a remote login session, what is a significant overhead observed throughout the process of sending and receiving a character?",Excessive disk I/O operations,Continuous memory page faults,Frequent context switches and state switches,CPU idle time due to waiting for network packets,Graphics rendering delays,C,The text concludes the remote login example with: 'Throughout: context switches and state switches.'
What is the purpose of using separate front-end processors for terminal I/O in some systems?,To increase the main CPU's interrupt burden.,To solely manage network connections.,To reduce the main CPU's interrupt burden.,To perform only mathematical computations.,To act as a secondary storage device.,C,The text states: 'Some systems use separate front-end processors for terminal I/O to reduce main CPU interrupt burden.'
What is a 'terminal concentrator'?,A device that consolidates CPU processing from multiple servers.,A type of front-end processor that multiplexes traffic from hundreds of remote terminals into one port.,A specialized memory unit for high-speed I/O.,A software component that manages display output for terminals.,A network switch for local area networks.,B,The text and glossary define a 'terminal concentrator' as a 'Type of front-end processor for terminals' that 'multiplexes traffic from hundreds of remote terminals into one port.'
What is the primary role of an I/O channel in mainframes and high-end systems?,To handle user interface graphics.,To offload I/O work from the main CPU and ensure smooth data flow.,To manage inter-process communication within the kernel.,To perform general-purpose computing tasks.,To provide power supply redundancy.,B,"The text describes the 'Channel job' as: 'offload I/O work from main CPU, keep data flowing smoothly.' The glossary defines an 'I/O channel' as 'Dedicated, special-purpose CPU in large systems for I/O or offloading main CPU.'"
Which of the following is NOT listed as a principle to improve I/O efficiency?,Reduce number of context switches.,Increase data copies in memory.,Reduce interrupt frequency.,Increase concurrency using DMA-knowledgeable controllers.,"Balance CPU, memory subsystem, bus, I/O performance.",B,"One of the principles listed is to 'Reduce data copies in memory (between device/application)', making 'Increase data copies in memory' the incorrect option."
How can interrupt frequency be reduced to improve I/O efficiency?,By using smaller data transfers.,By decreasing the intelligence of controllers.,By always using interrupt-driven I/O.,"By using large transfers, smart controllers, or polling (if busy waiting minimal).",By increasing the number of context switches.,D,"The text lists: 'Reduce interrupt frequency: use large transfers, smart controllers, polling (if busy waiting minimal).'"
"What is the benefit of increasing concurrency in I/O operations, particularly with DMA-knowledgeable controllers/channels?",It increases the number of CPU interrupts.,It allows the CPU to directly handle all data copying.,It offloads data copying from the CPU.,It reduces the need for specialized hardware.,It simplifies the device driver design.,C,The text states: 'Increase concurrency: use DMA-knowledgeable controllers/channels to offload data copying from CPU.'
"Why is balancing the performance of CPU, memory subsystem, bus, and I/O crucial for system efficiency?",To ensure that one area's overload does not cause idleness in others.,To minimize the total power consumption.,To allow for dynamic clock frequency adjustments.,To simplify the operating system's kernel.,To reduce the physical size of the components.,A,"The text advises to 'Balance CPU, memory subsystem, bus, I/O performance: overload in one area causes idleness in others.'"
Which of the following best describes the complexity of I/O devices?,All I/O devices have uniform complexity.,"I/O device complexity varies greatly, from simple (mouse) to highly complex (Windows disk driver).",Complexity only relates to the physical size of the device.,Complexity is inversely proportional to device speed.,Complexity is determined solely by the amount of data transferred.,B,"The text states: 'I/O device complexity varies (mouse simple, Windows disk driver complex).'"
A Windows disk driver typically performs all of the following functions EXCEPT:,Managing individual disks.,Implementing RAID arrays.,Converting requests to disk I/O.,Directly rendering graphical output to the display.,Error handling and data recovery.,D,"The text lists functions of a Windows disk driver: 'manages individual disks, implements RAID arrays, converts requests to disk I/O, error handling, data recovery, optimizes performance.' Display rendering is not mentioned as its function."
Which of the following represents the typical progression observed when implementing I/O functionality?,Kernel -> Application -> Hardware,Application -> Hardware -> Kernel,Hardware -> Kernel -> Application,Application -> Kernel -> Hardware,Kernel -> Hardware -> Application,D,The text describes the progression as: 'Initially: experimental I/O algorithms at application level. -> When proven: reimplement in kernel. -> Highest performance: specialized implementation in hardware (device or controller).'
What is a disadvantage of implementing experimental I/O algorithms at the application level?,Bugs are likely to crash the entire system.,It requires frequent reboots and driver reloads after code changes.,It is generally less flexible compared to kernel implementation.,It is inefficient due to context switch overhead and lack of kernel access.,It provides the highest possible performance.,D,"The text lists disadvantages of application level implementation: 'Inefficient: context switch overhead, no internal kernel data/functionality (messaging, threading, locking).'"
What is a primary benefit of reimplementing proven I/O algorithms from the application level into the kernel?,It makes development easier and faster.,It improves system performance.,It eliminates the need for debugging.,It reduces the complexity of the kernel.,It allows user processes to directly access hardware.,B,The text states that reimplementation in kernel 'Improves performance.'
"What is a major drawback of implementing I/O functionality at the specialized hardware level (e.g., a device or controller)?",It offers lower performance than software implementations.,It significantly decreases development time.,It is flexible and allows kernel influence over I/O order.,"Difficulty and expense of improvements/bug fixes, and decreased flexibility.",It increases context switch overhead.,D,"The text lists disadvantages of hardware implementation: 'Disadvantages: difficulty/expense of improvements/bug fixes. ... Decreased flexibility (e.g., hardware RAID controller may not allow kernel to influence I/O order/location).'"
"Which of the following I/O technologies is nearing DRAM speeds, increasing pressure on I/O subsystems?",Hard Disk Drives (HDD),Solid-State Drives (SSD),Non-Volatile Memory (NVM) devices,Peripheral Component Interconnect Express (PCIe),Small Computer System Interface (SCSI),C,The text states: 'I/O devices increasing in speed (NVM devices nearing DRAM speed).'
Front-end processors are best described as:,The main CPU of a system.,Small computers that manage I/O and offload the main CPU.,Graphics processing units (GPUs).,Memory modules used for caching.,Network interface cards (NICs).,B,"The glossary defines 'front-end processors' as 'Small computers performing tasks in overall system; manage I/O, offload CPU.'"
Which of the following correctly identifies the basic hardware elements of an I/O system?,"CPUs, RAM, and hard drives","Buses, device controllers, and devices","Operating systems, applications, and networks","Keyboards, mice, and monitors","File systems, directories, and files",B,"The text explicitly states that 'Basic I/O hardware elements' are 'buses, device controllers, devices'."
Data movement for I/O operations can be managed by which two primary mechanisms?,User applications or network protocols,System calls or library functions,CPU (programmed I/O) or DMA controller,Interrupt handlers or signal processors,Caching mechanisms or buffering techniques,C,The text indicates 'Data movement: CPU (programmed I/O) or DMA controller'.
What is a device driver primarily defined as?,A user-level application for hardware diagnostics,A kernel module responsible for controlling a specific device,A hardware component that manages I/O operations,A network protocol used for device communication,A file system component for storing device configurations,B,The text defines 'Device driver' as a 'kernel module controlling a device'.
Which of the following is NOT listed as a basic hardware category handled by the system-call interface?,Block devices,Network sockets,Character-stream devices,Graphics processing units (GPUs),Programmed interval timers,D,"The text lists 'block devices, character-stream devices, memory-mapped files, network sockets, programmed interval timers' as categories handled by the system-call interface. GPUs are not mentioned."
What is the typical behavior of system calls in relation to the process that invokes them?,They always execute in parallel with the process.,They usually block the process until completion.,They immediately return control to the process without waiting.,They always trigger an error if the process tries to sleep.,They are exclusively handled by user-level libraries.,B,The text mentions 'System calls usually block processes'.
Under what circumstances are nonblocking or asynchronous system calls typically used?,To ensure maximum process sleep time,For applications that require user interaction to pause,When kernel/applications must not sleep,To simplify the process of context switching,Only for non-critical background tasks,C,The text states 'nonblocking/asynchronous calls used by kernel/applications that must not sleep'.
Which of the following is a service provided by the kernel's I/O subsystem?,User interface design,I/O scheduling,Database indexing,Web server management,Application compilation,B,The text lists 'I/O scheduling' as one of the services provided by the kernel's I/O subsystem.
The kernel's I/O subsystem provides a comprehensive set of services. Which of the following lists accurately represents some of these services?,"Process creation, memory allocation, and CPU scheduling","File encryption, network routing, and graphic rendering","I/O scheduling, buffering, and error handling","User authentication, printer configuration, and sound mixing","Device driver development, application debugging, and system backup",C,"The text explicitly lists 'I/O scheduling, buffering, caching, spooling, device reservation, error handling' as services provided by the kernel's I/O subsystem. Option C correctly includes three of these."
What is the primary purpose of 'Name translation' in the context of I/O?,To convert binary data into human-readable text.,To connect hardware devices to symbolic file names.,To translate network addresses into IP addresses.,To encrypt and decrypt data for security purposes.,To optimize the performance of CPU caches.,B,The text states 'Name translation: connects hardware devices to symbolic file names'.
Name translation for I/O devices involves multiple mapping levels. Which sequence best describes these levels?,Physical addresses → character-string names → device drivers/addresses,Device drivers/addresses → physical addresses → character-string names,Character-string names → physical addresses → device drivers/addresses,Character-string names → device drivers/addresses → physical addresses,Physical addresses → device drivers/addresses → character-string names,D,The text specifies the mapping levels as 'character-string names → device drivers/addresses → physical addresses (I/O ports/bus controllers)'.
"Where can device name mapping occur, according to the provided text?",Only within the file-system name space (UNIX),Only within a separate device name space (MS-DOS),Within the file-system name space (UNIX) or a separate device name space (MS-DOS),Exclusively in the kernel's memory space,"Only at the hardware level, independent of the operating system",C,The text states 'Mapping can be within file-system name space (UNIX) or separate device name space (MS-DOS)'.
What is 'STREAMS' in the context of UNIX systems?,A method for inter-process communication via shared memory,A mechanism for dynamic assembly of driver code pipelines,A specific type of network file system protocol,A utility for managing background processes,A command-line interpreter for system administration,B,The text defines 'STREAMS' as a 'UNIX mechanism for dynamic assembly of driver code pipelines'.
"When drivers are stacked using mechanisms like STREAMS, how does data typically flow?","Only in one direction, from top to bottom",Randomly jumping between stacked drivers,Sequentially and bidirectionally,In parallel streams that merge at the end,Only during the initialization phase,C,"The text notes that when drivers are stacked, 'data passes sequentially and bidirectionally'."
"Which of the following is identified as a primary cost associated with I/O system calls, specifically due to crossing the kernel protection boundary?",Increased network latency,Context switching,Disk defragmentation,Memory leak detection,User interface rendering,B,The text lists 'Context switching (kernel protection boundary)' as a cost of I/O system calls.
A significant CPU/memory load cost associated with I/O system calls comes from what operation?,Frequent CPU clock frequency adjustments,Dynamic memory allocation for user applications,Data copying between kernel buffers and application space,Execution of graphics rendering pipelines,Pre-fetching instructions for future execution,C,The text explicitly states 'CPU/memory load for data copying (kernel buffers ↔ application space)' as a cost.
"Besides context switching and data copying, what other factor contributes to the high cost of I/O system calls?",The need for physical device maintenance,User-level library function calls,Signal/interrupt handling,Compiler optimization levels,The size of the system's hard drive,C,The text lists 'Signal/interrupt handling' as another factor contributing to the cost of I/O system calls.
What is the primary function of a file system?,To manage CPU scheduling and process execution.,To describe how files map to physical devices and how they are accessed/manipulated.,To provide memory management for virtual memory.,To handle network communications between devices.,To perform cryptographic operations for data security.,B,"The text describes the file system as: 'describes how files map to physical devices, how accessed/manipulated.'"
What are the two main components of a file system?,The CPU and the GPU.,A collection of files and a directory structure.,The kernel and the shell.,Input devices and output devices.,RAM and secondary storage.,B,"The text states: 'Consists of: Collection of files (storing related data). Directory structure (organizes, provides info about files).'"
"From a user's perspective, what is the 'file' considered to be?",The largest allotment of physical primary storage.,The smallest allotment of logical secondary storage where data is written.,A transient memory buffer.,A type of network packet.,An instruction set for the CPU.,B,The text indicates: 'User perspective: smallest allotment of logical secondary storage (data written only within a file).'
Which of the following best describes a 'text file'?,A series of code sections a loader can bring into memory and execute.,A sequence of functions including declarations and executable statements.,A sequence of characters organized into lines.,A collection of raw binary data without any internal structure.,A file whose meaning is solely defined by the operating system.,C,The text defines: 'Text file: sequence of characters organized into lines.'
What is the primary characteristic of an 'executable file'?,It is a sequence of characters intended for human reading.,It contains program source code written in a high-level language.,It is a series of code sections that a loader can bring into memory and execute.,It stores user-defined numerical or alphabetical data.,It is a temporary file used for system logs.,C,The text defines: 'Executable file: series of code sections loader can bring into memory and execute.'
"Which file attribute provides a non-human-readable, unique tag to identify a file within the file system?",Name,Type,Location,Identifier,Protection,D,"The text lists 'Identifier: unique tag (number), non-human-readable, identifies file within file system' as a file attribute."
What is the purpose of the 'Protection' file attribute?,To specify the physical location of the file on the device.,To store the current size of the file in bytes or blocks.,"To provide access-control information, such as read, write, or execute permissions.",To indicate the character encoding and file checksum.,"To record the creation, last modification, and last use times.",C,"The text states 'Protection: access-control info (read, write, execute)' as a file attribute."
What are 'extended file attributes' in newer file systems?,"The basic attributes like name, size, and protection.","GUI views of file metadata, like a 'file info window'.",Additional metadata such as character encoding and file checksum.,Pointers to the device and location of the file.,The timestamps for creation and last modification.,C,"The text states: 'Newer file systems support extended file attributes: character encoding, file checksum.'"
Which of the following is NOT one of the minimal set of file operations provided by an operating system?,Create,Write,Reposition,Defragment,Delete,D,"The text lists 'create, write, read, reposition, delete, truncate files' as the minimal set. Defragment is not listed."
What are the two steps involved in 'creating a file'?,Searching for the file in the directory and reading its contents.,Finding space in the file system and making an entry for the new file in the directory.,Setting the read pointer and the write pointer.,Acquiring a shared lock and then an exclusive lock.,Changing the file's attributes and then renaming it.,B,The text states: 'Creating a file: 1. Find space in file system. 2. Make entry for new file in directory.'
What is the primary purpose of the `open()` system call before performing operations like read or write on a file?,To physically transfer the entire file into RAM.,To secure the file with an exclusive lock preventing all other access.,To avoid constant directory searching by allowing the OS to keep file info in an open-file table.,To permanently delete the file from secondary storage.,To change the file's internal structure to match the application's needs.,C,The text explains: 'To avoid constant directory searching: `open()` system call before first use. OS keeps open-file table: info about all open files.'
"In file operations, what is the 'current-file-position pointer' used for?",To indicate the physical block address on the disk.,To specify the end-of-file marker.,To keep track of the next read or write location within the file for a specific process.,To point to the file's entry in the system-wide open-file table.,To store the access mode information for the file.,C,The text states: 'Current operation location: per-process current-file-position pointer (shared by read/write).'
The operation of changing the 'current-file-position pointer' to a given value without involving actual I/O is known as:,Truncate,Create,Seek,Delete,Write,C,The text states: 'Also known as file seek.'
When are the actual contents of a file with 'hard links' deleted?,Immediately after the first link is deleted.,Only when the last hard link to the file is deleted.,When the file is truncated.,"Upon system reboot, if any link exists.",When a shared lock is acquired on the file.,B,The text explains for hard links: 'actual content deleted only when last link deleted.'
What does 'truncating a file' achieve?,It deletes the file's entry from the directory structure.,It renames the file to a shorter name.,"It erases the contents of the file but keeps its attributes, resetting its length to zero.",It copies the file to a different location on the disk.,It compresses the file to save disk space.,C,"The text states: 'Truncating a file: Erase contents but keep attributes. Reset file length to zero, release file space.'"
What information does the system-wide open-file table primarily store?,The current file pointer and access rights for a specific process.,"Process-independent information like disk location, access dates, and file size.",The user ID of the process that opened the file.,A list of all recently closed files.,The shell script commands for file operations.,B,"The text distinguishes: 'System-wide open-file table: process-independent info (disk location, access dates, size).'"
What is the purpose of the 'open count' in the open-file table?,To track the total number of operations performed on the file.,To limit the maximum number of times a file can be opened.,To record how many processes currently have the file open.,To count the number of bytes read from or written to the file.,To denote the version number of the file.,C,The text states: 'Open-file table has open count: number of processes with file open.'
"Which type of file lock allows multiple processes to acquire it concurrently, similar to a reader lock?",Exclusive lock,Mandatory lock,Advisory lock,Shared lock,System lock,D,The text defines: 'Shared lock: multiple processes acquire concurrently (like reader lock).'
"In which file-locking mechanism does the operating system prevent other processes from accessing a locked file, ensuring locking integrity?",Advisory file-lock mechanism,Shared lock mechanism,Exclusive lock mechanism,Mandatory file-lock mechanism,Implicit lock mechanism,D,"The text states: 'Mandatory: OS prevents other processes from accessing locked file (e.g., Windows).'"
What is a 'shell script'?,A binary executable program.,A file containing a set series of commands specific to a shell.,A type of source code file written in C++.,A temporary file used for system logging.,A file containing encrypted data.,B,The text defines: '.sh is a shell script (ASCII commands).'
How does macOS indicate file type and associate files with their creator programs?,By relying solely on file extensions as hints.,Through a magic number at the beginning of the file.,By using a creator attribute that the OS sets during `create()` call and enforces.,By requiring explicit user input for type association.,By embedding the program executable within the file itself.,C,"The text states: 'macOS: each file has type (.app), creator attribute (program that created it). Creator attribute set by OS during `create()` call, enforced.'"
What is a 'magic number' in the context of UNIX file systems?,A random number generated when a file is created.,A unique identifier assigned by the OS to each file.,A number at the start of some binary files that indicates the data type.,The total number of blocks allocated to a file.,A code used for file protection and encryption.,C,"The text states: 'UNIX: magic number at beginning of some binary files indicates data type (e.g., image format).'"
What is a disadvantage of an operating system supporting many different internal file structures?,"It leads to smaller, more efficient operating systems.",It simplifies the development of new applications.,"It makes the OS large and cumbersome, requiring specific code for each supported structure.",It enhances file portability across different OS platforms.,It reduces internal fragmentation significantly.,C,"The text mentions: 'Disadvantage of OS supporting multiple file structures: large, cumbersome OS. OS needs code for each supported structure.'"
What is the common minimal file structure imposed by operating systems like UNIX and Windows?,"Files are sequences of lines, with strict line endings.",Files must conform to a rigid record-based structure.,"Each file is a sequence of 8-bit bytes, with no OS interpretation of its internal structure.",Files are always organized into fixed-size blocks interpreted by the OS.,All files must contain a magic number at their beginning.,C,The text states: 'UNIX: each file is sequence of 8-bit bytes; no OS interpretation. Provides maximum flexibility but little support...'
"In disk systems, what is the basic unit of all disk I/O operations?",A single byte.,A logical record.,One block (physical record) of a well-defined size.,The entire file at once.,A stream of characters.,C,The text states: 'All disk I/O in units of one block (physical record); all blocks same size.'
What is 'internal fragmentation' in the context of file storage?,Wasted space within a logical record due to improper formatting.,The scattering of file blocks across different disk locations.,The inability of an OS to recognize a file's type.,Wasted disk space in the last allocated block of a file because the file size is not an exact multiple of the block size.,The loss of data due to a disk error.,D,The text defines: 'internal fragmentation: Wasted disk space in last block of file due to block allocation.'
How does increasing the disk block size generally affect internal fragmentation?,It decreases internal fragmentation.,It has no effect on internal fragmentation.,It increases internal fragmentation.,It eliminates internal fragmentation completely.,It converts internal fragmentation into external fragmentation.,C,The text states: 'All file systems suffer internal fragmentation; larger block size -> greater fragmentation.'
What is considered a major design problem related to file access methods?,Ensuring data consistency across distributed systems.,Choosing the appropriate access method for a specific application.,Optimizing memory allocation for file buffers.,Implementing robust error handling for file operations.,Managing concurrent read/write operations efficiently.,B,The text states that 'Choosing right method: major design problem.'
Which of the following best characterizes the 'sequential access' method?,Records are read and written using a block number parameter.,"Information is processed in a random, non-ordered fashion.","Information is processed strictly in order, one record after another.",It primarily relies on constructing an index for rapid lookups.,It requires the entire file to be loaded into memory before access.,C,"Sequential access processes 'Information processed in order, one record after another.' This is also confirmed in the glossary."
The sequential access method is conceptually based on which model of a file?,Disk model,Tape model,Memory model,Network stream model,Database table model,B,The text explicitly states that sequential access is 'Based on tape model of file.'
Which set of operations is typical for a sequential access file?,"read(n), write(n)","position_file(n), seek_block(m)","read_next(), write_next()","create_index(), search_index()","allocate_block(), deallocate_block()",C,Sequential access uses `read_next()` to read the next portion and `write_next()` to append and advance.
What is a primary characteristic of the 'direct access' method?,It processes information strictly from beginning to end.,It is most commonly used by text editors and compilers.,It allows programs to read and write records rapidly in no particular order.,It always requires a multi-level index structure.,It is inherently inefficient on random-access devices.,C,Direct access allows programs to 'read/write records rapidly in no particular order.' This is also confirmed in the glossary.
The direct access method is conceptually based on which model of a file?,Tape model,Network stream model,Disk model,Mainframe queue model,Optical disc model,C,Direct access is 'Based on disk model of file (disks allow random access).'
"In the context of direct access, what is meant by 'logical records'?",Records whose size can vary widely within the same file.,File contents logically designated as fixed-length structured data.,Temporary records used only for file system internal operations.,Records that are always stored contiguously on disk.,"Records that must be processed in a specific, predefined order.",B,"The glossary defines 'logical records' as 'File contents logically designated as fixed-length structured data,' and the direct access section mentions 'File: fixed-length logical records.'"
Which term is used as an alternative name for 'direct access'?,Sequential access,Indexed access,Relative access,Stream access,Pointer access,C,The text and glossary state that 'direct access' is also known as 'relative access'.
"When performing direct access operations, what does a 'relative block number' specify?",The physical sector address on the disk where the block is stored.,"The total number of blocks in the file, starting from 1.","An index relative to the beginning of the file, with the first block often being 0.",A unique identifier for the file within the entire file system.,"The block's size in bytes, relative to a standard block size.",C,The glossary defines 'relative block number' as 'Index relative to beginning of file (first is block 0).'
What is the 'allocation problem' in the context of file access?,The challenge of assigning unique file IDs to new files.,The operating system's determination of where to store file blocks.,The difficulty in allocating enough memory for file caching.,The process of allocating user permissions to access files.,The issue of allocating network bandwidth for file transfers.,B,The glossary defines 'allocation problem' as 'OS determination of where to store file blocks.'
Why does the use of 'relative block numbers' in direct access help prevent users from accessing non-file portions of the file system?,Because relative block numbers are encrypted and cannot be deciphered by users.,"Because the OS transparently translates them to physical addresses, controlling access.",Because users are only allowed to specify absolute physical disk addresses.,"Because relative block numbers are only used for read operations, not write.",Because they enforce a strict sequential access pattern.,B,The text states that the OS deciding file placement ('allocation problem') using relative block numbers 'Prevents user from accessing non-file portions of file system.'
Simulating direct access on a sequential-access file system is characterized as:,Highly efficient and straightforward.,A common practice for performance optimization.,Extremely inefficient and clumsy.,Impossible due to fundamental differences in access patterns.,Only possible if the sequential file is sorted.,C,"The text explicitly states, 'Simulating direct-access on sequential-access: extremely inefficient and clumsy.'"
"What is the foundation upon which 'Other Access Methods', such as indexed access, are typically built?",Tape-based sequential access.,Network stream access.,Direct access method.,In-memory file system caching.,User-defined custom algorithms.,C,The text states that 'Other Access Methods' are 'Built on top of direct-access method.'
What is the primary purpose of an 'index' in file access methods?,To compress the file data to save storage space.,"To store metadata about the file, such as creation date and owner.","To provide pointers to various blocks within a file, enabling direct access.",To enforce security permissions for different users.,To log all read and write operations performed on the file.,C,"The text states, 'Index: contains pointers to various blocks (like book index),' and the glossary defines it as containing 'pointers to contents.'"
"For very large files, when the main index itself becomes too large to fit in memory, what is the suggested solution?",To switch to a purely sequential access method.,"To divide the file into multiple smaller, independent files.",To build an index for the index file (multi-level indexing).,"To store the index in a high-speed, persistent memory device.",To perform a full file scan for every access request.,C,The text states the solution is an 'index for the index file (primary index -> secondary index -> data).'
"In a multi-level indexing system like IBM ISAM, what is the maximum number of direct-access reads typically required to locate any record?",One,Two,Three,Four,Depends on the number of indexing levels,B,"The text states, 'Any record located by at most two direct-access reads.'"
Which of the following is cited as a common application type that benefits from 'direct access' due to its need for immediate access to large information amounts?,Text editors,Compilers,Streaming video players,Airline reservation systems (databases),Backup and archiving utilities,D,"The text states, 'Great use for immediate access to large info amounts (e.g., databases). Example: airline reservation system.'"
Which statement is true regarding the support for sequential and direct access by operating systems?,All operating systems fully support both sequential and direct access.,No operating systems support both sequential and direct access simultaneously.,"Some operating systems do not support both, and some require file definition at creation.",Direct access is always preferred over sequential access in modern OS.,"Sequential access can only be used on tape devices, and direct access only on disks.",C,"The text states, 'Not all OS support both sequential and direct access. Some require file defined as sequential/direct at creation.'"
How is a request for record 'N' in a direct access file with fixed-length logical records typically satisfied?,By reading records sequentially until the Nth record is reached.,By converting it into an I/O request for 'N' bytes starting at the beginning of the file.,By converting it into an I/O request for 'N' bytes starting at 'N * (logical record length)'.,By searching an in-memory table for the physical address of record 'N'.,By creating a temporary index for the file to locate record 'N'.,C,"The text states, 'Satisfying request for record N: turned into I/O request for N bytes starting at N * (logical record length).'"
What is one way systems can provide alternative operations for direct access while retaining sequential-like functions?,By removing `read_next()` and `write_next()` entirely.,By only supporting `read(n)` and `write(n)`.,By adding `position_file(n)` while retaining `read_next()` and `write_next()`.,By always requiring an index to be built first.,By converting direct access requests into sequential reads internally.,C,"The text states, 'Alternative: retain `read_next()`, `write_next()`, add `position_file(n)`.'"
What is the primary function of a directory in a file system?,To store the actual data content of files.,To manage the physical disk blocks allocated to files.,To act as a symbol table translating file names to file control blocks.,To execute user commands and system programs.,To serve as a temporary storage area for recently accessed files.,C,"A directory functions as a symbol table, mapping human-readable file names to their corresponding file control blocks (FCBs) which contain detailed information about the files."
Which of the following is NOT a fundamental capability that a directory organization must allow?,Inserting new entries.,Deleting existing entries.,Searching for a named entry.,Compressing directory size automatically.,Listing all entries within the directory.,D,"Fundamental capabilities for a directory organization include inserting, deleting, searching for, and listing entries. Automatic compression is not listed as a required basic capability."
"When deleting a file from a directory, what potential issue might arise that requires defragmentation?",The file's data blocks might become corrupted.,The directory entry might become read-only.,It may leave a 'hole' in the directory structure.,The file's timestamp might be reset incorrectly.,It automatically creates a backup of the deleted file.,C,"Deleting a file can leave a 'hole' in the directory structure, which may necessitate defragmentation to optimize space utilization and performance."
What is the primary purpose of 'Traversing the file system' as a directory operation?,To create new files and directories.,To change the permissions of files.,To find and display the contents of a single specific file.,"To access every directory and file, often for backup or space release.",To rename files based on their content.,D,"Traversing the file system involves accessing all directories and files, a process typically performed for tasks like creating backups or identifying space that can be released."
What is the main limitation of a single-level directory structure when multiple users are present?,It is difficult to support and understand.,"Files must have unique names across all users, leading to name collisions.",It requires complex permissions management for file sharing.,It offers poor performance for file search operations.,Only a single type of file can be stored in the directory.,B,"In a single-level directory, all files reside in the same directory, meaning every file must have a unique name, which quickly leads to name collision problems when multiple users are involved."
"In a two-level directory system, what is a User File Directory (UFD)?",A system directory that stores critical OS files.,A directory that is shared among all users for common files.,A per-user directory that contains only that user's files.,A temporary directory for caching frequently accessed files.,The root directory of the entire file system.,C,"The User File Directory (UFD) is a separate directory created for each user in a two-level directory system, containing only their specific files."
What is the role of the Main File Directory (MFD) in a two-level directory structure?,It stores the actual data blocks of all user files.,"It is an index, typically by user name or account, that points to each user's UFD.",It contains a list of all executable system programs.,It serves as a cache for recently accessed directory entries.,It manages the network connections for file sharing.,B,"The Main File Directory (MFD) acts as a top-level index, usually organized by user name or account, with each entry pointing to the respective User File Directory (UFD)."
A significant advantage of a two-level directory structure over a single-level directory is that:,It allows for unlimited nesting of subdirectories.,It enables different users to have files with the same name.,It automatically performs garbage collection for deleted files.,It eliminates the need for any form of path names.,It inherently supports shared files without links.,B,"Because each user has their own UFD, different users can have files with identical names (e.g., 'report.txt') without causing name collisions, as long as the names are unique within their respective UFDs."
"In a two-level directory system, how is a file typically referred to if it belongs to another user?",By its unique file ID only.,By simply changing the current directory to the other user's UFD.,By specifying both the user name and the file name.,By directly accessing the Main File Directory (MFD).,It is not possible to access another user's file.,C,"To access another user's file in a two-level directory system, the file must be specified using both the user's name and the file's name, forming a path name like `/userb/test.txt`."
"What is a 'search path' in the context of directory systems, especially useful for system files in a two-level structure?",The route taken by a user to manually locate a file.,A sequence of directories that the operating system searches to locate an executable file.,The unique identifier assigned to each file for fast retrieval.,A graphical representation of the file system hierarchy.,A list of recently accessed files for quick reopening.,B,"A 'search path' defines the sequence of directories that the operating system traverses when trying to locate a file, particularly executable files, often starting with the local user directory and then proceeding to shared system directories."
Which directory structure is considered a generalization of the two-level directory and is the most common in modern operating systems?,Single-level directory.,Flat directory.,Tree-structured directory.,Acyclic-graph directory.,General graph directory.,C,"Tree-structured directories extend the concept of the two-level directory by allowing directories to be nested to an arbitrary height, forming a hierarchical tree. This is the most common directory structure."
"In a tree-structured directory system, how is a directory (or subdirectory) often identified or treated?",It's treated as a special type of executable program.,"It's considered a special file, often with a bit defining it as a directory versus a regular file.",It's a fixed-size partition of the disk.,It's a temporary structure that gets deleted after each session.,It's a physical hardware component.,B,"In tree-structured directories, a directory is frequently treated as a special type of file, where one bit in its entry indicates whether it's a regular file (0) or a subdirectory (1)."
What is an 'absolute path name' in a tree-structured directory system?,A path that defines the location of a file relative to the current directory.,A path that identifies a file using only its unique file ID.,A path that begins at the root directory and specifies the full hierarchy to the file.,A path used exclusively for network file access.,A path that automatically adjusts if the file is moved.,C,"An absolute path name starts from the file system's root directory (e.g., '/') and lists all directories in the hierarchy down to the target file, providing a unique and unambiguous location."
"If your current directory is `/home/user/documents` in a tree-structured file system, what would the relative path `reports/monthly.pdf` refer to?",`reports/monthly.pdf` in the root directory.,`/home/user/documents/reports/monthly.pdf`.,`/reports/monthly.pdf`.,The `monthly.pdf` file in the user's home directory.,The `reports` directory at the same level as `/home`.,B,"A relative path name defines the path from the current directory. So, `reports/monthly.pdf` relative to `/home/user/documents` translates to `/home/user/documents/reports/monthly.pdf`."
Which statement accurately describes a common policy for deleting non-empty directories in tree-structured file systems?,All systems automatically delete a directory and its contents recursively.,Most systems require manual confirmation for each file before deletion.,"Some systems only allow deletion if the directory is empty, others allow recursive deletion (e.g., UNIX `rm -r`).","Non-empty directories can only be renamed, not deleted.",Directory deletion is strictly prohibited to prevent data loss.,C,"Deletion policies vary: some systems require a directory to be empty before it can be deleted, forcing the user to delete contents manually, while others (like UNIX `rm -r`) offer a convenient but potentially dangerous recursive deletion."
What is the defining characteristic that acyclic-graph directories introduce compared to tree-structured directories?,They enforce unique file names globally.,They allow directories to share subdirectories and files.,They are limited to two levels of hierarchy.,They strictly prevent any form of file sharing.,They only store temporary files.,B,"Acyclic-graph directories are a more flexible structure that allows multiple directories to point to and share the same subdirectory or file, unlike tree structures where each file/directory has only one parent."
"When a file is shared using an acyclic-graph directory structure, what is true about modifications to its content?",Changes are only visible through the original directory entry.,A separate copy of the file is created for each shared instance.,"Changes made via any shared link are visible to all other links, as there is only one actual file.",The file becomes read-only to prevent inconsistencies.,Only the administrator can modify shared files.,C,"In an acyclic-graph directory, when a file is shared, there is only one physical file on disk. Any modifications made through any of its linked entries will be immediately visible through all other entries pointing to that same file."
"In the context of acyclic-graph directories, what is a 'link'?",A physical connection between two hard drives.,A pointer within a directory entry to another file or subdirectory.,A cryptographic key used for file encryption.,A system command used to create new directories.,A backup copy of a file.,B,"A 'link' in acyclic-graph directories is a pointer (which can be an absolute or relative path name) within one directory entry that refers to another file or subdirectory, enabling sharing."
What does the operation 'resolve' mean in the context of file system links?,To fix errors in a file's content.,To delete a broken link from the directory.,To follow the path name specified in a link to locate the actual target file.,To change the permissions of a linked file.,To convert a relative path to an absolute path.,C,To 'resolve' a link means to use the path name stored within the link itself to find and access the actual file or directory that the link points to.
What is 'aliasing' a problem in acyclic-graph directories?,It prevents files from being deleted.,It refers to the situation where the same file can be accessed via multiple distinct absolute path names.,It causes system slowdowns due to excessive memory usage.,It changes the file's content without user permission.,It restricts files to be accessed by only one user at a time.,B,"Aliasing is a problem in acyclic-graph directories because a single file or subdirectory can be pointed to by multiple links, resulting in it having several different absolute path names, which can be confusing for users."
"What problem can arise if a file linked in an acyclic-graph directory is deleted without proper management (e.g., if it's a symbolic link and the original is deleted)?",The directory structure automatically converts to a tree.,The remaining links to the file become 'dangling pointers'.,The entire file system crashes due to inconsistency.,The file is automatically restored from a backup.,All other users are immediately logged out.,B,"If a file that has symbolic links pointing to it is deleted, the symbolic links themselves remain but now point to a non-existent file, becoming 'dangling pointers'."
How do systems like UNIX manage space deallocation for 'hard links' in acyclic-graph directories to prevent dangling pointers?,By automatically creating a backup copy when a link is deleted.,"By using a 'reference count' to track the number of links to a file, deleting the file only when the count reaches zero.",By converting all hard links to symbolic links upon deletion.,By requiring manual confirmation from all users before deleting any linked file.,By storing copies of the file in every directory it's linked to.,B,"Hard links, often used in UNIX-like systems, employ a 'reference count' (or link count). This count is incremented when a new link is created and decremented when a link is deleted. The actual file's space is only deallocated when its reference count drops to zero, ensuring no dangling pointers."
What is the defining characteristic of a 'general graph directory' that differentiates it from an 'acyclic-graph directory'?,It uses only absolute path names.,It does not allow any files to be shared.,It allows cycles (loops) in the directory structure.,It is limited to a single level of directories.,It requires all files to have unique names globally.,C,"The key distinction of a general graph directory is that it permits cycles or loops within the directory structure, meaning a directory can contain a link that eventually leads back to an ancestor directory or itself."
What is a major problem introduced by cycles in a general graph directory structure during file system traversal or search operations?,Inability to find any files.,Automatic file corruption.,Risk of infinite loops during traversal.,Reduced storage capacity of the disk.,Increased network latency.,C,"The presence of cycles in a general graph directory can lead to infinite loops if traversal algorithms don't specifically account for and bypass them, potentially causing the system to hang or crash during operations like search or backup."
Why is space deallocation (like determining when a file can be truly deleted) more complex in general graph directories than in acyclic-graph directories?,All files are automatically backed up before deletion.,"The presence of cycles means a reference count might not reach zero even if the file is inaccessible, requiring garbage collection.",Users must manually specify deallocation for each block.,Deletion is only allowed at specific times of the day.,There is no mechanism for tracking file references.,B,"In general graph directories, a file or directory might become inaccessible (no valid path from the root leads to it), but its reference count might not drop to zero due to cycles. This requires a more complex 'garbage collection' process to identify and reclaim truly unreachable space."
What is 'garbage collection' in the context of general graph directories?,A process of deleting temporary files to free up space.,A mechanism for automatically repairing corrupted file data.,"The recovery of space containing no-longer-valid data, especially when reference counts don't suffice due to cycles.",A system utility for creating backup copies of the entire file system.,The process of reorganizing disk blocks for better performance.,C,"Garbage collection refers to the process of identifying and recovering disk space occupied by data that is no longer accessible or valid, even if traditional reference counts (which can be problematic with cycles) don't indicate it."
Which concept primarily addresses the issue of physical damage to information by methods like duplicate copies and backups?,Protection,Authentication,Reliability,Encryption,Firewalling,C,"Reliability, often achieved through duplicate copies and backups, is the concept associated with protecting information from physical damage."
What is the main concern addressed by 'protection' in the context of information safety?,Data loss due to hardware failure,Ensuring data integrity during transmission,Preventing improper access to information,Protecting against natural disasters,Optimizing storage space usage,C,The text states that 'protection' is concerned with 'Improper access'.
Which of the following is NOT explicitly listed as a protection mechanism in the text?,User name/password authentication,Encrypting secondary storage,Firewalling network access,Regular antivirus scans,Advanced mechanisms for valid data access in multiuser systems,D,"User name/password authentication, encrypting secondary storage, firewalling network access, and advanced mechanisms for valid data access in multiuser systems are all listed protection mechanisms. Regular antivirus scans are not explicitly mentioned."
Which file access operation is defined as 'Write or rewrite file'?,Read,Execute,Append,Write,Attribute change,D,The 'Write' operation is explicitly defined as 'Write or rewrite file'.
"If a user needs to add new information to the end of an existing file without altering its current content, which file operation should they use?",Write,Read,Execute,Append,Delete,D,The 'Append' operation is defined as 'Write new info at end of file'.
Where is protection typically applied for higher-level functions such as 'rename' or 'copy'?,"At the user interface level, by disabling the functions for unauthorized users.","Within the application layer, using custom access checks.",At the lower-level system calls that implement these functions.,Through external security audits and compliance checks.,Via hardware-enforced memory protection units.,C,"The text states, 'Higher-level functions (rename, copy, edit) often implemented by system programs using lower-level calls. Protection at lower level.'"
What is the 'most general scheme' for access control where access is dependent on user identity?,Password authentication,Role-Based Access Control (RBAC),Access-Control List (ACL),Discretionary Access Control (DAC),Mandatory Access Control (MAC),C,The text explicitly states: 'Most general scheme: access-control list (ACL)'.
Which of the following is identified as a disadvantage of using a general Access-Control List (ACL) scheme?,It limits the complexity of access methodologies.,"It results in fixed-size directory entries, wasting space.","The lists can be lengthy and tedious to construct, especially if users are unknown.",It requires less frequent updates compared to other methods.,It does not allow for fine-grained control over file access.,C,"The text lists 'Lengthy lists (tedious to construct, especially if users unknown)' as a disadvantage of ACLs."
"In the condensed ACL classification scheme, which category refers to 'All other users' who are not the owner or part of a specific group?",Guest,Public,Universe,Other,Default,D,The condensed ACL scheme defines 'Other' as 'All other users'.
"In UNIX permissions, what does 'rwx' signify for a user category (e.g., owner, group, or other)?",Read and Write permissions only.,"Read, Write, and Execute permissions.",Rewrite and Execute permissions only.,"Read, Execute, and Delete permissions.","Rename, Write, and Execute permissions.",B,"The text states that 'rwx' stands for 'read, write, execute' in UNIX permissions."
"When ACLs are combined with regular permissions in Solaris, how is the presence of an ACL indicated?",A special character like '@' prefixes the permissions.,The permission string is enclosed in square brackets.,A plus sign ('+') is appended to the regular permissions.,The permissions are displayed in a different color.,A separate column explicitly states 'ACL Present'.,C,"Solaris indicates ACLs by appending a '+' to regular permissions (e.g., `-rw-r--r--+`)."
"When ACLs are combined with traditional UNIX-like permissions, which typically takes precedence in determining access rights?","Owner permissions, as they are the most specific.","Group permissions, as they apply to a set of users.","ACLs, due to their specificity.","Other (universe) permissions, as they are the least restrictive.",The most restrictive permission set applied to the user.,C,"The text states, 'Precedence: ACLs typically take precedence over group permissions (specificity priority)'."
What is a major disadvantage of using a single password for all files in a system that relies on passwords with each file for protection?,It requires users to remember an excessive number of passwords.,It only allows read-only access to the files.,"It provides an 'all-or-none' protection, making all files vulnerable if that single password is compromised.",It is less efficient than authenticating users with a username.,It does not allow for varying access rights between different files.,C,"The text identifies 'single password for all files (all-or-none protection)' as a disadvantage, meaning the security of all files rests on that one password."
"In multilevel directory structures, what additional aspect does directory protection control besides the creation and deletion of files within the directory?",The encryption method for files in the directory.,The frequency of backups for the directory's contents.,The user's ability to determine a file's existence by listing directory contents.,The physical disk location of the directory.,The maximum size of files that can be stored in the directory.,C,Directory protection also controls 'Control user's ability to determine file existence (listing directory contents)'.
"If a user wants to access a file by its path name in a multilevel directory structure, what access rights are necessary?",Only read access to the file itself.,Only execute access to the directory containing the file.,Access to both the directory and the file.,Write access to the directory and read access to the file.,Administrator privileges for the entire system.,C,"The text states, 'If path name refers to file, user needs access to both directory and file'."
What is the primary characteristic of the memory mapping file access method?,It requires exclusive access to the file by a single process.,It treats file I/O as routine memory accesses using virtual memory techniques.,It is exclusively used for networked file systems.,It strictly uses dedicated system calls for every byte read or written.,It involves caching the entire file in RAM before any access.,B,"Memory mapping treats file I/O as routine memory accesses, leveraging virtual memory techniques for efficiency."
What is a significant performance advantage of using memory-mapped files for I/O?,It guarantees immediate write-through to secondary storage for all changes.,"It eliminates the need for any form of caching, reducing complexity.",It simplifies and speeds up file access by avoiding `read()` and `write()` system call overhead.,"It can only be used for small files, making their access faster.","It provides built-in encryption for all file data, improving security.",C,"Memory-mapped files bypass the overhead of traditional `read()` and `write()` system calls, leading to performance increases."
"When a process first attempts to access a specific portion of a memory-mapped file, what typically occurs?",The entire file is loaded into physical memory immediately.,A direct disk I/O operation is performed for the specific byte accessed.,"A demand paging operation results in a page fault, reading a page-sized portion of the file.",The operating system prevents access until the file is manually cached.,The process must first explicitly call `read()` for that section.,C,"Initial access to a memory-mapped file typically triggers demand paging, leading to a page fault that brings the required page-sized portion of the file into physical memory."
"After the initial page fault, how are subsequent reads and writes to a memory-mapped file handled?",They continue to incur system call overhead for each access.,They are handled as routine memory accesses to the process's virtual address space.,"They trigger a new page fault for every access, regardless of locality.","They are buffered entirely in kernel space, inaccessible directly by the process.",They require the process to explicitly flush changes to disk after each write.,B,"Once a page is loaded due to an initial access, subsequent reads and writes to that portion of the memory-mapped file are treated as routine memory accesses, speeding up operations."
Memory-mapped files simplify and speed up file access by avoiding the overhead of which specific type of operations?,CPU context switches,Network socket communications,Virtual memory management,Disk block allocation,`read()` and `write()` system calls,E,"A key benefit of memory-mapped files is the avoidance of the overhead associated with explicit `read()` and `write()` system calls, as file I/O becomes memory access."
When are updates made to a memory-mapped file generally written back to secondary storage?,Immediately after every byte is written to memory.,Only when the system is shut down.,"Generally, when the file is closed.",When the operating system detects low system memory.,"Never, as they are only temporary in memory.",C,Updates to memory-mapped files are not necessarily immediate; they are generally written back to secondary storage when the file is closed.
What might happen to intermediate changes made to a memory-mapped file if the system experiences memory pressure?,They are immediately discarded to free up memory.,They are forced to be written back to secondary storage without delay.,They may be moved to swap space.,The operating system prevents further writes to the file.,The file mapping is automatically unmapped.,C,"Under memory pressure, intermediate changes in memory-mapped files, like any other virtual memory pages, may be moved to swap space."
"Which operating system is specifically mentioned as memory-mapping all file I/O to the kernel address space, even when using standard file access calls?",Linux,Windows,macOS,Solaris,FreeBSD,D,"The text states that some OS, like Solaris, memory-map all file I/O to kernel address space, even with standard calls."
How do multiple processes achieve data sharing using memory-mapped files concurrently?,By each process making independent copies of the file.,By only allowing one process to map the file at a time.,By mapping the same file concurrently into their respective virtual address spaces.,By using network sockets to transfer data between them.,By serializing access through a central file server.,C,"Multiple processes can map the same file concurrently, allowing them to share data by accessing the same underlying physical pages in memory."
"When multiple processes map the same section of a file, what is the visibility characteristic of writes made by one process?",Writes are only visible to the writing process until the file is closed.,Writes are immediately visible to all other processes mapping the same section.,Writes are only visible after explicit synchronization calls are made by all processes.,Writes are visible to others only if they remap the file.,Writes are never visible to other processes due to isolation.,B,"Writes by one process to a shared memory-mapped section are immediately visible to other processes mapping the same section, as they all point to the same physical memory."
How is concurrent data sharing among processes using memory-mapped files generally implemented at a low level?,Through dedicated hardware registers for each process.,By replicating data across separate physical memory blocks for each process.,By having the virtual memory map of each process point to the same physical page(s).,By using inter-process pipes for data exchange.,By requiring kernel intervention for every data access.,C,Shared memory through memory-mapped files is implemented by the virtual memory maps of participating processes pointing to the same physical pages in memory.
Memory-mapped files support which mechanism for processes that share read-only data but require private copies for modification?,Write-through caching,Copy-on-write,Direct Memory Access (DMA),Pre-fetching,Journaling,B,"Memory-mapped files support copy-on-write, allowing processes to share read-only data efficiently and only get their own copies if they attempt to modify the data."
"When multiple processes share data via memory-mapped files, what mechanism should they typically use for coordination to prevent data corruption?",Sending signals to each other before accessing data.,Polling the file for changes at regular intervals.,Using mutual exclusion primitives like mutexes or semaphores.,Relying on the operating system to automatically manage concurrent writes.,Restricting access to only one process at a time globally.,C,"When processes share data, including through memory-mapped files, they must use mutual exclusion mechanisms to coordinate access and prevent race conditions or data corruption."
What is a common method for implementing shared memory between processes?,Using network sockets for inter-process communication.,Implementing it solely through kernel system calls without direct memory access.,By memory mapping files into the processes' virtual address spaces.,By passing data through temporary files on disk.,Through direct CPU register sharing.,C,The text explicitly states that 'Shared memory often implemented by memory mapping files'.
"According to the Windows API outline for shared memory using memory-mapped files, what is the first step?",Establishing a view of the mapped file.,Creating a file mapping for the file.,Opening the file with `MapViewOfFile()`.,Calling `UnmapViewOfFile()`.,Creating a named pipe.,B,The outline states the first step is to 'Create a file mapping for the file'.
"In the Windows API process of using memory-mapped files for shared memory, what is the second step after creating a file mapping?",Opening the file using `CreateFile()`.,Closing the file mapping object.,Establishing a view of the mapped file in the process's virtual address space.,Writing data to the file mapping object.,Creating a named shared-memory object.,C,The outline states the second step is to 'Establish a view of the mapped file in process's virtual address space'.
"In the context of the Windows API, a mapped file primarily acts as what for inter-process communication?",A transient data buffer.,A message queue.,A shared-memory object.,A named pipe.,A remote procedure call endpoint.,C,The text states: 'Mapped file acts as shared-memory object for inter-process communication'.
Which Windows API function is used to open a file and obtain a `HANDLE` for subsequent memory mapping steps?,CreateFileMapping(),MapViewOfFile(),UnmapViewOfFile(),CreateFile(),OpenFile(),D,The first step in the Windows API example is to 'Open file with `CreateFile()` (returns `HANDLE`)'.
Which Windows API function is responsible for creating a file mapping object using a file `HANDLE`?,CreateFile(),MapViewOfFile(),CreateFileMapping(),OpenFileMapping(),GetFileMappingHandle(),C,The steps list 'Create file mapping with `CreateFileMapping()` (uses file `HANDLE`)'.
Which Windows API function is used to establish a view of a mapped file in a process's virtual address space?,CreateFile(),CreateFileMapping(),UnmapViewOfFile(),MapViewOfFile(),GetViewOfFile(),D,The steps list 'Establish view with `MapViewOfFile()` (uses mapped object `HANDLE`)'.
"What type of object does the `CreateFileMapping()` function create, which can be accessed by name from multiple processes in the Windows API?",A view object,A file handle,A named shared-memory object,A file stream,A temporary file,C,"The text states: '`CreateFileMapping()` creates a named shared-memory object (e.g., `SharedObject`)'."
"What does the `MapViewOfFile()` function return, and how do accesses to this return value relate to the file?",It returns a file handle; accesses refer to the file's metadata.,It returns a pointer to the shared-memory object; accesses to this memory are accesses to the file.,It returns a block ID; accesses require further system calls.,It returns a process ID; accesses imply inter-process communication via messages.,It returns a boolean indicating success; accesses are not direct.,B,The text clarifies: '`MapViewOfFile()` returns pointer to shared-memory object; accesses to this memory are accesses to the file'.
What portion of a file can be mapped into memory using memory mapping?,Only the first page of the file.,"Only the entire file, no partial mapping.",Either the entire file or a specific portion of it.,Only the metadata of the file.,"A fixed 4KB block, regardless of file size.",C,The text states: 'Entire file or portion can be mapped'.
How is a memory-mapped file typically loaded into physical memory?,The entire file is loaded into physical memory at once upon mapping.,It is loaded only when specifically requested via a `read()` system call.,"It may be demand-paged, meaning portions are loaded as accessed.",It is always streamed directly from disk on access without caching.,It resides entirely in swap space until explicitly moved by the user.,C,"The text mentions that 'Mapped file may be demand-paged', meaning parts are loaded into physical memory only when accessed."
Which Windows API function is used by processes to remove a view of a mapped file from their virtual address space?,CloseHandle(),DeleteFileMapping(),UnmapViewOfFile(),ReleaseMappedFile(),FreeMemoryMap(),C,The text specifies: 'Both processes remove view with `UnmapViewOfFile()`'.
"According to the provided glossary, what is the definition of 'memory mapping'?",A technique to allocate dynamic memory on the heap.,A file-access method where file is mapped into process memory space for direct memory access.,The process of storing program instructions in ROM.,A method to cache frequently accessed data in CPU registers.,A system call for direct interaction with secondary storage devices.,B,The glossary defines 'memory mapping' as 'File-access method where file is mapped into process memory space for direct memory access'.
"In the context of Windows API, what does the term 'file mapping' refer to according to the glossary?",The final step in releasing shared memory resources.,The act of creating a physical file on disk.,The first step in memory-mapping a file.,A process of transferring data over a network.,A method for encrypting file contents.,C,The glossary defines 'file mapping' (in Windows) as 'the first step in memory-mapping a file'.
"According to the glossary, what is a 'view' in the context of Windows memory-mapped files?",A graphical user interface component.,An abstract representation of file permissions.,An address range mapped in shared memory; the second step in memory-mapping a file.,A snapshot of the file's contents at a specific time.,A kernel-level object representing an open file.,C,The glossary defines 'view' (in Windows) as 'an address range mapped in shared memory; second step in memory-mapping a file'.
"In the Windows API, what is a 'named shared-memory object'?",A temporary file created by the operating system.,A network resource shared among multiple computers.,A section of a memory-mapped file accessible by name from multiple processes.,A special type of mutex for inter-process synchronization.,A data structure used for direct kernel communication.,C,"The glossary defines 'named shared-memory object' as 'In Windows API, a section of a memory-mapped file accessible by name from multiple processes'."
"According to the text, what is a file characterized as?",A physical block on a hard disk,An abstract data type consisting of a sequence of logical records,A collection of executable programs,A temporary storage area for system processes,A hardware component responsible for data storage,B,"The text states: 'File: abstract data type, sequence of logical records (byte, line, complex data).'"
What is a primary task of the Operating System (OS) concerning files?,To create new application programs,To manage network traffic for remote users,To map the logical file concept to physical storage,To perform garbage collection for all memory spaces,To encrypt all data before it's stored,C,"The text states: 'OS task: map logical file concept to physical storage (hard disk, NVM).'"
"Regarding file record types, who primarily supports them or leaves the support to the application?",The user directly,The compiler,The operating system,The hardware,The network protocol,C,The text notes: 'OS may support record types or leave to application.'
"What is a significant problem encountered with a single-level directory system, especially in a multiuser environment?",Difficulty in assigning read-only permissions,Limited storage capacity for files,Naming problems requiring unique names for all files,Inability to support executable files,High latency for file access,C,The text explicitly mentions: 'Single-level directory: naming problems in multiuser systems (unique names required).'
How does a two-level directory system address the naming problems found in single-level directories?,By allowing shorter file names,By assigning each file a unique hexadecimal ID,By creating a separate directory for each user,By using a distributed hash table for file lookup,By encrypting file names,C,"The text states: 'Two-level directory: separate directory for each user, solves naming problems.'"
What information is typically listed for files within a two-level directory system?,Only the file's content and creation date,"File name, location, length, type, owner, and times","Network path, MAC address, and checksum",Processor architecture and memory usage,User's IP address and operating system version,B,"The text specifies: 'Lists file name, location, length, type, owner, times.'"
What is the primary characteristic of a tree-structured directory system?,"All files are stored in a single, flat directory.","It is a generalization of the two-level directory, allowing subdirectories.",It exclusively uses hard links for file sharing.,It requires manual deletion of unused file entries.,It only supports read-only files.,B,"The text describes it as: 'Tree-structured directory: generalization of two-level, allows subdirectories for organization.'"
Which type of directory structure allows for the sharing of subdirectories and files but complicates searching and deletion?,Single-level directory,Two-level directory,Tree-structured directory,Acyclic-graph directory,General graph structure,D,"The text states: 'Acyclic-graph directory: allows sharing of subdirectories/files, but complicates searching/deletion.'"
What is a potential complication when using an acyclic-graph directory structure?,Increased simplicity in file deletion.,Guaranteed unique file names across the system.,Complication in searching and deletion processes.,Restriction to a single owner per file.,Inability to link files across different volumes.,C,"The text states: 'Acyclic-graph directory: allows sharing of subdirectories/files, but complicates searching/deletion.'"
Which directory structure offers complete flexibility in sharing but may necessitate garbage collection for unused space?,Single-level directory,Two-level directory,Tree-structured directory,Acyclic-graph directory,General graph structure,E,"The text describes: 'General graph structure: complete flexibility in sharing, but may require garbage collection for unused space.'"
What are the key challenges associated with remote file systems as mentioned in the text?,Only storage capacity and file size limits.,Primarily user interface design and software licensing.,"Reliability, performance, and security.",Operating system compatibility and hardware upgrades.,Text encoding and character set support.,C,"The text lists: 'Remote file systems: challenges in reliability, performance, security.'"
In which type of system is file protection identified as particularly necessary?,Single-user systems,Batch processing systems,Multiuser systems,Embedded systems,Standalone systems,C,The text states: 'File protection: needed on multiuser systems.'
Which of the following are listed as types of access control for file protection?,"Compile, link, debug","Setup, install, uninstall","Read, write, execute, append, delete, list directory","Download, upload, stream","Zip, unzip, compress",C,"The text specifies: 'Access controlled by type: read, write, execute, append, delete, list directory.'"
What methods are mentioned for achieving file protection?,Only biometric authentication,Through encryption keys exclusively,"Via access lists, passwords, and other techniques",By limiting physical access to storage devices,Using only network firewalls,C,"The text states: 'Protection via access lists, passwords, other techniques.'"
"When a thread requests a resource that is currently unavailable, what state does it enter?",Terminated state,Running state,Waiting state,Ready state,Suspended state,C,"The text explicitly states under 'Resource Request': 'If unavailable, thread enters a waiting state'."
"Which of the following best describes a deadlock situation, as defined in the introduction?",A thread is indefinitely running a critical section.,A waiting thread can never change state because its requested resources are held by other waiting threads.,Multiple threads simultaneously access a shared resource without synchronization.,A thread requests a resource that does not exist.,A system crash due to excessive resource allocation.,B,The introduction defines deadlock as 'A situation where a waiting thread can never change state because its requested resources are held by other waiting threads'.
"According to the formal definition, a deadlock occurs when every process in a set is waiting for an event that can only be caused by which of the following?",An external interrupt,Another process in the set,The operating system kernel,User input,A timer expiration,B,The formal definition of deadlock is: 'Every process in a set is waiting for an event that can only be caused by another process in the set'.
How is the system described in terms of resources and threads?,An infinite number of resources shared among infinite threads.,A finite number of resources distributed among competing threads.,Resources are unlimited and available on demand.,Resources are exclusively allocated to a single thread.,"Resources are only available to processes, not threads.",B,The 'System model' section states the 'System Composition' is a 'Finite number of resources distributed among competing threads'.
How are resources typically organized or partitioned in the system model?,They are randomly assigned without structure.,"They are partitioned into types (classes), each with identical instances.",They are grouped by their physical location.,They are categorized by their usage frequency.,"They are all considered to be of a single, generic type.",B,"Under 'Resource Types', the text states: 'Resources partitioned into types (classes), each with identical instances'."
What characteristic of resource types ensures flexibility when a thread requests a resource?,Resources must be allocated sequentially.,Resources can only be used by their specific owner thread.,Any instance of a resource type should satisfy a request.,Resource instances are unique and non-interchangeable.,Resources are always pre-allocated to threads.,C,The text specifies that 'Any instance of a resource type should satisfy a request'.
Which of the following is NOT explicitly given as an example of a resource type or class in the text?,CPU cycles,Files,I/O devices,Mutex locks,User input,E,"CPU cycles, files, and I/O devices are given as examples of resource types. Mutex locks are stated to be 'common sources of deadlock' and 'Each lock instance is typically its own resource class'. User input is not mentioned as a resource type."
What are two common synchronization tools explicitly mentioned as sources of deadlock?,Message queues and pipes,Mutex locks and semaphores,Event flags and condition variables,Spinlocks and atomic operations,Monitors and barriers,B,"The text states, 'Mutex locks and semaphores are common sources of deadlock'."
What is the correct sequence of steps for a thread utilizing a resource?,"Use, Request, Release","Release, Request, Use","Request, Use, Release","Request, Release, Use","Use, Release, Request",C,"The 'Resource Utilization Sequence' is listed as 1. Request, 2. Use, 3. Release."
"During the 'Request' step of resource utilization, what happens if the requested resource is not immediately available?",The thread immediately terminates.,The thread proceeds without the resource.,The thread waits.,The operating system grants the resource anyway.,The resource is forcibly taken from another thread.,C,The description for the 'Request' step states: 'Thread requests a resource. Waits if not immediately available'.
Which of the following is explicitly mentioned as a mechanism for threads to request and release resources via system management?,Direct memory manipulation,Command-line interface commands,System calls like request() and release(),Inter-process communication via sockets,Hardware interrupts,C,"The text states: 'Request/release can be system calls (request(), release(), open(), close(), allocate(), free())'."
How does the Operating System (OS) check for resource allocation?,By polling all active threads.,Through a system table.,By directly inspecting hardware registers.,By receiving notifications from applications.,By user configuration files.,B,The text mentions that 'OS checks for resource allocation via a system table'.
What information does the OS system table track regarding resources?,Only the total number of resources.,Only resources currently in use.,Free/allocated resources and the owning thread.,The performance metrics of resource usage.,Historical logs of resource requests.,C,The system table 'tracks free/allocated resources and the owning thread'.
"What are the main events involved when a set of threads enters a deadlocked state, as described in the system model?",Only resource release.,Only CPU scheduling.,Resource acquisition and release.,Network communication.,Disk I/O operations.,C,"The text states that in a 'Deadlocked State', the 'Main events: resource acquisition and release'."
"Which classic problem is given as an example to illustrate a deadlocked state where threads exhibit a circular wait (e.g., holding one resource and waiting for another)?",Producer-consumer problem,Reader-writer problem,Dining-philosophers problem,Bounded-buffer problem,Sleeping barber problem,C,"The Dining-philosophers problem is specifically mentioned as an example of a deadlocked state: 'Each philosopher holds one chopstick and waits for another, creating a circular wait'."
What is a key responsibility of a developer regarding deadlock possibilities?,To ignore them as the OS handles all deadlocks.,To ensure locks are always acquired in an arbitrary order.,To be aware of them and manage lock acquisition/release carefully.,To use as many locks as possible without concern.,To delegate all locking to external libraries.,C,"Under 'Developer Responsibility', it states: 'Must be aware of deadlock possibilities' and 'require careful management of lock acquisition/release to avoid deadlocks'."
"While locking tools are effective in preventing race conditions, what specific management is required from developers to avoid deadlocks?",Minimizing the number of locks used.,Using only reentrant locks.,Careful management of lock acquisition/release.,Relying on hardware-level atomicity.,Disabling preemption.,C,The text advises that locking tools 'require careful management of lock acquisition/release to avoid deadlocks'.
"According to the section glossary, what defines a deadlock?",A situation where a process consumes all available CPU cycles.,The state in which two processes or threads are stuck waiting for an event that can only be caused by one of the processes or threads.,An error where a process attempts to access invalid memory.,A condition where a system runs out of disk space.,An infinite loop within a single thread.,B,The section glossary defines 'deadlock' as 'The state in which two processes or threads are stuck waiting for an event that can only be caused by one of the processes or threads'.
What is the primary function of the `pthread_mutex_init()` function in Pthreads?,Acquires a mutex lock.,Releases a mutex lock.,Initializes an unlocked mutex.,Destroys a mutex.,Blocks if a lock is held.,C,The text states that `pthread_mutex_init()` initializes an unlocked mutex.
What happens when a thread calls `pthread_mutex_lock()` on a mutex that is currently held by another thread?,It immediately acquires the lock.,It returns an error code.,It blocks until the lock is released.,It initializes a new mutex.,It releases the lock it holds.,C,The text states that `pthread_mutex_lock()` acquires a lock and 'blocks if the lock is held'.
What is the purpose of the `pthread_mutex_unlock()` function?,To attempt to acquire a lock without blocking.,To release a mutex lock.,To initialize a mutex.,To check the status of a mutex.,To destroy a mutex.,B,The text explicitly states that `pthread_mutex_unlock()` releases a lock.
"In the Pthreads example demonstrating deadlock, how many mutex locks and threads are involved in the scenario?","One mutex, two threads.","Two mutexes, one thread.","Two mutexes, two threads.","Three mutexes, two threads.","One mutex, one thread.",C,"The deadlock scenario description mentions 'Two mutex locks created: `first_mutex`, `second_mutex`' and 'Two threads, `thread_one` and `thread_two`'."
Which locking order between `thread_one` and `thread_two` is described as leading to a potential deadlock in the provided example?,`thread_one` locks `first_mutex` then `second_mutex`; `thread_two` also locks `first_mutex` then `second_mutex`.,`thread_one` locks `first_mutex` then `second_mutex`; `thread_two` locks `second_mutex` then `first_mutex`.,`thread_one` locks `second_mutex` then `first_mutex`; `thread_two` locks `first_mutex` then `second_mutex`.,Both threads attempt to lock both mutexes simultaneously.,Both threads lock `second_mutex` then `first_mutex`.,B,"The text states: '`thread_one` locks in order: (1) `first_mutex`, (2) `second_mutex`' and '`thread_two` locks in order: (1) `second_mutex`, (2) `first_mutex`'."
What specific condition must occur for a deadlock to be possible in the given Pthreads example?,Both threads acquire both `first_mutex` and `second_mutex` successfully in their specified order.,"`thread_one` acquires `first_mutex` and `thread_two` acquires `second_mutex`, leading both to block waiting for the other's lock.",One thread acquires and releases both locks before the other thread starts execution.,Neither thread is able to acquire any mutex lock.,Both threads acquire `second_mutex` and then `first_mutex`.,B,"The text explains the deadlock is possible 'If `thread_one` acquires `first_mutex` and `thread_two` acquires `second_mutex`, both threads will block waiting for the other's lock'."
Why is it often difficult to identify and test for deadlocks in multithreaded applications?,"Deadlocks always cause immediate system crashes, preventing analysis.",Deadlock detection tools are not available for multithreaded environments.,The occurrence of deadlock is intermittent and depends on the CPU scheduler.,"Deadlocks only manifest on specific hardware architectures, not in testing environments.","Deadlocks only occur when applications are under extremely high load, making replication hard.",C,The text states: 'Deadlock might not occur if one thread acquires and releases both locks before the other thread starts.' and 'Occurrence depends on the CPU scheduler.' which 'Makes identifying and testing for deadlocks difficult'.
Which characteristic distinguishes livelock from deadlock?,"In livelock, threads are permanently blocked and cannot execute.",Livelock applies only to single-threaded applications.,"In livelock, threads are actively attempting operations but fail to make progress.",Livelock results in resources being continuously acquired and released productively.,"Livelock is a type of resource starvation, not a liveness failure.",C,"The text states livelock is 'Similar to deadlock, but threads are not blocked' and illustrates with the analogy of two people 'They are active but make no progress'."
"According to the provided text, what is the core condition that defines livelock?",A thread becomes permanently blocked while waiting for a resource.,A thread continuously attempts an action that fails.,A thread repeatedly acquires and releases a lock successfully.,A thread is unable to acquire any resource due to other threads holding them indefinitely.,A thread enters an infinite loop performing meaningful work.,B,The text defines livelock as a 'Condition: A thread continuously attempts an action that fails' and the glossary also states this.
What is the behavior of `pthread_mutex_trylock()` when it attempts to acquire a lock that is already held by another thread?,It blocks the calling thread until the lock becomes available.,It forces the lock to be released by the current holder.,"It attempts to acquire the lock and returns immediately, indicating success or failure, without blocking.",It initializes a new mutex if the requested one is unavailable.,It causes a deadlock if the lock is held.,C,"The text states `pthread_mutex_trylock()` 'attempts to acquire a lock without blocking'. If it cannot acquire, it will indicate failure, allowing the thread to proceed without waiting."
"In the Pthreads livelock scenario involving `pthread_mutex_trylock()`, what sequence of events leads to livelock?",Both threads successfully acquire their first mutex and then block indefinitely on the second mutex.,"`thread_one` acquires `first_mutex`, `thread_two` acquires `second_mutex`; both fail to acquire the other's mutex using `trylock`, release their own, and repeat indefinitely.","One thread acquires both locks and completes its work, allowing the other thread to proceed.","Both threads attempt to acquire all locks simultaneously, causing a system error.","Threads acquire locks in a random order, eventually resolving the contention.",B,"The text describes the livelock scenario: '`thread_one` acquires `first_mutex`, `thread_two` acquires `second_mutex`. Both then call `trylock` on the other mutex, which fails. They release their locks and repeat indefinitely.'"
What is a common strategy mentioned to avoid livelock?,Ensuring all threads acquire locks in a predefined global order.,Having threads retry failing operations at random times.,Using blocking lock acquisition mechanisms exclusively.,Minimizing the number of shared resources between threads.,Implementing a watchdog timer to terminate non-progressing threads.,B,The text states that livelock 'Can be avoided by having threads retry at random times'.
"The Ethernet example, where hosts involved in a network collision 'backoff' for a random period before retransmitting, is used to illustrate a method for avoiding which concurrency issue?",Deadlock.,Race condition.,Starvation.,Livelock.,Data inconsistency.,D,"This example is provided directly under the 'Avoidance' section for Livelock, showing how random backoff helps prevent threads from repeatedly failing at the same time."
"Compared to deadlock, what is stated about the commonness of livelock in concurrent application design?",It is far more common than deadlock.,It is equally common as deadlock.,It is less common than deadlock.,It is a theoretical concept that does not occur in real-world applications.,Its commonness depends entirely on the operating system used.,C,"The text explicitly states that livelock is 'Less common than deadlock, but still a challenge'."
"According to the Section Glossary, what is the definition of livelock?","A condition in which two or more threads are permanently blocked, waiting for each other.",A condition in which a thread continuously attempts an action that fails.,"A situation where a thread holds a resource indefinitely, preventing others from acquiring it.","A state where multiple threads try to access a shared resource simultaneously, leading to incorrect data.","A scenario where a thread is repeatedly preempted by the scheduler, preventing it from making progress.",B,The glossary defines 'livelock' as 'A condition in which a thread continuously attempts an action that fails'.
What is a fundamental requirement for a deadlock situation to arise in a system?,At least two conditions must hold simultaneously.,Only the circular-wait condition is strictly necessary.,"All four specified conditions (Mutual exclusion, Hold and wait, No preemption, and Circular wait) must hold concurrently.",Deadlocks can arise even if only one condition is met.,Deadlocks are only possible if resources are sharable.,C,"The text states, ""A deadlock situation can arise if the following four conditions hold simultaneously in a system."""
Which of the following conditions for deadlock states that at least one resource must be held in a nonsharable mode?,Hold and wait,No preemption,Circular wait,Mutual exclusion,Resource allocation,D,"Mutual exclusion is defined as ""At least one resource must be held in a nonsharable mode."""
"In the context of deadlock conditions, what does ""Hold and wait"" mean?",A thread is waiting for resources but holds none.,A thread has released all its resources and is waiting for new ones.,A thread must be holding at least one resource and waiting to acquire additional resources held by other threads.,A thread is preempting resources from another thread.,A thread is only holding resources and not waiting for any.,C,"""Hold and wait"" means ""A thread must be holding at least one resource and waiting to acquire additional resources held by other threads."""
"According to the necessary conditions for deadlock, what does ""No preemption"" imply about resource release?",A resource can be forcibly taken away from a thread by the operating system.,Resources are always preemptible if a higher-priority thread requests them.,A resource can be released only voluntarily by the thread holding it.,Resources are automatically released after a fixed time.,A thread can preempt resources from another thread if it waits long enough.,C,"""No preemption"" means ""A resource can be released only voluntarily by the thread holding it."""
"Describe the ""Circular wait"" condition for deadlock.",All threads are waiting for the same resource.,"A set of waiting threads {T_0, T_1, ..., T_n} must exist such that T_0 is waiting for a resource held by T_1, T_1 is waiting for a resource held by T_2, ..., T_{n-1} is waiting for a resource held by T_n, and T_n is waiting for a resource held by T_0.",Threads are waiting in a linear sequence for resources.,A thread is waiting for a resource it previously held.,Threads are waiting for resources that are not currently available anywhere in the system.,B,The definition of circular wait explicitly states this chain of waiting threads.
Which of the following conditions is implied by the circular-wait condition?,Mutual exclusion,No preemption,Hold and wait,Resource starvation,Deadlock avoidance,C,"The text states, ""The circular-wait condition implies the hold-and-wait condition."""
What is the primary purpose of a system resource-allocation graph?,To visualize thread execution order.,To describe deadlocks precisely.,To manage memory allocation.,To represent network topology.,To monitor CPU utilization.,B,"The text and glossary define it as ""A directed graph for precise description of deadlocks."""
What are the two types of vertices that partition the set V in a system resource-allocation graph?,Edges and Nodes,Processes and Resources,Active threads and Resource types,Input and Output,Mutexes and Semaphores,C,Vertices V are partitioned into two types: T (active threads) and R (resource types).
"In a system resource-allocation graph, what does a directed edge from thread T_i to resource type R_j (T_i \rightarrow R_j) represent?",Resource R_j is allocated to thread T_i.,Thread T_i has released resource R_j.,Thread T_i has requested an instance of R_j.,Resource R_j is available for T_i.,Thread T_i is preempting resource R_j.,C,"An edge T_i \rightarrow R_j is a request edge, signifying that T_i has requested an instance of R_j."
What does a directed edge from resource type R_j to thread T_i (R_j \rightarrow T_i) signify in a system resource-allocation graph?,Thread T_i is requesting resource R_j.,An instance of R_j has been allocated to T_i.,Resource R_j is about to be released by T_i.,Thread T_i is waiting for R_j.,Resource R_j is being shared by T_i and other threads.,B,"An edge R_j \rightarrow T_i is an assignment edge, signifying that an instance of R_j has been allocated to T_i."
What can be definitively concluded if a system resource-allocation graph contains no cycles?,A deadlock may still exist.,All threads are actively running.,No thread is deadlocked.,All resources are fully utilized.,"The system is in a safe state, but deadlocks are still possible.",C,"The text states, ""If the graph contains no cycles, no thread is deadlocked."""
"If a system resource-allocation graph contains a cycle and each resource type has exactly one instance, what is the implication?","A deadlock is possible, but not certain.",No deadlock can occur.,A deadlock has occurred.,The system is in a safe state.,All resources are sharable.,C,"The text specifies, ""If each resource type has exactly one instance, a cycle implies a deadlock has occurred."""
"When a system resource-allocation graph contains a cycle, but each resource type has several instances, what does the cycle indicate?",A deadlock is guaranteed.,The system is deadlock-free.,A cycle is a necessary but not a sufficient condition for deadlock.,The system will recover from the deadlock automatically.,All resources are currently available.,C,"The text specifies, ""If each resource type has several instances, a cycle is a necessary but not a sufficient condition for deadlock."""
What are the three fundamental approaches to handling deadlocks mentioned in the text?,"Prevention, Detection, and Recovery.","Ignore, Prevent/Avoid, and Detect/Recover.","Avoidance, Resolution, and Preemption.","Monitoring, Logging, and Restarting.","Scheduling, Prioritizing, and Terminating.",B,"The text lists 'Ignore problem', 'Prevent/Avoid', and 'Detect/Recover' as the three ways to deal with deadlocks."
Which deadlock handling method is most commonly adopted by operating systems like Linux and Windows?,Deadlock Prevention.,Deadlock Avoidance.,Deadlock Detection and Recovery.,Ignoring the problem.,A combination of Prevention and Avoidance.,D,"The text states that 'Ignore problem' is common due to the infrequency of deadlocks and the cost of other methods, specifically mentioning Linux and Windows."
Why is ignoring the deadlock problem a common solution in many operating systems?,It is the most efficient method to prevent deadlocks from occurring.,Deadlocks are very frequent but easily resolved by users.,"Deadlocks occur infrequently, and other methods are costly.",It guarantees that the system never enters a deadlocked state.,It allows the system to automatically recover without user intervention.,C,The text explicitly states that 'ignoring' is common due to 'infrequency of deadlocks and cost of other methods'.
Which deadlock handling strategy explicitly allows the system to enter a deadlocked state before taking action?,Deadlock Prevention.,Deadlock Avoidance.,Ignoring the problem.,Deadlock Detection and Recovery.,Resource preemption.,D,"The 'Detect/Recover' method is described as allowing the system to 'enter deadlocked state, then detect and recover'."
Which type of systems are specifically mentioned as typically employing the Deadlock Detection and Recovery method?,General-purpose desktop operating systems.,Real-time embedded systems.,Database systems.,Mobile operating systems.,Web servers.,C,The text provides 'databases' as an example of systems that use 'Detect/Recover'.
"What is the primary goal of Deadlock Prevention, as defined in the text?",To provide advance information to the OS about resource requests.,To allow the system to enter a deadlocked state for later recovery.,To ensure at least one necessary condition for deadlock cannot hold.,To detect deadlocks quickly after they occur.,To manually restart the system upon deadlock detection.,C,The glossary defines 'deadlock prevention' as 'Methods to ensure at least one necessary condition for deadlock cannot hold.'
How does Deadlock Prevention prevent deadlocks from occurring?,By providing the OS with advance information on resource use.,By examining the system state to determine if a deadlock occurred.,By constraining resource request methods.,By allowing resources to be preempted from processes.,By recovering the system after a deadlock has formed.,C,The text states Deadlock Prevention 'Prevents deadlocks by constraining resource request methods'.
"For Deadlock Avoidance, what kind of information is provided to the OS in advance?",Current CPU utilization of all threads.,Memory usage limits for each thread.,The resources a thread will request/use.,The priority levels of various system processes.,Network bandwidth requirements for future operations.,C,Deadlock Avoidance is described as the OS being 'given advance info on resources a thread will request/use'.
"When the OS employs Deadlock Avoidance, which of the following is NOT one of the factors it considers to decide if a resource request can be satisfied or delayed?",Currently available resources.,Resources allocated to each thread.,Future requests/releases of each thread.,The total number of CPU cores in the system.,Information provided in advance by the threads.,D,"The text explicitly lists 'Currently available resources', 'Resources allocated to each thread', and 'Future requests/releases of each thread' as factors considered by the OS. The number of CPU cores is not mentioned."
What is the definition of 'deadlock avoidance' according to the provided glossary?,Methods to ensure at least one necessary condition for deadlock cannot hold.,"Allowing the system to enter a deadlocked state, then detecting and recovering.",OS method where processes inform OS of resource use; system approves/denies requests to avoid deadlock.,A common approach in most OS due to the infrequency of deadlocks.,Algorithms designed to examine system state to determine if deadlock occurred and recover from it.,C,The glossary defines 'deadlock avoidance' as 'OS method where processes inform OS of resource use; system approves/denies requests to avoid deadlock.'
Under what circumstances would a system primarily rely on Deadlock Detection and Recovery?,As a primary proactive measure to prevent all deadlocks.,When deadlocks are exceptionally rare and insignificant.,If no prevention or avoidance methods are currently in place.,To minimize the cost associated with resource allocation.,When the system design mandates immediate resolution of all resource conflicts.,C,"The text states: 'If no prevention/avoidance, deadlock may arise,' implying detection and recovery is used in such cases."
The algorithms provided by a system for Deadlock Detection and Recovery are designed to perform which two main functions?,Constrain resource requests and allocate resources safely.,Receive advance information and approve/deny requests.,Examine system state to determine if deadlock occurred and recover from it.,Ignore deadlocks and then restart the system manually.,Prioritize threads and preempt resources when necessary.,C,The text specifies that the algorithms are for 'Examine system state to determine if deadlock occurred' and 'Recover from deadlock'.
Which of the following is a primary outcome if a deadlock remains undetected in a system?,The system automatically and immediately resolves the resource conflicts.,Improved overall system performance due to efficient resource locking.,Resources are automatically released after a short timeout period.,"System performance deteriorates, resources remain held, leading eventually to a manual restart.",The system enters a safe state where no new deadlocks can form spontaneously.,D,"The text explicitly states that 'Undetected deadlock: system performance deteriorates, resources held, more threads deadlock. Eventually, manual restart needed.'"
"What other type of system failure's manual recovery method might be applied for deadlock recovery, according_to the text?",System crash due to hardware failure.,Memory leak leading to out-of-memory errors.,"Livelock, which is a type of liveness failure.",Data corruption during file operations.,Process starvation due to low priority.,C,"The text mentions: 'Manual recovery for other liveness failures (e.g., livelock) may be used for deadlock recovery.'"
Under what circumstances does deadlock occur?,If any one of the four necessary conditions holds.,Only if the mutual exclusion and no preemption conditions hold.,If all four necessary conditions hold simultaneously.,When a thread requests resources in increasing order of enumeration.,If resources are allocated but remain unused for long periods.,C,The text states: 'Deadlock occurs if all four necessary conditions hold.'
What is the primary goal of deadlock prevention?,To detect deadlocks quickly and recover from them.,To ensure that at least one of the necessary conditions for deadlock cannot hold.,To allow threads to request resources only when they are holding none.,To predefine a safe sequence for resource allocation.,To maximize resource utilization and prevent starvation.,B,The text states: 'Prevent deadlock by ensuring at least one condition cannot hold.' The glossary also defines 'deadlock prevention' as 'A set of methods intended to ensure that at least one of the necessary conditions for deadlock cannot hold.'
"Which type of resource, due to its nature, cannot be involved in a deadlock because it does not require mutual exclusion?",Mutex locks,Semaphores,CPU registers,Read-only files,Database transactions,D,"The text states: 'Sharable resources (e.g., read-only files) do not require mutual exclusion, thus cannot be involved in deadlock.'"
Why is denying the mutual-exclusion condition generally not a viable strategy for preventing deadlocks?,It leads to low resource utilization.,It causes threads to wait indefinitely for popular resources.,"Some resources are intrinsically nonsharable, like mutex locks.","It only prevents circular wait, not other conditions.",It requires threads to release all current resources before requesting more.,C,"The text states: 'Cannot generally prevent deadlocks by denying mutual-exclusion, as some resources are intrinsically nonsharable (e.g., mutex locks).'"
What is Protocol 1 for preventing the 'hold and wait' condition?,Threads must release all current resources before requesting more.,Threads must request and allocate all resources before execution begins.,Threads' currently held resources are preempted if a new request must wait.,Threads are required to request resources in an increasing order of enumeration.,The operating system denies requests if they lead to an unsafe state.,B,The text describes Protocol 1 for hold-and-wait prevention as: 'Thread requests/allocates all resources before execution.'
Which of the following describes Protocol 2 for preventing the 'hold and wait' condition?,A thread acquires all necessary resources in a single atomic operation.,A thread may request resources only when it is not holding any other resources.,"If a thread cannot get a requested resource, it releases all held resources and tries again.",Resources are preempted from a waiting thread and allocated to a requesting thread.,Resources are allocated based on a predefined hierarchical ordering.,B,The text describes Protocol 2 for hold-and-wait prevention as: 'Thread requests resources only when holding none. Must release all current resources before requesting more.'
What is a common disadvantage of both protocols designed to prevent the 'hold and wait' condition?,Increased system overhead due to frequent preemption.,High resource utilization as resources are quickly released.,"Low resource utilization, as resources may be allocated but unused for long periods.",Inability to apply to sharable resources.,Complexity in imposing a total ordering of resource types.,C,The text lists 'Low resource utilization: resources allocated but unused for long periods' as a disadvantage of both hold-and-wait protocols.
Another disadvantage common to both 'hold and wait' prevention protocols is:,They require mutual exclusion to be denied.,They are impractical for static resource allocation.,They lead to an increase in deadlocks.,"Starvation is possible, where a thread waits indefinitely for popular resources.",They are only applicable to mutex locks and semaphores.,D,The text lists 'Starvation possible: thread waits indefinitely for popular resources' as a disadvantage of both hold-and-wait protocols.
"In one protocol to prevent 'no preemption', what happens if a thread requests a resource and must wait?",The thread's request is immediately denied to prevent deadlock.,All resources currently held by that thread are preempted (implicitly released).,The thread is put into a queue and waits for all its requested resources to become available without releasing any current ones.,The operating system checks if other threads can release resources to satisfy the request.,The thread is immediately terminated to free up resources.,B,"Protocol 1 for no-preemption states: 'If thread requests resource and must wait, all currently held resources are preempted (implicitly released).'"
Which type of resources are often suitable for implementing deadlock prevention protocols based on 'no preemption'?,Mutex locks and semaphores,"Resources whose state cannot be saved or restored, like printers.","Resources whose state can be saved and restored, such as CPU registers or database transactions.",Sharable resources like read-only files.,Network connections that are continuously in use.,C,"The text states that no-preemption protocols are 'Often applied to resources whose state can be saved/restored (e.g., CPU registers, database transactions).'"
Deadlock prevention protocols based on 'no preemption' generally cannot be applied to which of the following?,CPU registers,Database transactions,Memory pages,Mutex locks and semaphores,Files that support exclusive write access,D,The text states: 'Cannot generally apply to mutex locks and semaphores (where deadlocks commonly occur).'
What is the practical solution mentioned for preventing the 'circular wait' condition?,Ensuring all resources are requested before execution.,Preempting resources from threads that are waiting.,Imposing a total ordering of all resource types.,Denying mutual exclusion for all resources.,Ensuring that no thread holds resources while waiting for another.,C,The text states: 'Practical solution: impose total ordering of all resource types.'
"When preventing 'circular wait' by imposing a total ordering of resource types, what rule must threads follow when requesting resources?",Threads must request resources in decreasing order of enumeration.,"Threads can request resources in any order, but must release them in increasing order.",Threads must request resources in increasing order of enumeration.,Threads must request all resources of the same type in separate requests.,Threads are only allowed to request resources they do not currently hold.,C,The text states: 'Require threads to request resources in increasing order of enumeration.'
"According to the total ordering protocol for circular wait prevention, what should a thread do if it needs multiple instances of the same resource type?",It must issue separate requests for each instance.,It must issue a single request for all instances needed.,It should only request one instance at a time to avoid complex ordering.,It must release all other resources before requesting multiple instances.,This scenario is not covered by the total ordering protocol.,B,"The text specifies: 'If multiple instances of same resource type needed, single request for all must be issued.'"
What is a known challenge in implementing the total ordering protocol for deadlock prevention?,The protocol does not guarantee deadlock prevention.,It often leads to high resource utilization.,It is difficult to apply to sharable resources.,Developing a consistent and effective ordering can be difficult for many locks.,"It requires threads to release all resources before requesting more, which is inefficient.",D,The text notes: 'Developing ordering can be difficult for many locks.'
Which Java mechanism is mentioned as being used for lock acquisition ordering in the context of preventing circular wait?,synchronized keyword,java.util.concurrent.locks.ReentrantLock,System.identityHashCode(Object),Object.wait() and Object.notify(),java.util.concurrent.Semaphore,C,The text states: 'Java uses `System.identityHashCode(Object)` for lock acquisition ordering.'
What is the definition of 'deadlock prevention' as provided in the glossary?,An operating system method where processes inform the OS of resources they will use during their lifetimes.,A set of methods intended to ensure that at least one of the necessary conditions for deadlock cannot hold.,A technique to detect deadlocks as they occur and recover gracefully.,A strategy to minimize resource allocation overhead.,A method to prioritize resource requests to avoid starvation.,B,The glossary defines 'deadlock prevention' as 'A set of methods intended to ensure that at least one of the necessary conditions for deadlock cannot hold.'
What is the definition of 'deadlock avoidance' as provided in the glossary?,A method to strictly enforce a total ordering of all resource types.,A technique where threads allocate all resources before execution begins.,An operating system method in which processes inform the operating system of which resources they will use during their lifetimes so the system can approve or deny requests to avoid deadlock.,A protocol that preempts resources from waiting threads to satisfy new requests.,A strategy to ensure that mutual exclusion never holds for any resource.,C,The glossary defines 'deadlock avoidance' as 'An operating system method in which processes inform the operating system of which resources they will use during their lifetimes so the system can approve or deny requests to avoid deadlock.'
"Which of the following is an alternative rule for preventing circular wait using total ordering, as mentioned in the text?",A thread requesting resource R_j must also request all R_i with F(R_i) < F(R_j).,A thread requesting R_j must have released any R_i such that F(R_i) >= F(R_j).,A thread requesting R_j must acquire R_j before any R_i where F(R_i) > F(R_j).,A thread requesting R_j must ensure F(R_j) is the highest function value among its held resources.,A thread must only request R_j if no other thread is holding any R_i.,B,"The text states: 'Alternatively, thread requesting R_j must have released any R_i such that F(R_i) >= F(R_j).'"
When does lock ordering NOT guarantee deadlock prevention?,When resources are requested in decreasing order of enumeration.,When multiple instances of the same resource type are needed.,"If locks are acquired dynamically (e.g., in a transaction function).",If the `System.identityHashCode(Object)` method is used for ordering.,When resource utilization is low.,C,"The text states: 'Lock ordering does not guarantee deadlock prevention if locks acquired dynamically (e.g., `transaction()` function example).'"
Which of the following is a side effect of deadlock prevention mechanisms?,Increased system throughput,Higher device utilization,Reduced system throughput,Elimination of all resource request errors,Automatic detection of unsafe states,C,"Deadlock prevention limits how requests are made, which can lead to side effects such as low device utilization and reduced system throughput."
What additional information does a deadlock-avoidance algorithm typically require compared to deadlock prevention?,The current CPU utilization of each thread,A priori knowledge of the maximum resources each thread may need,The history of all resource requests made by threads,The priority level of each thread,The total number of threads currently in the system,B,"Deadlock avoidance requires additional information, specifically that the system needs to know the maximum resources each thread may need (a priori information)."
What is the primary goal of a deadlock-avoidance algorithm regarding the resource-allocation state?,To maximize device utilization regardless of deadlock potential,To ensure that all resource requests are immediately granted,To dynamically examine the resource-allocation state to prevent circular-wait,To convert all request edges into assignment edges,To reduce the complexity of resource allocation to O(n) operations,C,"A deadlock-avoidance algorithm dynamically examines the resource-allocation state to prevent circular-wait, which is a necessary condition for deadlock."
How is the resource-allocation state defined in the context of deadlock avoidance?,By the number of active threads and their priorities,By the amount of free memory and disk space,"By the available and allocated resources, and the maximum demands of each thread",By the historical success rate of resource requests,By the total system uptime and number of system calls,C,"The resource-allocation state is defined by the available and allocated resources, and the maximum demands of each thread."
What characterizes a 'safe state' in deadlock avoidance?,A state where no thread is currently holding any resources.,A state where the system can allocate resources to each thread (up to its maximum) in some order and avoid deadlock.,A state where all resource requests are immediately denied to prevent future issues.,A state where the system has detected a cycle in its resource-allocation graph.,A state where no new threads are allowed to enter the system.,B,A state is safe if the system can allocate resources to each thread (up to its maximum) in some order and avoid deadlock.
When is a system considered to be in a 'safe state'?,Only when no threads are currently executing.,If a safe sequence of threads exists.,If the current resource utilization is below 50%.,If no new resource requests have been made in the last minute.,If the system has sufficient memory to run all processes simultaneously.,B,A system is safe if a safe sequence exists.
"What defines a 'safe sequence' $<T_1, T_2, \dots, T_n>$?",Each $T_i$ finishes execution before $T_{i+1}$ begins.,"For each $T_i$, its resource requests can be met by currently available resources plus resources held by all $T_j$ where $j < i$.",All $T_i$ have exactly the same resource needs.,"The sequence is ordered by thread priority, from highest to lowest.",No thread in the sequence ever makes a resource request.,B,"A safe sequence is one where for each $T_i$, its resource requests can be met by currently available resources plus resources held by all $T_j$ where $j < i$."
"Which statement accurately describes the relationship between safe, unsafe, and deadlocked states?",All unsafe states are deadlocks.,A deadlocked state can be a safe state.,"A safe state is not deadlocked, and a deadlocked state is unsafe.",Unsafe states always lead to deadlock.,Only safe states can experience deadlocks.,C,"A safe state is not deadlocked; a deadlocked state is unsafe. While not all unsafe states are deadlocks, unsafe states may lead to deadlock."
"In an unsafe state, who or what controls whether a deadlock occurs?","The operating system, by enforcing strict resource limits.","The system administrator, by manually reallocating resources.","The thread behavior, as the OS cannot prevent deadlocks.",External hardware controllers.,"The resource-allocation-graph algorithm, which automatically resolves cycles.",C,"In an unsafe state, the OS cannot prevent deadlocks; thread behavior controls unsafe states."
The Resource-Allocation-Graph algorithm for deadlock avoidance is applicable to systems with:,Multiple instances of each resource type.,Only one instance of each resource type.,Only two resource types in total.,An unlimited number of resources.,Resources that are never released.,B,The Resource-Allocation-Graph algorithm is specifically for systems with only one instance of each resource type.
What does a 'claim edge' ($T_i 	o R_j$ dashed line) signify in a Resource-Allocation-Graph for deadlock avoidance?,Thread $T_i$ currently holds resource $R_j$.,Resource $R_j$ is currently available to $T_i$.,Thread $T_i$ may request resource $R_j$ in the future.,Resource $R_j$ is permanently assigned to $T_i$.,Thread $T_i$ has released resource $R_j$.,C,A claim edge ($T_i 	o R_j$ dashed line) indicates that $T_i$ may request $R_j$ in the future.
"In the Resource-Allocation-Graph algorithm, when does a claim edge $T_i 	o R_j$ get converted to a request edge?",When $T_i$ releases $R_j$.,When $R_j$ is allocated to $T_i$.,When $T_i$ requests $R_j$.,When $R_j$ becomes available.,When a new thread enters the system.,C,A claim edge $T_i 	o R_j$ is converted to a request edge when $T_i$ requests $R_j$.
"According to the Resource-Allocation-Graph algorithm, when is a resource request granted?","Always, as long as the resource is available.",Only if no other thread is waiting for the same resource.,Only if no cycle is formed in the graph after the hypothetical allocation.,If the requesting thread has the highest priority.,Only if the graph contains no claim edges.,C,"A request is granted only if no cycle is formed in the graph, which is checked by a cycle-detection algorithm."
What does the presence of a cycle in the Resource-Allocation-Graph indicate in the context of deadlock avoidance?,A safe state.,An efficient resource allocation.,"An unsafe state, potentially leading to deadlock.",That all resources are fully utilized.,That a thread has finished its execution.,C,"A cycle indicates an unsafe state, meaning that the system could potentially enter a deadlocked state."
What is the time complexity of the Resource-Allocation-Graph algorithm for deadlock avoidance?,$O(n)$ operations,$O(m 	imes n)$ operations,$O(n^2)$ operations,$O(m^2)$ operations,$O(log n)$ operations,C,"The algorithm complexity for the Resource-Allocation-Graph algorithm is $O(n^2)$ operations, where $n$ is the number of threads."
The Banker's algorithm is specifically designed for systems with:,Only one instance of each resource type.,Multiple instances of each resource type.,A fixed number of threads.,No resource sharing.,Resource types that cannot be claimed a priori.,B,The Banker's algorithm is applicable to systems with multiple instances of each resource type.
How does the efficiency of the Banker's algorithm compare to the Resource-Allocation-Graph scheme?,It is generally more efficient.,It is generally less efficient.,They have comparable efficiency.,Efficiency depends solely on the number of resource types.,Efficiency depends solely on the number of threads.,B,The Banker's algorithm is described as 'Less efficient than resource-allocation graph scheme'.
What information must a new thread declare when it enters a system using the Banker's algorithm?,Its current CPU usage.,Its maximum demand for instances of each resource type.,The total time it expects to run.,Its priority level.,Whether it will ever release resources.,B,"When a new thread enters, it declares the maximum instances of each resource type needed (which cannot exceed total system resources)."
"Under the Banker's algorithm, a resource request is granted only if:",The requested resources are immediately available.,The allocation leaves the system in a safe state.,The requesting thread has the highest priority.,No other thread is currently waiting for any resource.,The system's total resource utilization is below 80%.,B,A request is granted only if allocation leaves the system in a safe state.
"In the Banker's algorithm, what does the `Available` vector represent?",The maximum number of instances of each resource type in the system.,The number of resources of each type currently allocated to threads.,The number of available resources of each type.,The remaining resource need of each thread.,The total number of threads waiting for resources.,C,`Available` is a vector of length $m$ representing the number of available resources of each type.
Which data structure in the Banker's algorithm represents the maximum demand of each thread?,Allocation,Need,Work,Max,Available,D,`Max` is an $n 	imes m$ matrix representing the max demand of each thread.
"In the Banker's algorithm, the `Need` matrix is calculated as:",`Need[i][j] = Available[j] - Allocation[i][j]`,`Need[i][j] = Max[i][j] + Allocation[i][j]`,`Need[i][j] = Max[i][j] - Allocation[i][j]`,`Need[i][j] = Allocation[i][j] - Available[j]`,`Need[i][j] = Work[j] - Max[i][j]`,C,The `Need` matrix represents the remaining resource need of each thread and is calculated as `Need[i][j] = Max[i][j] - Allocation[i][j]`.
What is the first step in the Banker's Safety Algorithm?,Find an index `i` such that `Finish[i] == false`.,Initialize `Work = Available` and `Finish[i] = false` for all threads.,Check if `Request_i <= Need_i`.,Grant the resource request.,Restore the old state of resources.,B,"The first step of the Safety Algorithm is to 'Initialize Work = Available, Finish[i] = false for i = 0, 1, ..., n-1'."
"In the Banker's Safety Algorithm, if no index `i` can be found such that `Finish[i] == false` and `Need_i <= Work`, what happens next?",The system is declared safe.,The algorithm restarts from step 1.,The algorithm proceeds to check if `Finish[i] == true` for all `i`.,A deadlock is immediately declared.,The `Work` vector is reset to `Available`.,C,"If no such `i` is found, the algorithm proceeds to step 4: 'If Finish[i] == true for all i, system is in safe state.'."
What is the time complexity of the Banker's Safety Algorithm?,$O(n)$ operations,$O(m + n)$ operations,$O(n^2)$ operations,$O(m 	imes n^2)$ operations,$O(m^2 	imes n)$ operations,D,The algorithm complexity for the Banker's Safety Algorithm is $O(m 	imes n^2)$ operations.
"When a thread $T_i$ makes a resource request $Request_i$ in the Banker's Resource-request algorithm, what is the first condition checked?",If $Request_i \le Available$,If the resulting state is safe.,If $Request_i \le Need_i$,If $Allocation_i + Request_i \le Max_i$,If $Need_i$ is zero.,C,"The first step is: 'If $Request_i \le Need_i$, go to step 2. Else, error (thread exceeded max claim).'"
"In the Banker's Resource-request algorithm, if $Request_i \le Available$ is false, what happens to thread $T_i$?",$T_i$ is immediately terminated.,The request is granted without further checks.,$T_i$ must wait.,The system enters an unsafe state.,The resource matrices are updated with the request.,C,"If $Request_i \le Available$ is false, it means the resources are currently unavailable, so '$T_i$ must wait'."
What happens during the 'pretend allocation' step in the Banker's Resource-request algorithm?,The system calculates the maximum possible future requests.,"The `Available` resources are increased, and `Allocation_i` is decreased.","The `Available`, `Allocation_i`, and `Need_i` matrices are updated as if the request were granted.",The algorithm checks for cycles in the resource graph.,The system asks for user confirmation to proceed.,C,"Pretend allocation involves updating `Available = Available - Request_i`, `Allocation_i = Allocation_i + Request_i`, and `Need_i = Need_i - Request_i`."
"If the state resulting from a 'pretend allocation' in the Banker's Resource-request algorithm is found to be unsafe, what action is taken?","The request is granted, but a warning is issued.",The system immediately initiates a deadlock recovery process.,"The request is granted, and the system moves to an unsafe state.","$T_i$ waits, and the old state of the resource allocation is restored.",All other threads are temporarily suspended.,D,"If the resulting state is unsafe (using the Safety Algorithm), then '$T_i$ waits, restore old state'."
Under what circumstances is a deadlock detection algorithm typically employed in a system?,When the system uses aggressive deadlock prevention mechanisms.,When the system exclusively relies on deadlock avoidance strategies.,When the system does not implement deadlock-prevention or deadlock-avoidance techniques.,Only in real-time operating systems where deadlocks are strictly forbidden.,During system startup to identify potential resource conflicts.,C,"Deadlock detection algorithms are used in systems that do not employ deadlock-prevention or deadlock-avoidance, requiring an algorithm to determine if deadlock has occurred and another to recover from it."
What are the primary components involved when a system uses a deadlock detection approach?,A resource preemption mechanism and a rollback strategy.,An algorithm to detect deadlock and an algorithm to recover from it.,A method for safe sequence generation and a resource request denial mechanism.,A system for resource ordering and a method for breaking mutual exclusion.,A Banker's algorithm for resource allocation and a wait-for graph.,B,"If a system doesn't use deadlock-prevention or deadlock-avoidance, it needs both an algorithm to determine if a deadlock occurred and an algorithm to recover from it."
What is a significant overhead associated with deadlock detection and recovery?,Increased memory consumption for data structures.,Higher CPU utilization due to frequent context switching.,Run-time costs for detection and potential losses from recovery.,Complexity in implementing resource preemption.,The need for user intervention during recovery.,C,The text states that detection-and-recovery overhead includes run-time costs and potential losses from recovery.
Which graph is used for deadlock detection when there is a single instance of each resource type?,Resource-allocation graph.,Process-resource graph.,Wait-for graph.,Dependency graph.,Allocation matrix.,C,"For single instances of each resource type, the wait-for graph is used, which is a variant of the resource-allocation graph."
How is a wait-for graph obtained from a resource-allocation graph?,By adding resource nodes and new edges.,By removing resource nodes and collapsing edges.,By reversing all existing edges.,By adding a separate node for each available resource instance.,By only including threads that are currently holding resources.,B,A wait-for graph is obtained by removing resource nodes and collapsing edges from the resource-allocation graph.
"In a wait-for graph, what does an edge from thread $T_i$ to thread $T_j$ ($T_i 	o T_j$) signify?",$T_i$ has allocated a resource to $T_j$.,$T_j$ is waiting for a resource held by $T_i$.,$T_i$ is waiting for $T_j$ to release a resource $T_i$ needs.,$T_i$ and $T_j$ are in a safe state.,$T_j$ is a child process of $T_i$.,C,An edge $T_i 	o T_j$ in a wait-for graph implies thread $T_i$ is waiting for thread $T_j$ to release a resource $R_q$ that $T_i$ needs.
"When detecting deadlocks using a wait-for graph, what condition indicates the presence of a deadlock?",The graph is empty.,The graph contains a directed cycle.,All threads are connected to each other.,The graph is a tree structure.,There is a direct edge from every thread to itself.,B,Deadlock exists if the wait-for graph contains a cycle.
"What is the computational complexity of cycle detection in a wait-for graph, where $n$ is the number of vertices?",$O(n)$,$O(n \log n)$,$O(n^2)$,$O(n^3)$,$O(2^n)$,C,"Cycle detection in a wait-for graph has a complexity of $O(n^2)$ operations, where $n$ is the number of vertices."
Which tool from the BCC toolkit is mentioned for detecting deadlocks in Pthreads mutex locks on Linux?,`pthread_analyzer`,`mutex_monitor`,`deadlock_detector`,`resource_tracker`,`lock_debugger`,C,The `deadlock_detector` tool from the BCC toolkit is specifically mentioned for Pthreads mutex locks on Linux.
How does the `deadlock_detector` tool identify deadlocks?,By analyzing system call logs for mutex timeouts.,By simulating thread execution paths.,"By inserting probes to trace `pthread_mutex_lock()` and `pthread_mutex_unlock()` calls, constructing a wait-for graph, and detecting cycles.",By directly inspecting thread stack traces for blocked calls.,By monitoring CPU utilization patterns for anomalies.,C,"The `deadlock_detector` tool inserts probes to trace `pthread_mutex_lock()` and `pthread_mutex_unlock()`, constructs a wait-for graph, and reports deadlock if a cycle is detected."
"When a system has several instances of a resource type, why is the wait-for graph scheme not applicable for deadlock detection?",It becomes too complex to draw.,It only works for a single resource type.,The graph representation cannot effectively model multiple instances of a resource.,Resource nodes cannot be collapsed when multiple instances exist.,The cycle detection algorithm is not efficient for this scenario.,C,"The text states that the wait-for graph scheme is not applicable for several instances of a resource type, implying that its simplified representation is insufficient for the more complex resource allocation involving multiple instances."
"What are the three main data structures used in the deadlock detection algorithm for several instances of a resource type, similar to the Banker's algorithm?","Total, Max, Need.","Available, Allocation, Request.","Work, Finish, Safe.","Resources, Processes, States.","Graph, Cycle, Path.",B,"The algorithm uses `Available` (vector), `Allocation` (n x m matrix), and `Request` (n x m matrix)."
"In the deadlock detection algorithm for multiple resource instances, what does `Request[i][j] = k` signify?",Thread $T_i$ has $k$ instances of resource $R_j$ allocated.,Thread $T_i$ requests $k$ more instances of resource type $R_j$.,There are $k$ total instances of resource $R_j$ available.,Resource $R_j$ is needed by $k$ threads.,Thread $T_j$ requests $k$ instances of resource $R_i$.,B,`Request[i][j] = k` means thread $T_i$ requests $k$ more instances of resource type $R_j$.
What is the first step in the deadlock detection algorithm for several instances of a resource type?,Find an index $i$ such that `Finish[i] == false` and `Request_i <= Work`.,Update `Work` by adding `Allocation_i`.,Initialize `Work` = `Available` and `Finish` array based on `Allocation`.,Check if `Finish[i] == false` for any $i$.,Select a victim thread to terminate.,C,"Step 1 of the algorithm is to Initialize `Work` = `Available` and set `Finish[i]` to `false` if `Allocation_i` is not zero, or `true` otherwise."
"In the deadlock detection algorithm for multiple resource instances, if no index $i$ is found where `Finish[i] == false` and `Request_i <= Work`, what is the next action?",Assume the system is in a safe state.,Go back to Step 1 and reinitialize.,Proceed to the step that checks for deadlocked threads.,Grant the pending resource request to a thread.,Rollback the last transaction.,C,"If no such $i$ exists (in step 2), the algorithm proceeds to step 4, which is where it determines if any threads are deadlocked."
"According to the deadlock detection algorithm for multiple resource instances, how is a deadlocked thread identified?",A thread is deadlocked if its `Request` matrix is non-zero.,A thread is deadlocked if its `Allocation` is greater than `Available`.,A thread $T_i$ is deadlocked if `Finish[i] == false` after the algorithm completes.,A thread $T_i$ is deadlocked if it is found in a cycle in the wait-for graph.,A thread $T_i$ is deadlocked if its resources cannot be reclaimed by other threads.,C,"Step 4 states: 'If `Finish[i] == false` for some $i$, $0 \leq i < n$, then thread $T_i$ is deadlocked.'"
What is the computational complexity of the deadlock detection algorithm for several instances of a resource type?,$O(n+m)$,$O(n 	imes m)$,$O(m 	imes n^2)$,$O(n^2)$,$O(m^2 	imes n)$,C,The algorithm complexity is stated as $O(m 	imes n^2)$ operations.
What 'optimistic attitude' is assumed by the deadlock detection algorithm for multiple resource instances?,That all resource requests will eventually be granted.,"That if a thread's `Request` is less than or equal to `Work`, it will complete and return its resources.",That deadlocks are rare and can be ignored.,That all threads will eventually release their allocated resources.,That resources are infinite and always available.,B,"The algorithm assumes an optimistic attitude: if `Request_i <= Work`, it assumes $T_i$ will complete and return resources."
"Given the initial state in the example for multiple instances (5 threads, 3 resource types, specific allocations and requests), what sequence of threads leads to all `Finish[i]` being true, indicating no deadlock?","$<T_4, T_1, T_3, T_2, T_0>$","$<T_0, T_2, T_3, T_1, T_4>$","$<T_1, T_0, T_4, T_2, T_3>$","$<T_2, T_3, T_0, T_1, T_4>$","$<T_0, T_1, T_2, T_3, T_4>$",B,"The text explicitly states: 'Sequence $<T_0, T_2, T_3, T_1, T_4>$ results in Finish[i] == true for all i', indicating no initial deadlock."
"If thread $T_2$ requests 1 additional instance of resource C in the provided example, which threads become involved in the deadlock?","$T_0, T_1, T_2$","$T_1, T_2, T_3, T_4$","$T_0, T_3, T_4$","$T_0, T_1, T_2, T_3, T_4$",No deadlock occurs.,B,"The text states: 'New claim: system is deadlocked. Can reclaim $T_0$'s resources, but not enough for others. Deadlock involves $T_1, T_2, T_3, T_4$."
What two factors determine when a deadlock detection algorithm should be invoked?,System uptime and available memory.,The complexity of the algorithm and the number of resource types.,How often a deadlock is likely to occur and how many threads will be affected.,The current CPU utilization and disk I/O rate.,The size of the wait-for graph and the number of cycles detected previously.,C,The decision of when to invoke depends on 'How often is a deadlock likely to occur?' and 'How many threads will be affected by deadlock when it happens?'
What is a consequence of not frequently invoking a deadlock detection algorithm when deadlocks are likely to occur?,Increased system throughput.,Reduced computational overhead.,"Resources allocated to deadlocked threads become idle, and the number of threads in the deadlock cycle may grow.",Faster recovery times once a deadlock is eventually detected.,Improved responsiveness for non-deadlocked threads.,C,"If deadlocks are frequent and detection is not, 'Resources allocated to deadlocked threads become idle; number of threads in deadlock cycle may grow.'"
What is the main advantage of invoking a deadlock detection algorithm every time a resource request cannot be granted immediately?,It ensures the lowest possible computational overhead.,It allows for easier selection of a victim thread.,It identifies deadlocked threads and the specific thread that 'caused' the deadlock.,It guarantees that no deadlock will ever occur.,It is simpler to implement than periodic checks.,C,Invoking the algorithm every time a request cannot be granted 'Identifies deadlocked threads and the specific thread that 'caused' the deadlock.'
"What is a disadvantage of invoking a deadlock detection algorithm at defined intervals (e.g., hourly or when CPU utilization drops)?",It has very high computational overhead.,It might not identify the 'causing' thread.,It cannot detect deadlocks involving multiple resource types.,It requires manual intervention to start the detection process.,It prevents resources from becoming idle.,B,The text states that invoking at defined intervals 'May not identify the 'causing' thread.'
How do database systems typically manage deadlocks?,By preventing them entirely through careful transaction design.,By avoiding them using a strict ordering protocol.,By using deadlock detection and recovery mechanisms.,By preempting transactions that hold too many locks.,By limiting the number of concurrent transactions.,C,Database systems manage deadlock using detection and recovery.
"In database systems, what happens after a deadlock is detected?",All transactions involved in the deadlock are immediately terminated.,The system pauses until manual intervention resolves the deadlock.,"A victim transaction is selected, aborted, rolled back, and then reissued.",All locks held by deadlocked transactions are automatically released without rollback.,The database is put into a read-only state until the deadlock clears.,C,"When a deadlock is detected: a victim transaction is selected, aborted and rolled back (releasing its locks), remaining transactions are freed, and the aborted transaction is reissued."
"In the context of database deadlocks, what is a common criterion for selecting a victim transaction?",The transaction that has been running the longest.,The transaction that holds the most locks.,"The transaction that has modified the fewest rows (e.g., in MySQL).",The transaction with the highest priority.,The transaction that was started most recently.,C,"For victim choice, the text provides an example: 'e.g., MySQL minimizes rows inserted, updated, or deleted.'"
What is the definition of a 'wait-for graph' in the context of deadlock detection?,A graph showing which resources are currently available.,A graph illustrating the flow of data between threads.,"A variant of the resource-allocation graph with resource nodes removed, indicating a deadlock if it contains a cycle.","A graph used exclusively for deadlock avoidance, not detection.",A graph representing the historical usage of resources by threads.,C,"The glossary defines 'wait-for graph' as 'In deadlock detection, a variant of the resource-allocation graph with resource nodes removed; indicates a deadlock if the graph contains a cycle.'"
"What is a 'thread dump' and for what purpose is it useful, especially in Java?",A utility to delete inactive threads from memory.,A method to reorder thread execution for performance optimization.,"A snapshot of the state of all threads in an application, useful for debugging deadlocks.",A process of transferring threads between different CPU cores.,A tool for automatically resolving deadlocks without user intervention.,C,"The glossary defines 'thread dump' as 'In Java, a snapshot of the state of all threads in an application; a useful debugging tool for deadlocks.'"
"When a deadlock is detected in a system, what are the two primary high-level options for recovery?",Increase system resources or perform a system reboot.,Inform the operator for manual intervention or allow the system to recover automatically.,Implement a resource ordering scheme or decrease the number of active processes.,Initiate a full system backup or restart all services.,Isolate the deadlocked threads or implement a timeout mechanism.,B,"The text states that when deadlock is detected, the options are to 'Inform operator (manual recovery)' or for the 'System recovers automatically.'"
Which two specific methods are identified for actively breaking a detected deadlock?,Implementing a banker's algorithm and resource allocation graph.,Increasing process priorities and allowing threads to self-terminate.,"Aborting one or more threads, or preempting resources from deadlocked threads.",Rolling back all transactions and forcing a context switch.,Reducing the number of available resources and blocking new process creation.,C,The text explicitly lists 'Abort one or more threads (break circular wait)' and 'Preempt resources from deadlocked threads' as the two options for breaking deadlock.
"When a process or thread is aborted as a method to eliminate a deadlock, what happens to the resources it was holding?",They remain allocated to the aborted process for a grace period.,They are immediately reallocated to other waiting processes.,The system reclaims all resources held by the aborted entity.,They are put into a pending state until manual intervention.,They are transferred to a quarantine area to prevent further issues.,C,"The text states, 'System reclaims all resources' when a process/thread is aborted."
What is a significant drawback of choosing to 'abort all deadlocked processes' as a deadlock recovery method?,It may not break the deadlock cycle immediately.,It leads to low overhead as all processes are treated uniformly.,It is expensive due to discarded computations and the need for recomputation.,"It primarily affects only the resources, not the processes themselves.",It requires continuous deadlock detection after each abort.,C,"The text states that 'Abort all deadlocked processes' is 'expensive (discarded computations, recomputation needed).'"
Why does the method of 'abort one process at a time until deadlock eliminated' have high overhead?,It requires complex resource preemption strategies.,It involves discarding all system-wide computations.,It necessitates deadlock-detection after each individual abort.,It often results in file and shared data integrity issues.,It can lead to system-wide starvation of processes.,C,The text indicates that 'Abort one process at a time until deadlock eliminated' has 'High overhead (deadlock-detection after each abort).'
What issue can arise if a process that is currently updating a file is aborted during deadlock recovery?,The file will be permanently deleted from the system.,The file may remain in an incorrect or inconsistent state.,The system will automatically create a backup of the file.,The file's access permissions will be corrupted.,"The file will be locked indefinitely, preventing any further access.",B,The text lists 'File in incorrect state if updating' as an issue when aborting processes.
"If a process updating shared data while holding a mutex lock is aborted during deadlock recovery, what is a primary concern?",The mutex lock will be permanently lost.,Other processes will immediately gain access to the data.,"Shared data integrity issues may occur, and the lock status must be restored.",The system will automatically reinitialize all shared data.,It will trigger a complete system reboot.,C,The text notes 'Shared data integrity issues if updating while holding mutex lock (must restore lock status)' as a concern.
"In the context of partial termination for deadlock recovery, how is the specific process to be terminated typically determined?",Randomly selected by the system.,The process that has been running the longest.,"Based on a policy decision, often economic in nature.",The process that is holding the fewest resources.,The process with the lowest CPU utilization.,C,"The text states that determining which process to terminate is a 'policy decision, economic.'"
"Which of the following is explicitly listed as a factor for choosing a victim process to terminate during deadlock recovery, aiming to minimize cost?",The process's network bandwidth usage.,The process's input/output operations per second.,The process's priority.,The process's total memory footprint.,The process's average response time.,C,'Process priority' is explicitly listed as a factor for choosing a victim to minimize cost.
"When selecting a victim process to terminate, what aspect of 'computation time' is considered to minimize cost?",Only the future projected computation time.,Only the past historical average computation time.,How long the process has computed and how much longer it needs.,The peak computation time achieved by the process.,The time spent waiting for I/O operations.,C,"The text specifies 'Computation time (how long computed, how much longer)' as a factor for choosing a victim."
"Which characteristic of 'Resources used' is relevant when choosing a victim for deadlock termination, aiming for minimum cost?",The absolute monetary value of the resources.,The types of resources and their ease of preemption.,Whether the resources are shared or exclusive.,The total number of unique resources held.,The age of the resources allocated.,B,"The text lists 'Resources used (types, ease of preemption)' as a factor for choosing a victim."
"In the context of deadlock recovery, what does 'resource preemption' primarily involve?",Preventing processes from acquiring new resources.,Allowing processes to voluntarily release resources.,Successively taking resources from processes and giving them to others until the deadlock is broken.,Limiting the total number of resources available in the system.,Reordering resource requests to avoid circular wait.,C,"The text defines resource preemption as 'Successively preempt resources from processes, give to others until deadlock broken.'"
"When performing resource preemption, what is the primary objective when 'selecting a victim' process or resource?",To ensure the highest priority process is always selected.,"To minimize the cost, such as resources held or time consumed.",To select the process that has used the least amount of CPU time.,To always preempt resources that are easy to reclaim.,To avoid selecting any process that is performing I/O.,B,"The text states, 'Selecting a victim: Which resources/processes to preempt? Minimize cost (e.g., resources held, time consumed).'"
"If a process has its resources preempted during deadlock recovery, what is the immediate consequence regarding its execution?",It can continue normal execution but with reduced performance.,It must roll back to a safe state and restart its operation.,It is immediately terminated without any further action.,It waits indefinitely for the preempted resources to become available.,It enters a recovery mode where it can request new resources.,B,"The text states that a preempted process 'Cannot continue normal execution (missing resource)' and must 'Roll back to safe state, restart.'"
What is considered the simplest method for rollback when a process has its resources preempted?,Partial rollback to the last checkpoint.,Selective restoration of only critical resources.,"Total rollback, which involves aborting and restarting the process.",Waiting for the system to automatically restore the process state.,Initiating a recovery mode boot.,C,"The text describes 'Simplest: total rollback (abort, restart).'"
"For a more effective rollback strategy in resource preemption, what is generally required from the system?",Less system state information.,The ability to roll back only as necessary.,A simpler recovery mode.,Avoiding any form of process restart.,Preempting all resources from all processes.,B,"The text states, 'More effective: roll back only as necessary (requires more state info).'"
What is a common solution to prevent starvation when continuously preempting resources from processes during deadlock recovery?,Randomly selecting processes for preemption.,Setting a maximum limit on the number of resources a process can hold.,Including the number of rollbacks in the cost factor for victim selection.,"Implementing a first-come, first-served resource allocation policy.",Prioritizing processes that have never been preempted before.,C,"The text suggests, 'Common solution: include number of rollbacks in cost factor' to ensure a process is picked as a victim a finite number of times."
"According to the provided glossary, what is the primary purpose of 'recovery mode'?",To provide full system services for normal operation.,To allow users to create new system configurations.,To enable system administrators to repair system problems and debug system startup.,To automatically restore data from the last backup.,To monitor system performance under heavy load.,C,The glossary defines 'recovery mode' as 'A system boot state providing limited services and designed to enable the system admin to repair system problems and debug system startup.'
"In the context of operating systems, what is the definition of a deadlock?",A state where a process continuously acquires and releases resources without making progress.,A set of processes where each process is waiting for an event that can only be caused by another process within that same set.,An error state where a process terminates unexpectedly due to a resource conflict.,"A condition where a single process holds all available resources, preventing others from running.",A mechanism used by the operating system to prioritize important processes over less critical ones.,B,"The text defines a deadlock as a 'set of processes, each waiting for event caused by another process in set.'"
Which of the following is NOT listed as one of the four necessary conditions for a deadlock to occur?,Mutual exclusion,Hold and wait,Resource sharing,No preemption,Circular wait,C,"The four necessary conditions for deadlock are Mutual exclusion, Hold and wait, No preemption, and Circular wait. Resource sharing is not one of them."
"For a deadlock to be possible, which of the following statements must be true regarding the four necessary conditions?",At least one of the conditions must be present for a deadlock to initiate.,Only two of the four conditions are strictly required for a deadlock to form.,All four conditions must be present simultaneously for a deadlock to be possible.,"The conditions are only relevant for deadlock prevention, not for the occurrence itself.",The presence of any single condition is enough to guarantee a deadlock.,C,"The text explicitly states, 'Deadlock only possible if all four conditions present.'"
"How are deadlocks typically modeled, and what characteristic within this model indicates the presence of a deadlock?",Using process flowcharts; a linear progression without branches.,Using resource-allocation graphs; the presence of a cycle.,Using system call tables; a high number of repeated system calls.,Using memory usage diagrams; a significant spike in memory consumption.,Using CPU scheduling algorithms; a consistently low CPU utilization rate.,B,"The text states, 'Deadlocks modeled with resource-allocation graphs; cycle indicates deadlock.'"
"Which necessary condition for deadlock implies that at least one resource must be held in a non-sharable mode, meaning only one process at a time can use it?",Hold and wait,No preemption,Circular wait,Mutual exclusion,Resource availability,D,"Mutual exclusion refers to resources that cannot be shared simultaneously, meaning only one process can use them at a time."
A process currently holds a file lock and is waiting to acquire access to a database record that is currently held by another process. This scenario exemplifies which necessary condition for deadlock?,Circular wait,No preemption,Mutual exclusion,Hold and wait,Resource preemption,D,Hold and wait means a process holds at least one resource and is waiting to acquire additional resources held by other processes.
Which necessary condition for deadlock dictates that resources cannot be forcibly taken from a process; they must be released voluntarily by the process holding them upon completion?,Mutual exclusion,Hold and wait,No preemption,Circular wait,Resource sharing,C,No preemption means resources cannot be forcibly taken from a process; they must be released voluntarily.
"In a system, Process P1 is waiting for a resource held by P2, P2 is waiting for a resource held by P3, and P3 is waiting for a resource held by P1. This chain represents which necessary condition for deadlock?",Mutual exclusion,Hold and wait,No preemption,Resource preemption,Circular wait,E,"Circular wait describes a chain of processes, each waiting for a resource held by the next process in the chain, forming a closed loop."
What is the fundamental approach of deadlock prevention?,To detect deadlocks once they occur and initiate a recovery procedure.,To allow deadlocks to happen but ensure a rapid and efficient recovery process.,To ensure that at least one of the four necessary conditions for deadlock cannot occur.,To always grant resource requests as long as resources are physically available.,To run a detection algorithm periodically to find and log deadlocked states.,C,Deadlock prevention strategies aim to 'ensure one of four conditions cannot occur.'
Which of the four necessary conditions for deadlock is explicitly mentioned as the most practical to eliminate in a deadlock prevention strategy?,Mutual exclusion,Hold and wait,No preemption,Circular wait,Resource preemption,D,The text specifies 'Practical prevention: eliminate circular wait.'
Which algorithm is specifically mentioned as being used for deadlock avoidance?,"First-Come, First-Served (FCFS) algorithm",Round Robin (RR) algorithm,Banker's algorithm,Priority Scheduling algorithm,Least Recently Used (LRU) algorithm,C,Deadlock avoidance is implemented using the 'banker's algorithm'.
The core principle guiding resource allocation in deadlock avoidance is to:,Allow processes to hold resources indefinitely until explicitly released.,Grant resource requests only if doing so will not lead the system into an unsafe state.,Preempt resources from any process that has been waiting for a resource for a prolonged period.,Immediately terminate any process identified as being part of a circular wait.,Always grant resource requests to maximize system throughput and resource utilization.,B,Deadlock avoidance involves not granting resources if doing so 'leads to unsafe state'.
What is the primary function of a deadlock detection algorithm?,To prevent any of the four necessary deadlock conditions from occurring in the first place.,To dynamically allocate resources in a way that avoids entering an unsafe state.,To evaluate processes and resources on a running system to identify if a deadlocked state currently exists.,To recover from a deadlock by automatically aborting all involved processes.,To ensure mutual exclusion for all critical resources in the system.,C,Deadlock detection involves an 'algorithm evaluates processes/resources on running system to find deadlocked state'.
Which of the following is a direct method for deadlock recovery?,Implementing a strict preemption policy for all resources.,Continuously checking for safe states using the Banker's algorithm.,Aborting one process that is part of the circular wait condition.,Allowing processes to complete their execution regardless of their state.,Increasing the number of available resources dynamically.,C,One recovery method listed is 'Abort one process in circular wait'.
"In addition to aborting processes, what is another method of deadlock recovery mentioned?","Granting all pending resource requests immediately, even if it leads to an unsafe state.",Disabling mutual exclusion for all resources system-wide.,Preempting resources that have been assigned to a deadlocked process.,Restarting the entire operating system to clear all resource allocations.,Delaying all new process creations until the system becomes idle.,C,The other recovery method listed is 'Preempt resources assigned to deadlocked process'.
Which of the following is true about modern computer systems regarding memory management?,They can only maintain one process in memory at a time.,Memory-management schemes are universal and do not vary.,Most memory management algorithms require specific software support only.,They maintain several processes concurrently in memory.,Memory management is solely handled by the user applications.,D,Modern systems are designed to maintain several processes in memory concurrently to improve CPU utilization and response speed.
What improves CPU utilization and response speed in a modern operating system?,Reducing the number of processes in memory.,Strictly limiting access to main memory.,"CPU Scheduling, which requires keeping many processes in memory.",Disabling hardware support for memory management.,Using only static linking for all programs.,C,"CPU Scheduling improves CPU utilization and response speed, and this requires keeping many processes in memory, necessitating memory sharing."
Which two general-purpose storage locations can the CPU access directly?,Hard disk and solid-state drive.,Main memory and registers.,Cache and external USB drives.,Network storage and CD-ROM.,Optical drives and magnetic tapes.,B,The CPU can only access instructions and data directly from main memory and registers.
How do access times for registers and main memory compare?,"Main memory is accessible within one CPU clock cycle, while registers take many cycles.",Registers and main memory have identical access times.,"Registers are accessible within one CPU clock cycle, while main memory may take many CPU cycles.",Access to main memory is faster than registers due to the memory bus.,Registers are slower than main memory because they are on the CPU chip.,C,"Registers are much faster, accessible within one CPU clock cycle, while main memory access via the memory bus can take many CPU cycles."
What is a 'stall' in the context of CPU operation?,A condition where the CPU runs at maximum speed.,"A state where the CPU is waiting for data from main memory, delaying execution.",An error that causes the CPU to halt permanently.,A process of transferring data between the CPU and registers.,A temporary increase in CPU clock speed to enhance performance.,B,"A 'stall' occurs when the CPU is forced to wait for data, typically from main memory, which delays its execution."
"What is the primary remedy for a CPU stall, and how is it managed?",Increasing the CPU clock speed; managed by the operating system.,Adding more main memory; managed by user programs.,Adding a fast memory called 'cache' between the CPU and main memory; managed automatically by hardware.,Implementing dynamic loading; managed by the application.,Using a faster memory bus; managed by the OS through privileged instructions.,C,"The remedy for a CPU stall is to add a fast memory called 'cache' between the CPU and main memory, and cache management is typically handled automatically by hardware with no OS control."
Why is memory protection necessary in a modern operating system?,To prevent unauthorized access to the CPU's registers.,To ensure that user processes can modify the operating system's code directly.,To protect the operating system from user processes and user processes from each other.,To allow all processes to share the same memory space without any restrictions.,To slow down memory access for security reasons.,C,"Memory protection is crucial for correct operation, protecting the OS from accidental or deliberate modification by user processes, and protecting user processes from each other."
How is memory protection typically implemented at the hardware level?,Through software-only checks by the operating system.,By assigning unique identifiers to each byte of memory.,Using a base register to store the smallest legal physical address and a limit register to store the size of the range.,By encrypting all memory contents to prevent unauthorized access.,By physically separating memory modules for different processes.,C,Memory protection is implemented by hardware using a base register to define the smallest legal physical memory address and a limit register to define the size of the legal address range.
What is the function of a base register in memory protection?,It defines the maximum legal physical memory address.,It stores the size of the memory region allocated to a process.,"It holds the starting address of an address space, representing the smallest legal physical memory address for a process.",It's used to store the last accessed memory address.,It manages the cache memory.,C,"The base register is a CPU register that holds the starting address of an address space, defining the smallest legal physical memory address for a process."
"What happens if a CPU, in user mode, attempts to access a memory address outside the range defined by its base and limit registers?",The access is silently redirected to a safe memory area.,"The operation completes successfully, but a warning is logged.",The CPU automatically adjusts the base and limit registers.,"A trap is generated to the operating system, indicating a fatal error.",The process is paused until the user manually corrects the address.,D,"An attempt to access memory outside the legal range defined by the base and limit registers results in a trap to the operating system, which treats it as a fatal error."
"Who is responsible for loading the base and limit registers, and what kind of instruction is used?",Any user process using a standard instruction.,The CPU hardware automatically during boot-up.,The operating system using a privileged instruction while in kernel mode.,The compiler during program compilation.,The linker/loader during program loading.,C,"The base and limit registers can only be loaded by the operating system using a privileged instruction while in kernel mode, ensuring proper memory protection."
Which of the following describes the sequence of address binding in a user program before execution?,Relocatable to symbolic to absolute.,Absolute to relocatable to symbolic.,Symbolic to absolute to relocatable.,Symbolic to relocatable to absolute.,Physical to logical to virtual.,D,"Addresses are initially symbolic in the source program, then the compiler binds them to relocatable addresses, and finally, the linker/loader binds relocatable addresses to absolute addresses."
What is 'binding' in the context of address management?,The process of compiling source code into an executable.,"The act of tying together different address spaces, such as symbolic to relocatable or relocatable to absolute.",The encryption of memory addresses for security.,The allocation of physical memory to a process.,The operation of moving data between cache and main memory.,B,"Binding refers to the process of tying together or mapping addresses from one address space to another, for example, from symbolic to relocatable or relocatable to absolute."
"Under what condition is 'absolute code' generated, and what type of address binding does it represent?",When the process can move during execution; execution-time binding.,When the process location is unknown at compile time; load-time binding.,When the process location is known at compile time; compile-time binding.,When dynamic loading is used; run-time binding.,When a relocation register is used; logical address binding.,C,"Absolute code is generated at compile time if the process's location in memory is known beforehand, representing compile-time binding."
When is 'relocatable code' typically generated?,Only when dynamic linking is employed.,If the process location in memory is unknown at compile time.,If the process location is fixed at compile time.,When the program is too large to fit in physical memory.,During the execution-time binding process.,B,Relocatable code is generated by the compiler if the process's location in main memory is not known until load time.
What is 'execution-time binding' used for?,When the program's memory location is fixed at compile time.,When the program's memory location is fixed at load time.,When the process can be moved in memory during its execution.,When static linking is preferred over dynamic linking.,When the program uses only registers for data storage.,C,"Execution-time binding is employed when the process can be moved around in memory during its execution, delaying the binding until run time."
What is the key difference between a logical address and a physical address?,"A logical address is seen by the memory unit, while a physical address is generated by the CPU.","A logical address is specific to the hardware, while a physical address is software-defined.",A logical address is generated by the CPU and translated before use; a physical address is the actual location in memory.,Logical and physical addresses are always identical in modern systems.,"A logical address refers to disk storage, while a physical address refers to main memory.",C,"A logical address is an address generated by the CPU, which is then translated by hardware to a physical address, which is the actual location in main memory."
"In the context of address binding, when do logical and physical addresses differ?","Always, as a fundamental principle of memory management.",Only when compile-time binding is used.,When execution-time binding is employed.,Only when load-time binding is used.,They never differ; they are just different names for the same thing.,C,"Logical and physical addresses differ when execution-time binding is used, meaning the process can be moved during execution, requiring run-time translation."
What is a 'virtual address' a synonym for?,Physical address.,Absolute address.,Logical address.,Relocatable address.,Symbolic address.,C,A logical address is also commonly referred to as a virtual address.
What hardware component is responsible for run-time mapping of virtual to physical addresses?,The CPU's arithmetic logic unit (ALU).,The hard disk controller.,The memory-management unit (MMU).,The graphics processing unit (GPU).,The network interface card (NIC).,C,The memory-management unit (MMU) is the hardware component responsible for translating logical (virtual) addresses to physical addresses at run time.
"In a simple MMU scheme, what is added to every address generated by a user process to create the physical address?",The value in the limit register.,The value in the program counter.,The value in the instruction register.,The value in the relocation register.,The base address of the operating system.,D,"In a simple MMU scheme, the value in the relocation register (a generalization of the base register) is added to every logical address generated by the user process to produce the physical address."
"From the perspective of a user program, what type of addresses does it typically deal with?",Only physical addresses directly.,"Real physical addresses, which it generates and accesses.","Logical addresses, with the memory-mapping hardware converting them to physical addresses.","Relocatable addresses, which are then manually converted to physical addresses.","Only symbolic addresses, without any direct interaction with memory locations.",C,A user program typically deals with logical addresses; it never accesses real physical addresses directly. The memory-mapping hardware (MMU) converts these logical addresses to physical ones.
What is 'dynamic loading' and what is its primary advantage?,"Loading the entire program into memory at process start, limiting process size.","Loading program routines only when they are called, improving memory-space utilization.",Loading data from a network drive instead of local storage.,The process of updating program code while it is running.,Loading system libraries at compile time to create a single executable.,B,"Dynamic loading means that a routine is not loaded until it is called. Its advantage is better memory-space utilization because routines are loaded only when needed, especially useful for large programs with infrequently used code."
Does dynamic loading typically require special operating system support?,"Yes, it is entirely managed by the OS.","No, it is primarily the responsibility of the user program.","Only for large programs, otherwise it's user responsibility.","It depends on the type of CPU, not the OS.","It requires special hardware, not OS support.",B,"Dynamic loading generally does not require any special operating system support; it is the responsibility of the user program to implement it, though the OS may provide library routines to help."
What are Dynamically Linked Libraries (DLLs) and when are they linked to user programs?,System libraries combined with user programs by the loader at compile time.,User-defined libraries that are linked manually by the developer.,"System libraries that are linked to user programs at run time, with linking postponed until execution.",Obsolete libraries no longer used in modern systems.,Libraries stored on a network drive and accessed remotely.,C,"Dynamically Linked Libraries (DLLs) are system libraries that are linked to user programs at run time, meaning the linking process is postponed until execution time."
What is a major advantage of using Dynamically Linked Libraries (DLLs) over static linking?,DLLs always result in larger executable file sizes.,DLLs provide better debugging capabilities for individual programs.,"DLLs allow system libraries to be shared among multiple processes, saving main memory.",Static linking offers better performance due to faster loading times.,DLLs make programs less portable across different operating systems.,C,"A significant advantage of DLLs (also known as shared libraries) is that they can be loaded once into memory and shared by multiple processes, which saves main memory space compared to static linking where each program has its own copy."
How do Dynamically Linked Libraries (DLLs) typically handle library updates and incompatible versions?,All programs must be recompiled and relinked manually when a DLL is updated.,"DLLs ignore version information, leading to potential crashes.","Programs use version information to ensure compatibility, and multiple versions of a library can be loaded concurrently.",DLLs are never updated once released to prevent compatibility issues.,Only programs compiled with the old library version can use the updated DLL.,C,"DLLs use version information to prevent incompatibilities. Multiple versions of a library can be loaded, and programs will use the version they were linked with. New versions only affect programs compiled with them, while older programs continue using older versions."
What is 'static linking'?,Linking that occurs only at execution time.,The process where system libraries are loaded dynamically when a routine is called.,A linking method where system libraries are combined by the loader into the binary program image.,A technique to move programs in memory during execution.,A method of memory management that uses a relocation register.,C,"Static linking is a linking method where system libraries are treated like object modules and combined by the loader into the final binary program image, meaning they become part of the executable file."
Does dynamic linking and the use of shared libraries generally require operating system help?,"No, it's entirely managed by the application program.","Yes, especially if processes are protected and need to access shared memory addresses.","Only for single-user systems, not multi-user.",Only if there is no cache memory present.,It depends on the amount of physical memory available.,B,"Dynamic linking and shared libraries generally require OS help, particularly if processes are protected, as the OS needs to check if a routine is already in another process's memory and allow multiple processes to access the same addresses."
"Which term describes a CPU state where it is waiting for data from main memory, causing delays in execution?",Cache hit,Bind,Stall,Trap,Relocation,C,"A 'stall' is defined as a CPU state when the CPU is waiting for data from main memory, which delays execution."
What is a 'cache' used for in a computer system?,To permanently store large datasets for archiving.,As a temporary copy of data in a reserved memory area to improve performance.,To convert logical addresses to physical addresses.,To manage network traffic between devices.,To perform arithmetic operations very quickly.,B,"A 'cache' is a temporary copy of data in a reserved memory area, strategically placed (like between CPU and main memory) to improve performance by reducing access times."
"In contiguous memory allocation, what is the fundamental characteristic of how each process is stored in main memory?",Each process is divided into multiple non-contiguous segments.,"Each process resides in a single, unbroken section of memory.","Processes are stored in fixed-size blocks, regardless of their actual size.","Memory is dynamically allocated to processes as needed, without fixed partitions.",Processes share a single memory partition to maximize utilization.,B,Contiguous memory allocation is defined as a method where each process is in a single contiguous memory section.
Main memory in a system using contiguous memory allocation is typically divided into two partitions. What do these two partitions accommodate?,"One for user data, one for system files.","One for active processes, one for swapped-out processes.","One for the operating system, one for user processes.","One for kernel modules, one for device drivers.","One for read-only memory, one for read-write memory.",C,"The text states: 'Memory usually divided into two partitions: one for OS, one for user processes.'"
"In many modern operating systems, including Linux and Windows, where is the operating system typically accommodated within main memory?",In low memory.,In high memory.,In the middle of user processes.,Dynamically partitioned across all memory.,Swapped in and out as needed.,B,"The text specifies: 'OS can be in low or high memory (many OS, including Linux/Windows, use high memory).'"
What is the primary goal of memory protection in a contiguous memory allocation scheme?,To optimize memory access speed.,To prevent a process from accessing memory that does not belong to it.,To allow dynamic resizing of memory partitions.,To facilitate efficient context switching between processes.,To ensure all memory is fully utilized at all times.,B,Memory protection's purpose is to 'Prevent process from accessing unowned memory.'
Which two registers are combined to implement memory protection in a system using contiguous memory allocation?,Program Counter and Stack Pointer.,Instruction Register and Memory Buffer Register.,Relocation Register and Limit Register.,Base Register and Index Register.,Segment Register and Offset Register.,C,Memory protection is implemented by combining a 'relocation register (smallest physical address) and limit register (range of logical addresses).'
"In the context of memory protection, what information does the 'relocation register' hold?",The largest physical address a process can access.,The range of logical addresses for a process.,The smallest physical address where a process's memory segment begins.,The current size of the operating system.,The address of the next instruction to be executed.,C,The relocation register stores the 'smallest physical address'.
How does the Memory Management Unit (MMU) dynamically map a logical address to a physical address using the relocation-register scheme?,It subtracts the limit register value from the logical address.,It multiplies the logical address by the relocation register value.,It adds the relocation register value to the logical address.,It divides the logical address by the limit register value.,It uses a lookup table to find the corresponding physical address.,C,The MMU maps logical addresses dynamically by 'adding relocation register value'.
When are the relocation and limit registers updated in a system utilizing this memory protection scheme?,Once at system startup and remain fixed.,Periodically by the operating system kernel.,During every CPU instruction fetch.,By the CPU scheduler during each context switch.,Only when a new process is loaded into memory.,D,The 'CPU scheduler loads relocation and limit registers during context switch'.
A significant benefit of the relocation-register scheme is its ability to allow what regarding the operating system?,For the OS to run entirely from read-only memory.,For the OS to be swapped out to disk when not in use.,For the OS size to change dynamically.,For the OS to reside in low memory exclusively.,For the OS to share memory partitions with user processes freely.,C,The 'Relocation-register scheme allows dynamic OS size changes'.
Which of the following is characteristic of the 'variable-partition' memory-allocation scheme?,Memory is divided into fixed-size partitions beforehand.,Each memory partition contains exactly one process.,Processes can occupy multiple non-contiguous partitions.,Only one process can reside in memory at a time.,The size of partitions is determined by the largest process.,B,The definition states: 'Each partition contains exactly one process (variable-partition scheme).'
"In a variable-partition memory allocation scheme, what is a 'hole'?",A section of memory reserved for the operating system.,A contiguous section of unused memory available for allocation.,"A small, fixed-size block of memory allocated to a process.",An area of memory currently being used by a terminated process.,A buffer used for inter-process communication.,B,A 'hole' is defined as 'a contiguous section of unused memory'.
What is the initial state of memory available for user processes in a variable-partition scheme?,It is completely occupied by a dummy process.,"It contains many small, scattered holes.","It consists of a single large block, known as a 'hole'.",It is divided into a predetermined number of fixed-size partitions.,It is entirely allocated to the operating system.,C,"Initially: 'all memory available for user processes, one large block (hole).'"
The challenge of satisfying a memory request of a given size from a list of free holes is an instance of what problem?,The producer-consumer problem.,The synchronization problem.,The dynamic storage-allocation problem.,The critical section problem.,The dining philosophers problem.,C,The text states: 'This procedure: instance of dynamic storage-allocation problem.'
Which memory allocation strategy involves allocating the first hole in the list that is large enough to satisfy the memory request?,Best-fit,Worst-fit,First-fit,Next-fit,Optimal-fit,C,First-fit is defined as: 'Allocate first hole big enough.'
"Which memory allocation strategy searches the entire list of free holes to find the smallest hole that is large enough for the request, often resulting in the smallest leftover hole?",First-fit,Worst-fit,Best-fit,Round-robin,Least-fit,C,Best-fit is defined as: 'Allocate smallest hole big enough. ... Produces smallest leftover hole.'
The 'Worst-fit' memory allocation strategy is characterized by selecting which type of hole for allocation?,The smallest hole that is big enough.,"The first available hole, regardless of size.",The hole that has been free for the longest time.,The largest available hole.,The hole closest to the operating system's memory partition.,D,Worst-fit is defined as: 'Allocate largest hole.'
"According to simulations, which memory allocation strategies are generally considered better than 'worst-fit' in terms of decreasing time and storage utilization?",Only First-fit.,Only Best-fit.,Both First-fit and Best-fit.,Random-fit and Next-fit.,All three strategies perform similarly.,C,"Simulations show: 'first-fit and best-fit better than worst-fit (decreasing time, storage utilization).'"
"Regarding storage utilization, what is the comparative performance between 'first-fit' and 'best-fit' strategies?",Best-fit is always superior for storage utilization.,First-fit is always superior for storage utilization.,"Neither is clearly better for storage utilization, but first-fit is generally faster.",Both strategies result in identical storage utilization.,Storage utilization is not a relevant metric for these strategies.,C,"'Neither first-fit nor best-fit clearly better for storage utilization, but first-fit generally faster.'"
"Which type of memory fragmentation occurs when there is enough total free memory to satisfy a request, but it is broken into many small, non-contiguous pieces?",Internal fragmentation.,Paging fragmentation.,External fragmentation.,Compaction fragmentation.,Process fragmentation.,C,"External fragmentation is defined as: 'enough total memory, but spaces not contiguous (storage fragmented).'"
Both the 'First-fit' and 'Best-fit' memory allocation strategies are known to suffer from which specific problem?,Internal fragmentation.,Excessive context switching.,External fragmentation.,Memory leaks.,Thrashing.,C,'First-fit and best-fit suffer from external fragmentation.'
What is 'internal fragmentation' in the context of memory management?,Memory that is free but cannot be allocated due to being too small.,Memory that is wasted between different processes.,Unused memory located within an allocated memory partition.,Memory that is used by the operating system for its own overhead.,The process of breaking down a large memory block into smaller ones.,C,Internal fragmentation is defined as: 'unused memory internal to a partition.'
Internal fragmentation typically occurs under which condition?,When memory is compacted to combine free space.,When the allocated memory block is slightly larger than the requested memory.,When a process attempts to access memory outside its allocated range.,When the system runs out of free memory altogether.,When processes are frequently swapped in and out of memory.,B,"It 'Occurs when allocated memory slightly larger than requested (e.g., fixed-sized blocks).'"
What is the primary solution proposed to address the problem of external fragmentation?,Increasing the total amount of physical memory.,Using a fixed-partition allocation scheme.,Implementing a compaction process.,Reducing the number of concurrent user processes.,Employing virtual memory techniques like swapping.,C,The 'Solution to external fragmentation: compaction.'
What is the main goal of 'compaction' in memory management?,To reduce the overall memory footprint of the operating system.,"To shuffle memory contents to consolidate all free memory into one large, contiguous block.",To reallocate memory to processes based on their priority.,To convert internal fragmentation into external fragmentation.,To identify and remove corrupted memory blocks.,B,"Compaction's 'Goal: shuffle memory contents, place all free memory together in one large block.'"
Compaction is only possible if what characteristic of memory relocation is present?,If relocation is static (assembly or load time).,If relocation is dynamic (execution time).,If memory is physically non-contiguous.,If the system uses fixed-size memory blocks.,If the operating system runs in low memory.,B,Compaction is 'Possible only if relocation dynamic (execution time).'
"The '50-percent rule' in the context of memory fragmentation, particularly with first-fit, suggests what outcome?",Fifty percent of memory requests will be denied due to lack of space.,Fifty percent of the allocated memory blocks are wasted.,Approximately one-third of memory may become unusable due to fragmentation.,Memory utilization will never exceed 50 percent.,Processes will only ever use 50% of their allocated memory.,C,The '50-percent rule' states: 'one-third of memory unusable.'
"What alternative solution to external fragmentation allows a process's logical address space to be noncontiguous, enabling it to be allocated physical memory wherever available?",Compaction.,The variable-partition scheme.,The best-fit allocation strategy.,Paging.,Fixed-size partitions.,D,Another solution to external fragmentation is to 'permit noncontiguous logical address space. ... Strategy used in paging'.
Which memory-management technique is described as the 'most common' and allows processes to be allocated physical memory in a noncontiguous manner?,Contiguous memory allocation.,Variable-partition scheme.,Compaction.,Paging.,Segmentation.,D,Paging is described as the 'most common memory-management technique' and allows noncontiguous physical memory allocation.
Which of the following best defines 'Paging' in the context of memory management?,A scheme that allocates contiguous physical memory blocks to processes.,A method to compress logical memory into smaller physical spaces.,A memory-management scheme that allows a process's physical address space to be noncontiguous.,A technique primarily used to speed up disk I/O operations.,A system for encrypting memory contents to enhance security.,C,"Paging is a memory-management scheme specifically designed to allow the physical address space of a process to be noncontiguous, which helps in avoiding external fragmentation."
What is one primary advantage of using Paging for memory management?,It eliminates internal fragmentation completely.,It guarantees faster CPU clock speeds.,It avoids external fragmentation and compaction issues.,It simplifies the physical memory layout for contiguous allocation.,It reduces the need for hardware support in memory translation.,C,"Paging is beneficial because it avoids external fragmentation, a common problem in contiguous memory allocation, and thus eliminates the need for compaction."
"In paging, what are the fixed-sized blocks of physical memory called?",Pages,Segments,Clusters,Frames,Blocks,D,"Physical memory is broken into fixed-sized blocks known as frames, while logical memory is broken into same-sized blocks called pages."
What are the two main parts into which a CPU-generated logical address is divided in a paged memory system?,Segment number and offset,Base address and limit register,Page number and page offset,Frame number and frame offset,Process ID and memory address,C,A CPU-generated logical address in a paged system is divided into a page number (p) and a page offset (d).
What is the primary function of the 'page table' in a paged memory system?,To store the contents of each logical page.,To record which processes are currently running.,To map logical page numbers to physical frame numbers.,To manage disk space allocation for backing store.,To store CPU register values for context switching.,C,The page table is a per-process data structure that contains the base address (or frame number) of each frame in physical memory corresponding to a logical page number.
"If a logical address space is $2^m$ bytes and the page size is $2^n$ bytes, how are the page number and page offset determined?",Page number: low-order $n$ bits; Page offset: high-order $m-n$ bits.,Page number: high-order $m-n$ bits; Page offset: low-order $n$ bits.,Page number: $m$ bits; Page offset: $n$ bits.,Page number: low-order $m-n$ bits; Page offset: high-order $n$ bits.,Page number: $n$ bits; Page offset: $m-n$ bits.,B,"For a logical address space of $2^m$ bytes and a page size of $2^n$ bytes, the high-order $m-n$ bits represent the page number, and the low-order $n$ bits represent the page offset."
What type of fragmentation is generally associated with paging?,External fragmentation only,Both external and internal fragmentation,Internal fragmentation only,No fragmentation,Contiguous fragmentation,C,"Paging avoids external fragmentation because any free frame can be allocated. However, it may result in internal fragmentation if the last frame allocated to a process is not completely filled."
What is the average internal fragmentation per process in a paged system?,One full page,No internal fragmentation,One-quarter page,One-half page,Varies significantly with no average,D,"The average internal fragmentation is approximately one-half page per process, as the last page may be partially used."
Which of the following statements about page size is generally true?,Smaller page sizes increase internal fragmentation.,Larger page sizes improve disk I/O efficiency.,Page sizes are typically not powers of 2.,Page sizes have decreased over time.,The optimal page size is always 1 KB.,B,"While smaller page sizes reduce internal fragmentation, larger page sizes reduce overhead per page-table entry and make disk I/O more efficient with larger data transfers."
What is the function of the 'frame table' maintained by the operating system?,It maps logical addresses to physical addresses for all processes.,It stores CPU register values during context switches.,It tracks the allocation status of each physical page frame (free/allocated) and which process/page it belongs to.,It contains the page tables for all active processes.,It caches frequently accessed page table entries.,C,"The frame table is a system-wide data structure with one entry per physical page frame, indicating whether the frame is free or allocated, and if allocated, to which process and page."
"When page tables are kept in main memory, what CPU register is used to point to the base of the current process's page table?",Instruction Pointer (IP),Stack Pointer (SP),Page-Table Base Register (PTBR),Memory Data Register (MDR),Program Counter (PC),C,"When the page table is stored in main memory, the Page-Table Base Register (PTBR) holds the base address of the current page table."
What is the primary drawback of storing page tables in main memory without additional hardware support?,Increased internal fragmentation.,Slower memory access times due to two memory accesses for each data access.,Inability to support shared pages.,Higher CPU utilization for page table management.,Elimination of dynamic relocation.,B,"Storing the page table in main memory means that accessing a data item requires two memory accesses: one to fetch the page-table entry and another to fetch the actual data, effectively doubling memory access time."
What is a 'Translation Look-aside Buffer' (TLB) primarily used for?,Buffering disk I/O operations.,Caching frequently used instructions.,"A small, fast-lookup hardware cache for page table entries.",Managing the queue of processes waiting for CPU time.,Translating assembly code into machine code.,C,"The TLB is a special, small, fast-lookup hardware cache designed to speed up address translation by caching a subset of the page table entries."
What happens during a 'TLB miss' when translating a logical address?,The system immediately generates a memory protection fault.,The CPU fetches the frame number directly from main memory without using the page table.,A memory reference is made to the page table in main memory to obtain the frame number.,The process is immediately swapped out to backing store.,The TLB automatically updates its contents from disk.,C,"If a page number is not found in the TLB (a TLB miss), a memory reference must be made to the page table in main memory to retrieve the corresponding frame number."
What is an 'Address-Space Identifier' (ASID) in a TLB entry used for?,To specify the size of the logical address space.,To uniquely identify a process and allow the TLB to contain entries for multiple processes simultaneously.,To indicate whether a page is valid or invalid.,To determine the priority of a process for TLB access.,To mark a TLB entry as 'wired down'.,B,"An ASID uniquely identifies the process that owns a TLB entry, allowing the TLB to hold translations for multiple processes concurrently without flushing on every context switch if the ASID matches."
"If a TLB does not use ASIDs, what must happen on every context switch?",The TLB must be expanded to accommodate new entries.,The TLB must be 'flushed' (erased) to prevent incorrect translations.,The operating system must rebuild the page table from scratch.,The CPU must halt all memory access until the TLB is manually reloaded.,All pages for the outgoing process must be written to disk.,B,"If a TLB does not store ASIDs, it must be flushed on every context switch to ensure that the next process does not use old, incorrect translation information from the previous process."
What is the 'hit ratio' in the context of a TLB?,The percentage of processes that use the TLB.,The total number of entries a TLB can hold.,The percentage of times a page number is found in the TLB.,The ratio of TLB size to main memory size.,The frequency of TLB flush operations.,C,"The hit ratio is the percentage of times that the requested page number is found in the TLB, indicating the effectiveness of the cache."
"Consider a system with a memory access time of 10 ns. If the TLB hit ratio is 80%, and a TLB miss adds an extra 10 ns (for page table lookup in main memory), what is the effective memory-access time?",10 ns,12 ns,14 ns,18 ns,20 ns,B,"Effective Memory-Access Time = (Hit Ratio * TLB Access Time) + (Miss Ratio * (TLB Access Time + Page Table Access Time)). Given 10 ns for memory access (which is also TLB access on a hit, and page table access on a miss), and a 80% hit ratio: (0.80 * 10 ns) + (0.20 * (10 ns + 10 ns)) = (0.80 * 10 ns) + (0.20 * 20 ns) = 8 ns + 4 ns = 12 ns."
How is memory protection implemented in a paged environment?,By encrypting all memory contents.,By storing protection bits in the CPU's general-purpose registers.,"By assigning protection bits to each frame in the page table, checked on every memory reference.",By requiring user confirmation for every memory write operation.,By physically separating user and kernel memory with hardware walls.,C,"Memory protection in a paged environment is achieved by associating protection bits (e.g., read-write, read-only, execute-only) with each frame entry in the page table, which are checked during the address translation process."
What is the purpose of the 'valid-invalid' bit in a page table entry?,To indicate if the page has been modified since it was loaded.,To mark whether the page is currently in physical memory or on disk.,To specify if the page is part of the process's legal logical address space or not.,To denote if the page is shared among multiple processes.,To control the caching policy for the specific page.,C,The valid-invalid bit indicates whether the corresponding page is a legal page within the process's logical address space. An attempt to access a page marked invalid results in a trap to the OS.
What is 'reentrant code' and why is it important for shared pages?,Code that can be loaded into memory multiple times for performance.,"Code that modifies itself during execution, ensuring uniqueness.","Non-self-modifying code that can be executed simultaneously by multiple processes, allowing memory sharing.",Code that automatically reconfigures its page table entries.,Code specifically designed for single-process execution to avoid conflicts.,C,"Reentrant code is non-self-modifying, meaning it does not change during execution. This property allows multiple processes to execute the same copy of the code simultaneously, making it suitable for sharing in a paged environment."
"In a system with shared pages, what typically happens if 40 processes all need to use the standard C library (libc)?",Each process loads its own full copy of libc into physical memory.,"Only one copy of libc is loaded into physical memory, and all processes' page tables map to it.",The operating system prevents processes from using libc simultaneously.,Libc is converted into a non-reentrant form for security.,Libc is stored exclusively in the TLB for fast access.,B,"If libc is reentrant code, only one copy needs to be loaded into physical memory. The page tables for all user processes can then map their logical pages to this single shared physical copy, resulting in significant memory savings."
What is the purpose of the 'page-table length register' (PTLR)?,To store the number of physical frames available in the system.,To indicate the size of the current process's page table.,To count the number of TLB misses.,To define the maximum allowable page size.,To track the total number of logical addresses generated by a process.,B,"The page-table length register (PTLR) indicates the size of the page table for the current process, which can be checked against a logical address to verify it falls within the valid range."
Which of the following is true regarding the impact of paging on context-switch time?,Paging significantly reduces context-switch time due to simpler memory management.,Paging has no impact on context-switch time.,Paging increases context-switch time because the OS needs to reload the hardware page table and potentially flush the TLB.,Paging only affects context-switch time if page tables are stored in dedicated hardware registers.,Paging decreases context-switch time because processes share page tables.,C,"Paging generally increases context-switch time because the CPU dispatcher must reload the hardware page table (e.g., PTBR) for the new process and, if ASIDs are not used, flush the TLB."
What is 'wired down' in the context of TLB entries?,An entry that is marked for immediate removal from the TLB.,"An entry that cannot be removed from the TLB by the usual replacement algorithms, typically used for frequently accessed kernel code.",An entry that signifies a page fault has occurred and the page needs to be loaded from disk.,An entry that points to an invalid or unallocated physical frame.,An entry that is temporarily disabled until a specific event occurs.,B,"Some TLBs allow entries to be 'wired down', meaning they are locked into the TLB and cannot be removed by standard replacement policies, often used for critical kernel code that needs constant fast access."
What are the three common techniques for structuring the page table mentioned in the text?,"Segmented paging, Indexed paging, Clustered paging","Hierarchical paging, Hashed page tables, Inverted page tables","Single-level paging, Multi-level paging, Forward-mapped paging","Direct-mapped paging, Cache-based paging, Linked-list paging","Virtual paging, Physical paging, Logical paging",B,"The text explicitly states that it ""Explores common techniques for structuring the page table: hierarchical paging, hashed page tables, and inverted page tables."""
What primary problem does the hierarchical paging scheme attempt to solve in modern computer systems?,Reducing the complexity of virtual address translation at the hardware level.,Minimizing the number of Translation Lookaside Buffer (TLB) misses.,Handling excessively large page tables that result from large logical address spaces.,Improving the speed of I/O operations by optimizing page transfers.,Enabling shared memory between multiple processes more efficiently.,C,"The text identifies that ""Modern computer systems support large logical address spaces... Page table itself becomes excessively large,"" and that hierarchical paging is a ""Solution: divide page table into smaller pieces."""
"For a 32-bit logical address space with a 4 KB page size, how many entries would a single-level page table typically have?",2^10 entries,2^12 entries,2^20 entries,2^32 entries,2^4 entries,C,"A 32-bit logical address space and 4 KB ($2^{12}$ bytes) page size implies a 12-bit page offset. The remaining bits for the page number are 32 - 12 = 20 bits. Thus, the page table would have $2^{20}$ entries."
"Considering a 32-bit logical address space with a 4 KB page size, if each page table entry is 4 bytes, how much physical memory could a single process's page table occupy?",4 KB,1 MB,4 MB,16 MB,1 GB,C,"The example calculates that with $2^{20}$ entries and 4 bytes per entry, the page table would consume ""up to 4 MB physical address space for page table alone"" ($2^{20} 	imes 4 	ext{ bytes} = 4,194,304 	ext{ bytes} = 4 	ext{ MB}$). "
"In a two-level paging algorithm for a 32-bit logical address space with a 4 KB page size, how is the 20-bit page number logically divided?",5-bit outer page number ($p_1$) and 15-bit inner page offset ($p_2$),12-bit outer page number ($p_1$) and 8-bit inner page offset ($p_2$),10-bit outer page number ($p_1$) and 10-bit inner page offset ($p_2$),15-bit outer page number ($p_1$) and 5-bit inner page offset ($p_2$),20-bit outer page number ($p_1$) and 0-bit inner page offset ($p_2$),C,"The text specifies: ""Page number further divided: $p_1$: 10-bit outer page number... $p_2$: 10-bit inner page offset."""
What is another term used to describe a hierarchical page table where address translation starts at the outer page table and moves inward?,Backward-mapped page table,Inverted page table,Hashed page table,Forward-mapped page table,Clustered page table,D,"The text explicitly states that this scheme is ""Also known as a forward-mapped page table."" The glossary also confirms this definition."
Why are hierarchical page tables generally considered inappropriate for 64-bit architectures?,They increase the complexity of I/O operations.,"They would require an excessive number of paging levels, leading to prohibitive memory accesses.",They are not compatible with the instruction sets of 64-bit CPUs.,They result in insufficient memory utilization for large address spaces.,They lack the necessary security features for modern 64-bit systems.,B,"The text states that a ""64-bit UltraSPARC: would require seven levels of paging (prohibitive memory accesses)"" and concludes that ""Hierarchical page tables generally inappropriate for 64-bit architectures."""
What is the primary purpose of hashed page tables?,To optimize physical memory allocation for small systems.,To handle address spaces larger than 32 bits.,To eliminate the need for a Translation Lookaside Buffer (TLB).,To provide direct mapping from virtual to physical addresses without intermediate structures.,To reduce the number of entries in the main system page table.,B,"The text states that hashed page tables are an ""Approach for handling address spaces larger than 32 bits."""
"In a hashed page table, what is typically used as the hash value?",The physical page number.,The page offset.,The virtual page number.,The process identifier (PID).,A combination of physical and virtual addresses.,C,"The text explicitly states: ""Hash value: virtual page number."""
How do hashed page tables typically resolve collisions when multiple virtual page numbers map to the same hash table entry?,By discarding the conflicting entry and marking it as an error.,By using a secondary hashing function to re-distribute entries.,By maintaining a linked list of elements at each hash table entry.,By dynamically resizing the hash table when collisions occur.,By triggering a page fault and loading the correct page.,C,"The text specifies that ""Each entry in hash table: linked list of elements (to handle collisions)."""
"What is a distinguishing characteristic of clustered page tables, a variation of hashed page tables for 64-bit address spaces?",Each entry refers to a single virtual page only.,They are exclusively used for mapping kernel memory regions.,"Each entry refers to several pages (e.g., 16), storing mappings for multiple physical-page frames.",They do not utilize a hash function for address lookup.,"They are optimized for dense, contiguous address spaces rather than sparse ones.",C,"The text states that in clustered page tables, ""Each entry refers to several pages (e.g., 16) instead of a single page,"" and they ""Single page-table entry stores mappings for multiple physical-page frames."""
For which type of address spaces are clustered page tables particularly useful?,Address spaces with very few entries.,Dense and contiguous address spaces.,"Sparse address spaces, where memory references are noncontiguous and scattered.",Physical address spaces exclusively.,"Small, fixed-size address spaces.",C,"The text states that clustered page tables are ""Useful for sparse address spaces (memory references noncontiguous, scattered)."" The glossary defines 'sparse' as a page table with noncontiguous, scattered entries or an address space with many holes."
What is the fundamental difference between an inverted page table and a standard page table regarding their entries?,"Standard page tables have one entry per process, while inverted have one entry per virtual page.","Standard page tables have one entry for each virtual address, while inverted page tables have one entry for each real physical page frame.","Inverted page tables are sorted by process ID, while standard page tables are sorted by virtual address.","Inverted page tables store only physical addresses, whereas standard page tables store only virtual addresses.","Standard page tables are managed by hardware, while inverted page tables are managed by software.",B,"The text clarifies: ""Standard page table: one entry for each page process is using (or each virtual address)... Inverted page table: one entry for each real page (frame) of memory."""
What information does each entry in an inverted page table typically contain?,Only the physical page number and permissions.,The virtual address of the page stored in that real memory location and process information.,The logical page number and page offset for all active processes.,A pointer to the next element in a linked list for collision resolution.,The hash value of the corresponding virtual page number.,B,"The text states that ""Each entry: virtual address of page stored in that real memory location, plus process information."""
How many inverted page tables typically exist within a single system?,One per active process.,One per CPU core.,One per physical memory block.,Only one for the entire system.,"Multiple, depending on the number of installed memory modules.",D,"The text specifies, ""Only one page table in system, one entry per physical memory page."""
What is a significant drawback of inverted page tables concerning address lookup performance?,They inherently cause more Translation Lookaside Buffer (TLB) misses.,"They increase the time to search the table because it's sorted by physical address, but lookups are by virtual address.","They are incompatible with multi-level paging schemes, slowing down translation.","They always require two physical memory reads for a single virtual memory reference, even with a hit.","They cannot support dynamic memory allocation, leading to fragmentation.",B,"The text notes, ""Drawback: increases time to search table (sorted by physical address, lookups by virtual address)."""
How is the increased search time drawback of inverted page tables commonly alleviated?,By reducing the page size to decrease table complexity.,By using a hash table to limit the search space.,By increasing the number of physical memory frames.,By storing the entire page table in the Translation Lookaside Buffer (TLB).,By implementing a Least Recently Used (LRU) page replacement policy.,B,"The text states, ""Alleviation: use a hash table to limit search."""
Which of the following describes a key issue with shared memory when using inverted page tables compared to standard paging?,Inverted page tables allow more flexible shared memory by design.,"Standard paging allows only one virtual address per physical page, while inverted allows many.","Inverted page tables allow only one virtual page entry for every physical page, making it difficult for multiple processes to directly share memory without special handling.",Shared memory segments are automatically swapped to disk with inverted page tables.,Inverted page tables do not support any form of shared memory.,C,"The text explains: ""Standard paging: multiple virtual addresses map to same physical address. Inverted page tables: only one virtual page entry for every physical page. One physical page cannot have two (or more) shared virtual addresses. Reference by another process sharing memory: page fault, replaces mapping."""
How does the Oracle SPARC Solaris system primarily solve the virtual memory problem efficiently for its 64-bit architecture?,By implementing a complex seven-level hierarchical page table structure.,"By exclusively using a single, centralized inverted page table.",Through the efficient use of hashed page tables.,"By eliminating the need for any form of page tables, using direct address mapping.",By relying solely on the Translation Lookaside Buffer (TLB) for all translations.,C,"The text states that Solaris ""Solves virtual memory problem efficiently using hashed page tables."""
How many distinct hash tables does Oracle SPARC Solaris typically utilize for virtual to physical memory mapping?,"One for all processes, including the kernel.",Two: one for the kernel and one for all user processes.,One for each active user process.,One per CPU core to minimize contention.,"Three: one for kernel, one for user, and one for shared memory.",B,"The text states, ""Two hash tables: one for kernel, one for all user processes."""
"In Oracle SPARC Solaris, what information does each hash-table entry typically represent for virtual memory mapping?",A single virtual page number mapped to a single physical page frame.,A linked list of all virtual pages that hash to that entry.,"A contiguous area of mapped virtual memory, including a base address and a span (number of pages represented).",A pointer to an entry in an inverted page table.,Only the permissions and dirty bit for a specific page.,C,"The text explains, ""Each hash-table entry: contiguous area of mapped virtual memory (more efficient than per-page entry). Entry has base address and span (number of pages represented)."""
What is the primary function of the TLB (Translation Lookaside Buffer) in the SPARC Solaris virtual memory system?,To store the complete page table in main memory.,To hold Translation Table Entries (TTEs) for fast hardware lookups.,To manage physical memory allocation and deallocation.,To handle page faults by initiating disk I/O.,To provide a software-managed cache for process IDs.,B,"The text states, ""TLB (Translation Lookaside Buffer): holds translation table entries (TTEs) for fast hardware lookups."""
What is the Translation Storage Buffer (TSB) in the SPARC Solaris system?,The primary disk storage area for virtual memory pages.,A hardware component responsible for hashing virtual addresses.,A cache of Translation Table Entries (TTEs) that includes an entry per recently accessed page.,A software routine that manages the page fault handling process.,The physical memory region where the kernel's hash table resides.,C,"The text defines it as a ""Cache of TTEs: translation storage buffer (TSB). TSB includes entry per recently accessed page."""
What specifically describes a 'TLB walk' in the context of virtual memory management?,The process of flushing all entries from the TLB.,The steps involved in traversing page-table structures to locate a needed translation and copying the result into the TLB.,A hardware error that occurs when the TLB cannot find a translation.,A software routine that preloads the TLB with frequently accessed translations.,The movement of a page from physical memory to disk when memory is low.,B,"The glossary defines ""TLB walk"" as ""Steps involved in traversing page-table structures to locate a needed translation and copying the result into the TLB."" The virtual address reference process also outlines this step."
Under what specific condition does the kernel get interrupted during the virtual address reference process in Oracle SPARC Solaris?,When the hardware successfully finds a translation in the TLB.,When a match is found in the Translation Storage Buffer (TSB).,When neither the Translation Lookaside Buffer (TLB) nor the Translation Storage Buffer (TSB) contains the required translation.,When the Memory Management Unit (MMU) successfully completes address translation.,When data is finally retrieved from the main memory location.,C,"The virtual address reference process outlines: ""1. Hardware searches TLB for translation. 2. None found: hardware walks through in-memory TSB... 4. No match in TSB: kernel interrupted to search hash table."""
The term 'sparse' in memory management refers to:,A memory region that is densely packed with frequently accessed data.,"A page table with noncontiguous, scattered entries; an address space with many holes.",A type of physical memory characterized by very fast access times.,A condition where a system has an abundance of available free physical memory.,A process that is designed to use a minimal amount of memory resources.,B,"The glossary defines 'sparse' as ""In memory management, describes a page table with noncontiguous, scattered entries; an address space with many holes."""
What is the primary reason process instructions and data are temporarily 'swapped' out of main memory?,To permanently archive the process data for future reference.,To prevent unauthorized access to sensitive information.,To free up main memory temporarily for other processes or portions.,To reduce the overall CPU load by offloading data processing.,To compress the data before it is loaded back into memory for faster access.,C,"Swapping is a mechanism to temporarily move a process or a portion of it out of main memory to a backing store, making room for other processes to execute, as process instructions and data must be in memory for execution."
Which of the following is a direct consequence of implementing swapping in an operating system?,It strictly limits the total physical address space of all processes to the real physical memory size.,It always decreases the overall degree of multiprogramming.,It allows the total physical address space of all processes to exceed the real physical memory.,It mandates the use of solid-state drives exclusively for backing stores.,It eliminates the need for any form of main memory in the system.,C,"Swapping allows the combined memory requirements of all processes to be larger than the available physical memory, thus enabling more processes to run than would otherwise be possible. It also increases the degree of multiprogramming."
What defines 'standard swapping'?,The movement of only a few select pages of a process between memory and backing store.,The primary method of memory management used by contemporary mobile operating systems.,The process of moving entire processes between main memory and a backing store.,The voluntary relinquishment of memory by applications when system resources are low.,A technique primarily focused on reducing the number of write operations to flash memory.,C,Standard swapping is defined as moving entire processes between main memory and a backing store. This method is generally no longer used in contemporary OS due to the prohibitive time involved.
"In the context of process swapping, what are the characteristics of a 'backing store'?",It is the main memory (RAM) and acts as the primary storage for active processes.,"It is a slow, archival storage area used for long-term data preservation.","It is a fast secondary storage, large enough for process parts, with direct access to memory images.","It is a dedicated cache within the CPU, designed for very rapid access to small data sets.",It refers to volatile memory that loses its contents when power is off.,C,"The backing store is described as a fast secondary storage area, large enough for process parts, and allowing direct access to memory images for efficient swapping operations."
"Why is standard swapping generally no longer used in contemporary operating systems like Linux and Windows, with few exceptions?",The metadata management for swapped-out processes became overly complex.,The time required to move entire processes between memory and backing store is prohibitive.,Backing stores available today are not fast enough to support it efficiently.,"It does not allow for physical memory oversubscription, which is a key modern requirement.","It only supports single-threaded applications, making it unsuitable for modern software.",B,The text states that standard swapping is largely replaced because the time to move entire processes between memory and backing store is prohibitive for efficient operation in modern systems.
"In the context of modern operating systems, what does the term 'paging' specifically refer to?",The standard swapping mechanism that moves entire processes.,The process of terminating applications when memory is low.,"A variation of swapping where only individual pages of a process are moved, not the entire process.",The initial loading of an application's executable code into memory.,The practice of allocating memory manually by application developers.,C,"The text clarifies that 'paging' refers to 'swapping with paging,' where only a subset of pages for processes are moved to/from the backing store, not the entire process, to avoid the high cost of moving whole processes."
What is the action described as 'page out' in the context of swapping with paging?,Moving a page from the backing store to main memory.,Moving a page from main memory to the backing store.,Deleting a page from the system without saving its contents.,Writing an application's current state to flash memory on a mobile device.,Loading an executable file from disk into the main memory.,B,The term 'page out' is specifically defined as the process of moving a page from main memory to the backing store.
Which of the following is NOT a stated reason why mobile systems typically do not support swapping?,"Their use of flash memory for nonvolatile storage, leading to space constraints.",The limited number of writes that flash memory tolerates before becoming unreliable.,Poor throughput between main memory and flash memory.,"Mobile systems are designed with an abundance of physical memory, negating the need for swapping.",The need for applications to voluntarily relinquish memory.,D,"The text explicitly lists flash memory constraints (space, limited writes) and poor throughput as reasons why mobile systems typically do not support swapping. It does not state that mobile systems have an abundance of physical memory; in fact, memory is a constraint that leads to alternative strategies. Option E is an alternative strategy, not a reason for *not* supporting swapping."
"How does Apple's iOS manage memory when free memory is low, as an alternative to traditional swapping?",It automatically swaps all idle processes to a cloud storage service.,It immediately terminates all background applications without warning.,It asks applications to voluntarily relinquish allocated memory.,It dynamically increases the physical memory capacity of the device.,It permanently deletes all read-only data from the system to free up space.,C,"Apple's iOS attempts to manage low memory by asking applications to voluntarily relinquish allocated memory. Read-only data (code) may be removed and reloaded, while modified data is never removed, and applications failing to comply may be terminated."
"What unique action might Android take when terminating a process due to insufficient free memory, which is a distinction from iOS's described approach?",It always swaps the entire process to a dedicated partition on the flash memory.,It prompts the user to manually select processes to terminate.,It writes the application's 'application state' to flash memory for a quick restart.,It compresses all remaining data in main memory to create more space.,It attempts to offload process execution to a connected desktop computer.,C,"While Android also terminates processes for insufficient memory, a distinguishing feature mentioned is that it writes the 'application state' to flash memory before termination, allowing for a quicker restart of the application later."
"According to the glossary, what is 'application state'?","The current execution status (e.g., running, paused) of an application.",A measure of an application's CPU and memory utilization.,"A software construct for data storage, noted for being written to flash memory for quick restart on Android.",The version number and developer information of an application.,The graphical user interface (GUI) layout of an application.,C,"The glossary defines 'application state' as a 'Software construct for data storage,' which the text elaborates is used by Android for quick restarts by writing it to flash memory."
"Excessive swapping, regardless of its form, is often an indicator of what system condition?",The CPU is idle and waiting for more processes to run.,The system has more active processes than available physical memory.,The backing store is operating at peak efficiency.,All applications are optimally managing their memory usage.,The network connection is experiencing high latency.,B,"The text explicitly states that swapping (in any form) is often a sign that there are more active processes than available physical memory, indicating a resource bottleneck."
What are the two common approaches suggested in the text to address system performance issues caused by excessive swapping?,Increasing the size of the backing store and compressing data in memory.,Terminating some processes or acquiring more physical memory.,Migrating to a mobile operating system and disabling virtual memory.,Implementing faster flash memory and optimizing the CPU's cache.,Converting standard swapping to paging and installing a new operating system.,B,The text provides two direct solutions for system performance issues under swapping: 'Terminate some processes' or 'Get more physical memory.'
Which types of processes are generally considered good candidates for standard swapping?,Processes that are constantly performing I/O operations.,Processes that are currently executing critical system functions.,Processes that are idle or mostly idle.,Processes requiring significant CPU-intensive computations.,Processes involved in real-time data streaming.,C,The text states that 'Idle/mostly idle processes [are] good candidates for swapping' because their memory can be temporarily freed and dedicated to active processes.
"When a process or part is swapped to a backing store, what happens to its associated data structures, including per-thread data for multithreaded processes?","They are automatically recreated when the process is swapped back in, so they are not saved.",They remain in main memory while only the core process code is swapped out.,They must be written to the backing store along with the process or its part.,They are permanently deleted to ensure data integrity during swapping.,"They are encrypted and stored in a separate, secure partition of the main memory.",C,"The text specifies that 'When process/part swapped to backing store, associated data structures (including per-thread data for multithreaded processes) must be written.' The OS also maintains metadata for restoration."
Which of the following Intel architectures was primarily 32-bit and included Pentium processors?,x86-64,ARM,IA-32,IA-64,Intel 8086,C,"The text states that '32-bit chips: IA-32, included Pentium processors'."
What is the primary memory management concept used in the Intel IA-32 architecture?,Swapping and Caching,Segmentation and Paging,Relocation and Protection,Virtualization and Emulation,Demand Paging and Thrashing,B,The text explicitly states: 'Memory management in IA-32: segmentation and paging'.
"In the IA-32 architecture, what is the correct sequence of address translation?",Logical address -> Paging unit -> Linear address -> Segmentation unit -> Physical address,Physical address -> Paging unit -> Linear address -> Segmentation unit -> Logical address,Logical address -> Segmentation unit -> Linear address -> Paging unit -> Physical address,Linear address -> Segmentation unit -> Logical address -> Paging unit -> Physical address,Physical address -> Segmentation unit -> Linear address -> Paging unit -> Logical address,C,The text describes the flow: 'CPU generates logical addresses -> segmentation unit. Segmentation unit produces linear address -> paging unit. Paging unit generates physical address in main memory.'
The memory-management unit (MMU) in IA-32 architecture is formed by which two units?,CPU and Main Memory,Cache and Registers,Segmentation and Paging units,I/O Controller and DMA,Arithmetic Logic Unit and Control Unit,C,The text states: 'Segmentation and paging units form memory-management unit (MMU)'.
What is the maximum segment size in IA-32 segmentation?,1 MB,4 KB,1 GB,4 GB,16 KB,D,The text specifies: 'IA-32 segment size: up to 4 GB'.
How many segments can a process have in the IA-32 segmentation architecture?,4 K,8 K,16 K,32 K,64 K,C,The text states: 'Max segments per process: 16 K'.
Which table holds information for segments private to a process in IA-32 segmentation?,Page Directory,Global Descriptor Table (GDT),Local Descriptor Table (LDT),Process Control Block (PCB),Translation Lookaside Buffer (TLB),C,"The text specifies: 'Information for first partition: local descriptor table (LDT)', and this partition is 'private to process'."
Each entry in the Local Descriptor Table (LDT) or Global Descriptor Table (GDT) is an 8-byte ______.,page table entry,segment descriptor,address translation entry,cache line,protection bit,B,The text states: 'Each LDT/GDT entry: 8-byte segment descriptor'.
A logical address in IA-32 segmentation is composed of which two parts?,Base and Limit,Page Number and Offset,Selector and Offset,Segment Register and Index,Linear Address and Physical Address,C,"The text defines a logical address as: '(selector, offset)'."
What is the purpose of the six segment registers in IA-32 segmentation?,To store physical addresses directly,"To act as a cache for segment descriptors, avoiding memory reads",To hold the base and limit of all 16K segments,To control I/O operations,To store the 32-bit linear address,B,The text states: 'Six 8-byte microprogram registers: hold descriptors (LDT/GDT cache). Cache avoids reading descriptor from memory for every reference.'
What is the length of a linear address in the IA-32 architecture?,16 bits,32 bits,48 bits,64 bits,128 bits,B,The text specifies: 'Linear address (IA-32): 32 bits long'.
"In IA-32 segmentation, what occurs if an address validity check fails against the segment limit?",The address is automatically corrected.,The system writes data to an invalid memory location.,A memory fault (trap to OS) is generated.,The operation is retried from a different segment.,The CPU enters a low-power state.,C,The text states: 'Limit checks address validity; invalid -> memory fault (trap to OS)'.
What are the typical page sizes supported by IA-32 paging?,1 KB or 2 MB,2 KB or 4 MB,4 KB or 4 MB,8 KB or 1 GB,16 KB or 2 GB,C,The text indicates: 'IA-32 page size: 4 KB or 4 MB'.
"For 4-KB pages in IA-32, a 32-bit linear address is divided into how many parts and what are their bit lengths?","Two parts: page number (20 bits), offset (12 bits)","Three parts: p1 (10 bits), p2 (10 bits), offset (12 bits)","Three parts: p1 (12 bits), p2 (10 bits), offset (10 bits)","Four parts: p1 (8 bits), p2 (8 bits), p3 (8 bits), offset (8 bits)","Two parts: page directory (16 bits), page table (16 bits)",B,The text details: '32-bit linear address division: Page number p1: 10 bits (high-order). Page number p2: 10 bits (inner). Page offset d: 12 bits (low-order).'
Which register points to the page directory for the current process in IA-32 paging?,EAX,ESP,CR0,CR3,EIP,D,The text specifies: 'CR3 register points to page directory for current process'.
"In IA-32 paging, if the `Page_Size` flag in a page directory entry is set, what is the size of the page frame and what happens to the paging process?","4 KB, and it uses a three-level paging scheme.","4 MB, and it bypasses the inner page table.","2 MB, and it triggers a memory fault.","1 GB, and it requires a TLB flush.","64 KB, and it swaps the page to disk.",B,The text states: 'If Page_Size set: page frame is 4 MB (bypasses inner page table)'.
What is the primary purpose of Page Address Extension (PAE) in IA-32 CPU hardware?,To enable 16-bit processors to run 32-bit software.,To allow 32-bit processors to access physical address space larger than 4GB.,To convert logical addresses directly to physical addresses without segmentation.,To increase the speed of the CPU clock.,To reduce the number of segment registers.,B,The glossary defines PAE as: 'Intel IA-32 CPU hardware allowing 32-bit processors to access physical address space larger than 4GB'.
How did PAE (Page Address Extension) change the paging scheme in IA-32?,It removed the paging hierarchy entirely.,It changed from a two-level to a three-level scheme.,It introduced a four-level hierarchy.,It reduced it to a single-level scheme.,It replaced paging with segmentation.,B,The text states: 'PAE changes paging from two-level to three-level scheme'.
"With PAE, what was the maximum physical memory supported by IA-32 systems?",4 GB,16 GB,32 GB,64 GB,128 GB,D,The text notes: 'PAE increased address space to 36 bits. Supports up to 64 GB physical memory'.
"Which company developed the x86-64 architecture, which extended the existing IA-32 instruction set?",Intel,IBM,ARM,Microsoft,AMD,E,The text states: 'AMD developed x86-64: extended existing IA-32 instruction set'.
What was Intel's initial 64-bit architecture that was not widely adopted?,x86-64,Pentium Pro,Itanium (IA-64),Atom,Core i7,C,"The text says: 'Intel's initial 64-bit architecture: IA-64 (later Itanium), not widely adopted'."
What is the practical virtual address space size used in the x86-64 architecture?,32-bit,48-bit,64-bit,96-bit,128-bit,B,The text specifies: 'x86-64 architecture: 48-bit virtual address'.
How many levels of paging hierarchy does the x86-64 architecture use?,Two,Three,Four,Five,One,C,The text states: 'Uses four levels of paging hierarchy'.
Which of the following page sizes are supported by the x86-64 architecture?,4 KB only,"4 KB, 2 MB, or 1 GB",4 MB or 1 GB only,64 KB or 2 MB,128 KB or 4 MB,B,"The text lists: 'Supports page sizes: 4 KB, 2 MB, or 1 GB'."
What is the primary reason for Intel's lack of dominance in mobile systems?,High power consumption of Intel chips.,Focus on server architectures.,ARM architecture's success in mobile.,Lack of 64-bit support in mobile devices.,Market saturation by competitors.,C,The text states: 'Intel dominance not in mobile systems; ARM architecture successful'.
Which statement accurately describes the primary application areas of ARM processors?,Primarily used in high-performance computing servers.,Mostly found in desktop computers and laptops.,"Common for mobile devices (smartphones, tablets) and real-time embedded systems.",Dominate the market for graphics processing units.,Are less common than Intel processors by quantity produced.,C,"The text states ARM processors are common for mobile devices (smartphones, tablets) and also for real-time embedded systems."
What is a key distinction between ARM and Intel's business models based on the provided text?,"Intel focuses on software development, while ARM focuses on hardware manufacturing.","ARM designs and manufactures chips, while Intel only licenses designs.","Intel designs and manufactures chips, while ARM only designs and licenses architectural designs.",Both companies primarily focus on cloud computing infrastructure.,"ARM specializes in graphics cards, whereas Intel specializes in CPUs.",C,"The text specifies that Intel designs and manufactures chips, while ARM only designs and licenses architectural designs to manufacturers."
"What is the bitness of the ARM v8 architecture, and how many bits are currently used for addressing?","32-bit architecture, 30 bits used.","64-bit architecture, 48 bits used.","64-bit architecture, 64 bits used.","128-bit architecture, 64 bits used.","32-bit architecture, 32 bits used.",B,"The text states 'ARM v8 is 64-bit architecture, but only 48 bits currently used'."
"In ARM v8 CPUs, what do 'translation granules' define?",The number of CPU cores and threads.,The clock speed and cache size.,Page sizes and regions.,Interrupt priority levels.,"The type of memory (e.g., DDR3, DDR4).",C,The glossary defines 'translation granules' as 'Features of ARM v8 CPUs defining page sizes and regions'.
What are 'regions' in the context of ARM v8 CPUs?,Logical divisions within a CPU's pipeline.,Physical memory banks on a motherboard.,Contiguous memory areas with separate privilege and access rules.,Sections of the operating system kernel.,Cache levels within the CPU.,C,"The glossary defines 'regions' as 'In ARM v8 CPUs, contiguous memory areas with separate privilege and access rules'."
"How many translation granules does ARM v8 have, and what are their sizes?",Two: 4 KB and 64 KB.,"Three: 4 KB, 16 KB, and 64 KB.","Four: 1 KB, 4 KB, 16 KB, and 64 KB.","Three: 8 KB, 32 KB, and 128 KB.",One: 4 KB.,B,"The text states 'ARM v8 has three translation granules: 4 KB, 16 KB, and 64 KB'."
"If a 4 KB translation granule is used in ARM v8, what are the corresponding page and region sizes?","Page Size: 16 KB, Region Size: 32 MB","Page Size: 64 KB, Region Size: 512 MB","Page Size: 4 KB, Region Size: 2 MB, 1 GB","Page Size: 4 KB, Region Size: 512 MB","Page Size: 16 KB, Region Size: 1 GB",C,"The table shows that for a 4 KB Translation Granule, the Page Size is 4 KB and the Region Size options are 2 MB, 1 GB."
Which page and region sizes are associated with the 16 KB translation granule in ARM v8?,"Page: 4 KB, Regions: 2 MB, 1 GB","Page: 16 KB, Region: 32 MB","Page: 64 KB, Region: 512 MB","Page: 16 KB, Region: 2 MB","Page: 4 KB, Region: 32 MB",B,"The table indicates that for a 16 KB Translation Granule, the Page Size is 16 KB and the Region Size is 32 MB."
Which page and region sizes are associated with the 64 KB translation granule in ARM v8?,"Page: 4 KB, Regions: 2 MB, 1 GB","Page: 16 KB, Region: 32 MB","Page: 64 KB, Region: 512 MB","Page: 64 KB, Region: 1 GB","Page: 4 KB, Region: 32 MB",C,"The table indicates that for a 64 KB Translation Granule, the Page Size is 64 KB and the Region Size is 512 MB."
What is the maximum number of paging levels supported by 4 KB and 16 KB translation granules in ARM v8?,One level,Two levels,Three levels,Four levels,Five levels,D,The text states: '4-KB and 16-KB granules: up to four levels of paging'.
How many levels of paging does the 64-KB translation granule support in ARM v8?,One level,Two levels,Three levels,Four levels,Five levels,C,The text states: '64-KB granules: up to three levels of paging'.
What is the function of the TTBR (translation table base register) in ARM v8?,It stores the current program counter value.,It points to the level 0 (outer) page table for the current thread.,It holds the address of the next instruction to be executed.,It controls the cache eviction policy.,It manages interrupt requests.,B,"The text defines TTBR as 'translation table base register, points to level 0 table for current thread.' The glossary further clarifies it as 'ARM v8 CPU register pointing to the level 0 (outer) page table for the current thread'."
"When all four levels of paging are used with a 4-KB granule, which bits refer to the offset within the 4-KB page?",Bits 0-7,Bits 0-11,Bits 0-15,Bits 0-20,Bits 0-29,B,The text states: 'If all four levels used (4-KB granule): offset (bits 0-11) refers to offset within 4-KB page'.
"If a Level-1 table entry refers to a 1-GB region, which low-order bits are used as the offset within that region?",Bits 0-11,Bits 0-20,Bits 0-29,Bits 0-31,Bits 0-39,C,The text specifies: 'Level-1 table refers to 1-GB region: low-order 30 bits (0-29) used as offset'.
"When a Level-2 table entry refers to a 2-MB region, how many low-order bits are used as the offset?",12 bits (0-11),21 bits (0-20),30 bits (0-29),32 bits (0-31),48 bits (0-47),B,The text states: 'Level-2 table refers to 2-MB region: low-order 21 bits (0-20) used as offset'.
How many levels of TLBs (Translation Lookaside Buffers) does the ARM architecture support?,One,Two,Three,Four,Five,B,The text states: 'ARM architecture supports two levels of TLBs'.
Which characteristic describes the inner-level TLBs in ARM architecture?,A single main TLB that does not support ASIDs.,"Two micro TLBs, one for data and one for instructions, which support ASIDs.",Three dedicated TLBs for different memory types.,A shared TLB for both data and instructions without ASID support.,A single L3 TLB that acts as a global cache.,B,"The text mentions: 'Inner level: two micro TLBs (one for data, one for instructions); support ASIDs'."
Where does the address translation process primarily begin in ARM architecture?,At the main TLB level.,At the page table walk in hardware.,At the micro-TLB level.,Directly accessing physical memory.,At the CPU's general-purpose registers.,C,The text states: 'Address translation process: Begins at micro-TLB level'.
What happens if a micro-TLB miss occurs during address translation in ARM?,The system immediately performs a page table walk.,The main TLB is checked next.,"An error is generated, halting the process.",The data is fetched directly from main memory.,The request is forwarded to another CPU core.,B,The text states: 'Micro-TLB miss: main TLB checked'.
What is the final step in the ARM address translation process if both micro TLB and main TLB miss?,The system requests a retry from the application.,The memory access is aborted.,A page table walk is performed in hardware.,The data is loaded from the swap file.,The operating system handles the translation in software.,C,The text states: 'Both TLBs miss: page table walk performed in hardware'.
"Which processor architecture is cited as the most widely used by quantity, with over 100 billion units produced?",Intel x86,RISC-V,PowerPC,ARM,SPARC,D,The text explicitly states: 'Over 100 billion ARM processors produced; most widely used architecture by quantity'.
What is the correct definition of a 'micro TLB' in ARM CPUs?,"The main, outer-level TLB.","A small, software-managed cache for page table entries.","Inner-level TLBs, one for instructions and one for data.",A dedicated TLB for I/O operations.,A unified TLB for both instructions and data at the highest level.,C,"The glossary defines 'micro TLB' as 'ARM CPU inner-level TLBs, one for instructions and one for data'."
How is the 'main TLB' described in the ARM architecture?,The first TLB checked during address translation.,An inner-level TLB specifically for instruction fetches.,"The outer-level TLB, checked after micro TLB lookup and before a page table walk.",A software-managed buffer that stores frequently accessed page table entries.,A TLB primarily used for storing kernel page table entries.,C,The glossary defines 'main TLB' as 'ARM CPU outer-level TLB; checked after micro TLB lookup and before page table walk'.
"What is a fundamental characteristic of memory in modern computer systems, as described?","A small, fixed-size cache for CPU data.","A large array of bytes, each with its own address.",A volatile storage unit used only for temporary data.,A non-addressable storage area for system files.,A component primarily used for long-term data archival.,B,"The text states that memory is 'a large array of bytes, each with own address'."
"Which mechanism is explicitly mentioned for address space allocation, involving two specific registers?",Virtual memory mapping using segment tables.,Dynamic memory allocation with heaps and stacks.,Address space allocation using base and limit registers.,Static allocation at compile time.,Paging with a single global page table.,C,The text highlights 'Address space allocation: using base and limit registers'.
"In the context of address space allocation using base and limit registers, what does the 'base register' define?",The largest legal physical memory address.,The total size of the allocated memory segment.,The smallest legal physical memory address.,The number of pages allocated to a process.,The starting address of the program's code segment.,C,The 'Base register' is defined as the 'smallest legal physical memory address'.
What is the primary function of the 'limit' register in address space allocation?,To specify the permission level for memory access.,To define the memory block size for caching.,To specify the total number of available physical frames.,To specify the size of the address range.,To indicate the maximum number of processes allowed in memory.,D,The 'Limit' is defined as specifying the 'size of address range'.
Which of the following is listed as a valid time for binding symbolic address references to physical addresses?,Installation time,Initialization time,Execution time,Shutdown time,Debugging time,C,"The text lists 'Compile time', 'Load time', and 'Execution time' as binding times."
What type of address is generated by the CPU?,Physical address,Hardware address,Logical address,Network address,Cache address,C,The 'Logical address' is explicitly stated as 'generated by CPU'.
What is the primary function of the Memory Management Unit (MMU)?,To allocate CPU time to processes.,To translate logical addresses to physical addresses.,To manage secondary storage devices.,To handle inter-process communication.,To control I/O operations.,B,The MMU is defined as translating 'logical address to physical address'.
"What is a common memory allocation approach mentioned, involving varying sizes?",Segmentation with fixed-size segments.,Paging with equal-sized pages.,Contiguous memory partitions of varying sizes.,Non-contiguous memory blocks.,Demand paging for all memory access.,C,The text specifies 'Memory allocation approach: contiguous memory partitions of varying sizes'.
Which of the following is listed as a partition allocation strategy?,Round Robin,Least Recently Used (LRU),Best fit,Shortest Job First (SJF),Aging,C,"The text lists 'First fit', 'Best fit', and 'Worst fit' as partition allocation strategies."
What memory management technique do modern operating systems primarily use?,Segmentation,Swapping,Contiguous allocation,Paging,Overlays,D,The text states 'Modern OS: use paging to manage memory'.
"In the context of paging, what are the fixed-sized blocks into which physical memory is divided called?",Segments,Pages,Frames,Blocks,Partitions,C,'Physical memory' is 'divided into fixed-sized blocks called frames'.
"In the context of paging, what are the blocks of logical memory, which are the same size as physical memory blocks, called?",Segments,Frames,Sections,Pages,Clusters,D,'Logical memory' is 'divided into blocks of same size called pages'.
How is a logical address structured in a paging system?,It's a direct pointer to a physical memory location.,It's divided into a segment number and an offset.,It's divided into a page number and a page offset.,It's a combination of base and limit register values.,"It's a single, unparsed integer representing the address.",C,Paging: 'logical address divided into page number and page offset'.
What is the purpose of the 'page number' component of a logical address in paging?,It specifies a specific location within a frame.,It serves as an index into a per-process page table.,It indicates the size of the page in memory.,It identifies the process that owns the page.,It points directly to the physical memory address.,B,'Page number: index into per-process page table'.
What essential information does a 'page table' contain?,The logical addresses of all pages.,The size of each page and frame.,The frame in physical memory holding the page.,The CPU's current instruction pointer.,A list of all active processes.,C,'Page table: contains frame in physical memory holding the page'.
"In paging, what does the 'offset' component of a logical address specify?",The starting address of the page table.,The specific location within the page table.,The total number of bytes in the page.,The specific location in the frame.,The distance from the base register.,D,'Offset: specific location in the frame'.
What is the Translation Look-aside Buffer (TLB)?,A software library for memory management.,A hardware cache of the page table.,A component that manages disk I/O operations.,A temporary storage for CPU registers.,A network buffer for data packets.,B,'Translation Look-aside Buffer (TLB): hardware cache of page table'.
What information is typically stored in each entry of a Translation Look-aside Buffer (TLB)?,Process ID and memory size.,Logical address and physical address.,Page number and corresponding frame.,Page size and offset value.,CPU register values and program counter.,C,'Each TLB entry: page number and corresponding frame'.
"During address translation using a TLB, what happens if the frame for a requested page is found in the TLB (a TLB hit)?",The system issues a page fault.,The frame is obtained directly from the TLB.,The page table must still be consulted.,The logical address is immediately used as a physical address.,The request is forwarded to the operating system kernel.,B,"According to the text, 'If in TLB: frame obtained from TLB'."
"If a frame for a given page is NOT found in the TLB during address translation (a TLB miss), where does the system retrieve the necessary information?",From the hard disk swap space.,By generating a new random frame number.,From the CPU registers.,From the page table.,By terminating the process.,D,"According to the text, 'If not in TLB: retrieve from page table'."
What is 'hierarchical paging' as described in the text?,A method of organizing pages on disk.,A technique for encrypting page table entries.,Logical address divided into multiple parts for different page table levels.,A system for prioritizing page access.,A way to back up page tables to secondary storage.,C,'Hierarchical paging: logical address divided into multiple parts for different page table levels'.
What problem is associated with hierarchical paging when logical addresses expand beyond 32 bits?,Increased TLB miss rate.,Decreased system security.,A large number of hierarchical levels.,Reduced CPU clock speed.,Incompatibility with older software.,C,The text states 'Problem with expanding addresses (beyond 32 bits): large number of hierarchical levels'.
Which of the following is a strategy mentioned to address the issue of a large number of hierarchical levels in paging for expanding addresses?,Segmented memory management.,Contiguous memory allocation.,Hashed page tables.,Static linking.,Using smaller page sizes exclusively.,C,Strategies to address this (large number of hierarchical levels): 'hashed page tables' and 'inverted page tables'.
What is the primary purpose of 'swapping' in memory management?,To reorder CPU instructions for faster execution.,To move pages to disk to increase the degree of multiprogramming.,To exchange data between CPU registers.,To ensure data integrity during power outages.,To convert logical addresses into physical addresses.,B,'Swapping: moves pages to disk to increase degree of multiprogramming'.
"How many levels of page tables does the Intel 32-bit architecture typically use, and what page sizes does it support?","One level, 2-KB or 8-KB.","Two levels, 4-KB or 4-MB.","Three levels, 1-KB or 2-KB.","Four levels, 16-KB or 64-KB.","No page tables, only segmentation.",B,Intel 32-bit architecture: 'two levels of page tables; supports 4-KB or 4-MB page sizes'.
What is 'Page-address extension' designed to allow?,32-bit processors to execute 64-bit applications.,32-bit processors to access physical address space > 4 GB.,64-bit processors to emulate 32-bit memory models.,Networking devices to extend their IP address range.,Processors to directly access hard disk sectors.,B,'Page-address extension: allows 32-bit processors to access physical address space > 4 GB'.
Which 64-bit architectures are mentioned as using hierarchical paging?,SPARC and MIPS.,PowerPC and Alpha.,x86-64 and ARM v8.,Z80 and 8086.,RISC-V and Itanium.,C,'x86-64 and ARM v8 architectures: 64-bit architectures using hierarchical paging'.
