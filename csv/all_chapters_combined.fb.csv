Front,Back
What is virtual memory?,A technique allowing execution of a process not completely in memory; it separates logical memory from physical memory.
What is a major advantage of virtual memory?,It allows programs to be larger than the physical memory available.
How does virtual memory abstract main memory?,"It abstracts main memory into a large, uniform storage array."
What is the relationship between logical and physical memory in virtual memory systems?,Virtual memory separates logical memory (the programmer's view) from physical memory.
How does virtual memory benefit programmers?,It frees programmers from memory-storage limitations and simplifies programming by removing worries about physical memory limits.
What sharing capabilities does virtual memory enable for processes?,"It allows processes to share files, libraries, and implement shared memory."
How does virtual memory impact process creation?,"It provides an efficient mechanism for process creation, including sharing pages during process creation (fork())."
What are potential drawbacks or considerations for virtual memory implementation?,Its implementation is complex and can decrease performance if used carelessly.
How did traditional memory management differ from virtual memory regarding process execution?,Traditional memory management required the entire process to be in physical memory for execution.
What was a limitation of traditional memory management regarding program size?,Program size was limited by the physical memory available.
Why do real programs often not need their entire code in memory?,"They may contain error handling code (seldom executed), arrays/lists allocated more memory than needed, or rare program options/features that are rarely used."
What are the benefits of executing only partial programs in memory?,"1. Programs are not constrained by physical memory size, allowing for a large virtual address space.
2. Less physical memory is needed per program, allowing more programs to run concurrently, increasing CPU utilization and throughput (without increasing response/turnaround time).
3. Less I/O is required for loading/swapping, leading to faster program execution."
What is a virtual address space?,It is the logical view of how a process is stored in memory.
How is a process's storage typically viewed in its virtual address space?,It typically starts at logical address 0 with contiguous memory.
How is physical memory organized in relation to logical pages?,"Physical memory is organized in page frames, which are not necessarily contiguous."
What is the function of the Memory-Management Unit (MMU) in virtual memory?,It maps logical pages to physical page frames.
How do the heap and stack grow within a virtual address space?,"The heap grows upward, and the stack grows downward."
What is the purpose of the large blank space typically found between the heap and stack in a virtual address space?,This space is part of the virtual address space but only requires physical pages if the heap or stack grows into it.
"What are ""sparse"" address spaces?","Virtual address spaces with holes; describes a page table with noncontiguous, scattered entries or an address space with many holes."
What are the benefits of sparse address spaces?,"1. Holes can be filled as the stack or heap grows.
2. They facilitate dynamic linking of libraries/shared objects during execution."
How does virtual memory enable sharing of system libraries?,"System libraries (e.g., standard C library) are shared by mapping them into the virtual address space, typically as read-only, with physical pages shared by multiple processes."
How do processes use virtual memory for communication?,Processes can share memory regions for communication.
How does Linux manage virtual memory?,Using demand paging.
How does Linux allocate pages?,From a list of free frames.
What page-replacement policy does Linux use?,A global policy similar to the LRU-approximation clock algorithm (second-chance).
What are the two main page lists maintained by Linux's virtual memory system?,`active_list` and `inactive_list`.
What is the purpose of the `active_list` in Linux's virtual memory management?,It contains pages considered in use.
What is the purpose of the `inactive_list` in Linux's virtual memory management?,"It contains pages not recently referenced, which are eligible for reclamation."
What bit is associated with each page in Linux to track its usage?,An `accessed` bit.
What happens when a page is first allocated in Linux regarding its `accessed` bit and list placement?,"Its `accessed` bit is set, and it's added to the rear of the `active_list`."
What happens when a page already in the `active_list` in Linux is referenced?,"Its `accessed` bit is set, and it moves to the rear of the `active_list`."
How often are the `accessed` bits for pages in the `active_list` reset in Linux?,Periodically.
"In Linux, where is the least recently used page typically found in the `active_list`, and what can happen to it?","At the front of the `active_list`, and it may migrate to the rear of the `inactive_list`."
What happens when a page in the `inactive_list` in Linux is referenced?,It moves back to the rear of the `active_list`.
What is the goal regarding the size of the `active_list` and `inactive_list` in Linux?,They are kept in relative balance.
What happens in Linux if the `active_list` grows larger than the `inactive_list`?,"Pages from the front of the `active_list` move to the `inactive_list`, making them eligible for reclamation."
What is the name of the page-out daemon process in the Linux kernel?,`kswapd`.
What is the primary function of the `kswapd` process in Linux?,It periodically awakens and checks the amount of free memory.
What action does `kswapd` take in Linux if free memory falls below a certain threshold?,It scans the `inactive_list` and reclaims pages for the free list.
What system architectures does Windows 10 support?,"32-bit and 64-bit systems (Intel, ARM)."
What is the default virtual address space and physical memory limit for 32-bit systems in Windows?,"Default 2 GB virtual address space (extendable to 3 GB), 4 GB physical memory."
What is the virtual address space and physical memory limit for 64-bit systems in Windows?,"128-TB virtual address space, up to 24 TB physical memory (Windows Server up to 128 TB)."
List key virtual memory management features implemented in Windows.,"Shared libraries, demand paging, copy-on-write, paging, memory compression."
How does Windows implement demand paging for virtual memory?,With clustering.
Define clustering in the context of Windows virtual memory management.,Paging in a group of contiguous pages when a single page is requested via a page fault.
How does clustering handle page faults in Windows?,By bringing in the faulting page plus several immediately preceding and/or following pages.
What is the cluster size for a data page fault in Windows?,3 pages (the faulting page + one before + one after).
What is the cluster size for other types of page faults in Windows?,7 pages.
"What is a key component of virtual memory management in Windows, related to page allocation for processes?",Working-set management.
What limits are assigned to a process upon creation in Windows regarding its memory usage?,A `working-set minimum` (50 pages) and a `working-set maximum` (345 pages).
Define `working-set minimum` in Windows.,Minimum number of frames guaranteed to a process in memory.
Define `working-set maximum` in Windows.,Maximum number of frames allowed to a process if sufficient memory is available.
Define `hard working-set limit` in Windows.,"Maximum amount of physical memory a process is allowed to use. If configured, working-set minimum and maximum values may be ignored."
Can a process in Windows grow beyond its `working-set maximum`?,"Yes, if memory is available."
Can memory allocated to a process in Windows shrink below its `working-set minimum`?,"Yes, during high memory demand."
What page replacement algorithm does Windows use?,LRU-approximation clock algorithm (second-chance) with local and global policies.
What does the Virtual Memory Manager in Windows maintain to manage page frames?,A free page frames list with a threshold.
What happens in Windows if a page fault occurs for a process that is currently below its `working-set maximum`?,A page is allocated from the free list.
"What happens in Windows if a process is at its `working-set maximum`, experiences a page fault, and there's sufficient free memory?","A free page is allocated, and the process grows beyond its `working-set maximum`."
What happens in Windows if there is insufficient free memory when a page fault occurs?,The kernel selects a page from the process's working set for replacement (using a local LRU policy).
What global replacement tactic does Windows employ if free memory falls below its threshold?,`Automatic working-set trimming`.
Define `automatic working-set trimming` in Windows.,Decreasing working-set frames for processes if the minimum free memory threshold is reached.
How does `automatic working-set trimming` work in Windows?,"It evaluates pages allocated to processes and, if a process has more pages than its `working-set minimum`, removes pages until sufficient memory is available or the process reaches its minimum."
Which processes are targeted first during `automatic working-set trimming` in Windows?,"Larger, idle processes are targeted before smaller, active processes."
How long does `automatic working-set trimming` continue in Windows?,"Until sufficient free memory is achieved, even if processes are trimmed below their `working-set minimum`."
What types of processes does Windows perform trimming on?,User-mode and system processes.
How does Solaris handle a page fault incurred by a thread?,The kernel assigns a page from the free list.
What is a critical imperative for the kernel in Solaris regarding memory?,To keep sufficient free memory.
What is the `lotsfree` parameter in Solaris?,"A threshold that, when free memory falls below it, causes the system to begin paging."
What is the typical value for the `lotsfree` parameter in Solaris?,1/64 of physical memory.
How often does the Solaris kernel check free memory against `lotsfree`?,Four times per second.
What happens in Solaris if the number of free pages falls below `lotsfree`?,The `pageout` process starts.
What mechanism does the `pageout` process in Solaris use for page replacement?,It's similar to a second-chance algorithm and uses two hands.
"Describe the action of the ""front hand"" in the Solaris `pageout` process.",It scans all pages and sets their reference bit to 0.
"Describe the action of the ""back hand"" in the Solaris `pageout` process.","It examines the reference bit; if it's still 0, the page is appended to the free list and written to secondary storage if modified."
"How does Solaris manage ""minor page faults""?",A process can reclaim a page from the free list if it was accessed before being reassigned.
What parameters does the Solaris pageout algorithm use to control its operation?,Parameters to control the `scanrate` (pages per second).
What is the range of `scanrate` in Solaris?,From `slowscan` to `fastscan`.
What is the default `slowscan` rate in Solaris?,100 pages/sec.
What is the `fastscan` rate in Solaris?,"Total physical pages/2, with a maximum of 8,192 pages/sec."
"What determines the `scanrate` in Solaris, particularly how it progresses from `slowscan` to `fastscan`?",The amount of free memory available.
What system parameter in Solaris determines the distance between the two hands of the pageout process?,`handspread`.
What determines the time between clearing a page's reference bit and checking it in Solaris?,The `scanrate` and `handspread` parameters.
How often does the Solaris `pageout` process check the amount of free memory?,Four times per second.
What happens in Solaris if free memory falls below `desfree`?,The `pageout` process runs 100 times per second.
What is the goal of the `desfree` parameter in Solaris?,To keep at least `desfree` amount of memory available.
What action does the Solaris kernel take if it's unable to maintain `desfree` memory for a 30-second average?,"It swaps processes, freeing all their pages."
Which processes does the Solaris kernel prioritize for swapping if it cannot maintain `desfree` memory?,Idle processes.
What happens in Solaris if the system is unable to maintain `minfree` memory?,The `pageout` process is called for every new page request.
"What types of pages does the Solaris page-scanning algorithm specifically skip, even if they are eligible for reclamation?",Shared library pages.
What distinction does Solaris make regarding pages in its virtual memory management?,It distinguishes between pages for processes and regular data files.
"What is the term for Solaris's approach of prioritizing the selection of victim frames based on certain criteria (e.g., avoiding shared library pages)?",`Priority paging`.
Define `priority paging` in Solaris.,"Prioritizing selection of victim frames based on criteria, such as avoiding shared library pages."
Define Virtual Memory.,Abstracts physical memory into an extremely large uniform array of storage.
What is a benefit of virtual memory regarding program size?,A program can be larger than physical memory.
What is a benefit of virtual memory regarding a program's entire presence in memory?,A program does not need to be entirely in memory.
What is a benefit of virtual memory regarding process memory sharing?,Processes can share memory.
What is a benefit of virtual memory regarding process creation?,Processes can be created more efficiently.
What is Demand Paging?,Pages are loaded only when demanded during program execution.
What happens to pages never demanded in a demand paging system?,Pages never demanded are never loaded.
What is a Page Fault?,Occurs when a page not in memory is accessed.
What action is required when a page fault occurs?,The page must be brought from the backing store into an available page frame.
Explain Copy-on-write.,A mechanism where a child process shares the same address space as its parent.
What happens in Copy-on-write if a child or parent modifies a page?,A copy of the page is made.
When is a page-replacement algorithm used?,"When available memory is low, it selects an existing page to replace."
Name common page-replacement algorithms.,"FIFO, optimal, LRU."
Why is pure LRU impractical to implement?,It is impractical to implement; most systems use LRU-approximation algorithms.
Define Global page-replacement algorithms.,Algorithms that select a page from any process for replacement.
Define Local page-replacement algorithms.,Algorithms that select a page from the faulting process for replacement.
What is Thrashing?,A state where the system spends more time paging than executing.
Define Locality in memory management.,A set of pages actively used together.
How does process execution relate to locality?,Process execution moves from locality to locality.
What is a Working Set?,"Based on locality, it is the set of pages currently in use by a process."
What is Memory Compression?,A technique that compresses a number of pages into a single page.
When is memory compression used as an alternative to paging?,On mobile systems without paging support.
How is Kernel memory allocated differently than user-mode processes?,It is allocated in contiguous chunks of varying sizes.
Name two common techniques for kernel memory allocation.,Buddy system and Slab allocation.
What is TLB reach?,The amount of memory accessible from the TLB (Translation Lookaside Buffer).
How is TLB reach calculated?,Equal to the number of entries in TLB multiplied by the page size (Number of entries in TLB × page size).
What is a technique to increase TLB reach?,Increase page size.
"How do Linux, Windows, and Solaris manage virtual memory?","They manage it similarly, using demand paging, copy-on-write, and variations of LRU approximation (clock algorithm)."
What is the traditional approach to program loading at execution time?,Loading the entire program into physical memory.
What is a problem with loading an entire program into memory at execution?,"The entire program may not be needed initially (e.g., unselected options), leading to inefficient memory use."
What is demand paging?,"An alternative program loading method where pages are loaded only as needed, or 'demanded,' during execution."
What happens to unaccessed pages in demand paging?,They are never loaded into physical memory.
What is demand paging similar to?,Paging with swapping.
What is the primary benefit of demand paging?,More efficient memory use by loading only needed portions of a program.
What hardware support is needed to distinguish between pages in memory and those in secondary storage for demand paging?,A valid-invalid bit scheme.
"In the valid-invalid bit scheme, what does a 'valid' bit indicate?",The page is legal and currently in physical memory.
"In the valid-invalid bit scheme, what does an 'invalid' bit indicate?",The page is not valid (not part of the logical address space) or it is valid but currently resides in secondary storage.
How is a page-table entry for a non-memory-resident page marked?,It is marked as invalid.
What event occurs when a process attempts to access a page marked invalid in its page table?,A page fault.
What does a page fault cause?,A trap to the operating system (OS).
What is the first step the OS takes when handling a page fault?,Check an internal table (like the process control block) to determine if the memory access is valid or invalid.
What happens if a page fault indicates an invalid memory access?,The process is terminated.
What happens if a page fault indicates a valid memory access but the page is not in memory?,The OS pages the required page into memory.
What is the second step in handling a page fault after determining the page needs to be brought in?,Find a free frame in physical memory.
What is the third step in handling a page fault after finding a free frame?,Schedule a secondary storage operation to read the desired page into the newly found free frame.
What is the fourth step in handling a page fault after the storage read is complete?,Modify the internal table and the process's page table to indicate that the page is now in memory.
What is the final step in handling a page fault?,"Restart the interrupted instruction, allowing the process to access the page as if it had always been in memory."
What is pure demand paging?,"A demand paging strategy where a process starts with no pages in memory, and page faults occur for every page as it is needed."
What concept contributes to reasonable demand paging performance?,Locality of reference.
What two main hardware components are crucial for demand paging?,"A page table (to mark entries invalid) and secondary memory (swap device, swap space) to hold non-main-memory pages."
What is a crucial requirement for an instruction set architecture to support demand paging effectively?,The ability to restart any instruction after a page fault without issues.
What process state information must be saved on a page fault to allow for instruction restart?,"Registers, condition code, and the instruction counter."
"What is a difficulty encountered when restarting instructions after a page fault, especially for complex instructions?","Instructions that modify multiple locations (e.g., IBM System 360/370 MVC instruction)."
What is one solution for handling instructions that modify multiple locations during a page fault?,"Using microcode to access both ends of memory blocks before modification; if a fault occurs, it happens before any modification."
What is another solution for handling instructions that modify multiple locations during a page fault?,Using temporary registers to hold overwritten values and restoring these old values on a fault before the trap occurs.
How should paging be perceived by the process using it?,It should be transparent to the process.
What is the purpose of the free-frame list?,"It is a pool of free physical memory frames maintained by the OS, used for handling page faults and other allocations."
"Besides page faults, for what other purposes are free frames allocated?",For stack and heap segment expansion.
What is zero-fill-on-demand?,A security practice where frames are 'zeroed-out' (filled with zeros) before being allocated to a process.
What is the state of the free-frame list at system startup?,All available physical memory is placed on the free-frame list.
How does demand paging affect system performance?,"It significantly affects performance, primarily through the page-fault rate."
What is the formula for calculating effective access time for demand-paged memory?,"Effective access time = (1 - p) × memory-access time (ma) + p × page fault time, where p is the probability of a page fault."
What are the three main components of page fault service time?,1. Servicing the page-fault interrupt. 2. Reading in the page from secondary storage. 3. Restarting the process.
Roughly how long do the software components (interrupt service and process restart) of a page fault typically take?,1 to 100 microseconds.
Roughly how long does an HDD page-switch operation typically take?,"~8 milliseconds (composed of ~3ms latency, ~5ms seek, and ~0.05ms transfer)."
What is the general relationship between effective access time and page-fault rate?,Effective access time is directly proportional to the page-fault rate.
"If the memory access time is 200 ns and page-fault service time is 8 ms, how much slowdown occurs with a page-fault rate of 1/1000?","The effective access time becomes 8.2 microseconds, which is a 40x slowdown compared to 200 ns."
Why is a very low page-fault rate crucial for good demand-paging performance?,"Because the time to service a page fault (milliseconds) is orders of magnitude slower than memory access time (nanoseconds), even a small fault rate can drastically increase effective access time."
Why is swap space I/O generally faster than file system I/O?,"Swap space I/O often involves larger blocks and avoids file lookups, making it more efficient."
What is one strategy for using swap space where the entire file image is copied at startup?,"Copying the entire file image to swap space at startup, then demand-paging from swap space. (Disadvantage: initial copy)."
What swap space usage strategy is employed by Linux and Windows for demand paging?,"Demand-paging from the file system initially, and writing pages to swap space only when they are replaced (become dirty)."
How do Linux and BSD UNIX often handle demand paging for binary executables?,"They demand-page them directly from the file system, treating the file system itself as the backing store for these read-only pages. Frames are overwritten when replaced."
"What type of memory still typically uses swap space, regardless of the overall swap strategy?","Anonymous memory (e.g., stack and heap)."
How do mobile operating systems like iOS typically handle memory and swapping?,They usually have no traditional swapping. They demand-page from the file system and reclaim read-only pages if memory is constrained. Anonymous memory pages are not reclaimed unless the app terminates or releases memory.
What is an alternative to swapping used in some mobile systems?,Compressed memory.
demand paging,Bringing in pages from storage as needed rather than entirely at process load time.
page fault,Fault from reference to a non-memory-resident page.
pure demand paging,Demand paging where no page is brought into memory until referenced.
locality of reference,"Tendency of processes to reference memory in patterns, not randomly."
swap space,Secondary storage backing-store space for paged-out memory.
free-frame list,Kernel-maintained list of available free physical memory frames.
zero-fill-on-demand,Writing zeros into a page before making it available to a process.
effective access time,"Measured/calculated time to access something (e.g., memory)."
page-fault rate,Measure of how often a page fault occurs per memory access attempt.
anonymous memory,Memory not associated with a file; stored in swap space if dirty and paged out.
How can `fork()` initially interact with demand paging?,It can bypass demand paging.
What technique is similar to page sharing for rapid process creation?,Copy-on-write.
What does Copy-on-write minimize for the child process?,New pages allocated to the child process.
Describe traditional `fork()` behavior regarding address space.,"Traditionally, `fork()` copied the parent's entire address space for the child."
When is traditional address space copying by `fork()` potentially unnecessary?,If the child immediately calls `exec()`.
What is the core principle of Copy-on-write for process creation?,Parent and child processes initially share the same pages.
How are shared pages managed in Copy-on-write?,They are marked as copy-on-write.
What happens when a process writes to a shared page marked Copy-on-write?,A copy of the shared page is created.
Illustrate a Copy-on-write scenario when a child modifies a stack page.,"The OS gets a free frame, copies the page, and maps it to the child's address space."
"After a Copy-on-write operation, which page does the child modify?","Its newly copied page, not the parent's original."
What types of pages are copied and what types can remain shared under Copy-on-write?,"Only modified pages are copied; unmodified pages (e.g., executable code) can be shared."
Name operating systems that commonly use Copy-on-write.,"Windows, Linux, macOS."
What is `vfork()`?,"A variation of `fork()` in UNIX-based systems (Linux, macOS, BSD UNIX)."
Describe the parent and child process states after `vfork()`.,"The parent process is suspended, and the child uses the parent's address space."
Does `vfork()` use Copy-on-write?,"No, `vfork()` does not use copy-on-write."
What happens to parent's address space changes made by a child using `vfork()`?,Child process changes to the parent's address space are visible to the parent upon resumption.
What is a critical caution when using `vfork()`?,The child must not modify the parent's address space without careful management.
What is the primary intended use case for `vfork()`?,When the child process calls `exec()` immediately after creation.
What is the efficiency characteristic of `vfork()` for process creation?,"Extremely efficient, involving no page copying."
Where is `vfork()` sometimes used in practice?,To implement UNIX command-line shell interfaces.
"Define ""copy-on-write"" (from glossary).","Write causes data to be copied then modified; on shared page write, page copied, write to copy."
"Define ""virtual memory fork"" (`vfork()`) (from glossary).","`vfork()` system call; child shares parent's address space for read/write, parent suspended."
How does demand paging save I/O?,By loading only used pages.
How can demand paging increase the degree of multiprogramming?,By over-allocating memory.
Provide an example of how over-allocating memory can increase CPU utilization and throughput.,"6 processes, each needing 10 pages but using only 5, can run on 40 frames, leading to higher CPU utilization and throughput."
What problem can arise from over-allocating memory?,"Processes may suddenly need all their allocated pages (e.g., 60 frames needed when only 40 are available), leading to a memory shortage."
What other system component contributes to memory strain in an over-allocated system?,System memory is also used for I/O buffers.
How does over-allocation manifest in a system?,As a page fault with no free frames available.
What happens if there is no free frame available during a page fault?,The system must find a frame that is not in use and free it.
Describe the process of freeing a frame for page replacement.,"Write its contents to swap space (if modified), then change the corresponding page table entry to indicate the page is no longer in memory."
What is the freed frame used for?,To load the faulted page.
What is the first step in a modified page-fault service routine?,Find the desired page on secondary storage.
What are the steps to find a free frame in a modified page-fault service routine?,"If a free frame exists, use it. If not, use a page-replacement algorithm to select a victim frame. Write the victim frame to secondary storage (if modified) and update page/frame tables."
"After a frame is freed or found, what is the next step in the modified page-fault service routine?",Read the desired page into the newly freed frame and update page/frame tables.
What is the final step in the modified page-fault service routine?,Continue the process that incurred the page fault from where it was interrupted.
What is the consequence of having no free frames during a page fault?,"It requires two page transfers (a page-out for the victim, then a page-in for the new page), which doubles the page-fault service time."
How can the overhead of page replacement be reduced?,By using a modify bit (or dirty bit).
How is the modify bit set?,Hardware sets the modify bit if the page is written to.
What action is taken if a page's modify bit is set before replacement?,The page must be written to secondary storage.
What action is taken if a page's modify bit is not set before replacement?,"There is no need to write the page to secondary storage, as it is unchanged."
What is the benefit of using the modify bit in terms of page-fault service time?,It significantly reduces page-fault service time if the page has not been modified.
What fundamental separation does page replacement enable?,It separates logical memory from physical memory.
What is a key outcome of page replacement regarding memory size?,It allows for an enormous virtual memory space to be used on a smaller physical memory.
What are the two major problems that demand paging must address?,1. Frame-allocation algorithm (how many frames to allocate to each process). 2. Page-replacement algorithm (which frames to replace).
What is the primary goal of page replacement algorithms?,To achieve the lowest possible page-fault rate.
How are page-replacement algorithms evaluated?,"Using a reference string, which is a trace of memory accesses."
How is a reference string typically simplified for evaluation?,"It includes only the page number for each access, ignoring immediate repeated references to the same page."
What is the general relationship between the number of allocated frames and page faults?,More allocated frames generally lead to fewer page faults.
What does FIFO stand for in the context of page replacement?,"First-in, first-out."
Which page does the FIFO algorithm replace?,"The oldest page, meaning the first one that was brought into memory."
How can FIFO page replacement be implemented?,"Using a FIFO queue where the page at the head is replaced, and the new page is inserted at the tail."
What are some advantages of the FIFO page replacement algorithm?,It is easy to understand and program.
What is a potential drawback of FIFO page replacement regarding performance?,Its performance is not always good because it may replace actively used pages.
What are the consequences of a bad replacement choice by a page replacement algorithm?,An increased page-fault rate and slowed execution (though the program still runs correctly).
What anomaly does FIFO page replacement suffer from?,"Belady's anomaly, where the page-fault rate may increase as the number of allocated frames increases."
What is the full name and common abbreviations for the optimal page replacement algorithm?,Optimal page-replacement algorithm (OPT or MIN).
What are the key characteristics of the optimal page-replacement algorithm regarding page faults and Belady's anomaly?,It achieves the lowest page-fault rate and never suffers from Belady's anomaly.
What is the rule for the optimal page-replacement algorithm?,Replace the page that will not be used for the longest period of time.
What guarantee does the optimal page-replacement algorithm provide?,It guarantees the lowest possible page-fault rate.
Why is the optimal page-replacement algorithm difficult to implement in practice?,Because it requires future knowledge of the reference string.
What is the primary use of the optimal page-replacement algorithm in practice?,It is used mainly for comparison studies to evaluate other algorithms.
"What does LRU stand for, and what is its relationship to the optimal algorithm?",Least recently used (LRU) algorithm; it is an approximation of the optimal algorithm.
Which page does the LRU algorithm replace?,The page that has not been used for the longest period of time.
How can the LRU algorithm be described in relation to the optimal algorithm?,"It is like the optimal algorithm, but looking backward in time instead of forward."
Does LRU suffer from Belady's anomaly?,"No, it does not, because it is a stack algorithm."
What is a key requirement for implementing a true LRU algorithm?,It requires substantial hardware assistance.
How can LRU be implemented using counters?,"Associate a time-of-use field with each page-table entry. A CPU logical clock increments, and its value is copied to the field whenever a page is referenced. The page with the smallest time value is replaced."
How can LRU be implemented using a stack?,"Maintain a stack of page numbers. On reference, remove the page from its current position and put it on top. The most recently used page is at the top, and the least recently used is at the bottom. A doubly linked list is best for this."
Why is true LRU implementation considered expensive?,Due to the need for per-memory-reference updates.
Why are LRU-approximation algorithms necessary?,Many systems lack the hardware for true LRU implementation.
What hardware feature is often used as a basis for LRU approximation?,"The reference bit, which hardware sets when a page is referenced."
"How does the OS interact with the reference bits, and what information can it infer?","The OS clears the reference bits periodically. It can determine which pages have been used, but not the precise order of usage."
Describe the additional-reference-bits algorithm for LRU approximation.,"It keeps an 8-bit byte for each page. On a timer interrupt, the OS shifts the reference bit into the high-order bit of the byte and shifts other bits right. The 8-bit shift registers show the history of page use. The page with the lowest number (interpreted as unsigned integer) is considered LRU. Among pages with the smallest value, FIFO can be used."
Explain the second-chance page-replacement algorithm (clock algorithm).,"It's a basic FIFO algorithm. When a page is selected for replacement: if its reference bit is 0, replace it. If its reference bit is 1, give it a ""second chance"" by clearing the bit and resetting its arrival time to the current time. This page will not be replaced until others have been replaced or given second chances. It's implemented as a circular queue with a pointer that advances, clearing reference bits, until a 0-bit page is found. It degenerates to FIFO if all bits are set."
Describe the enhanced second-chance algorithm.,"It considers the (reference bit, modify bit) as an ordered pair, categorizing pages into four classes: (0,0) neither recently used nor modified (best to replace); (0,1) not recently used but modified (needs write-out); (1,0) recently used but clean (likely used again soon); (1,1) recently used and modified (likely used again soon, needs write-out). The algorithm replaces the first page found in the lowest nonempty class, preferring clean pages to reduce I/Os."
What is the fundamental idea behind counting-based page replacement algorithms?,To keep a counter of references for each page.
How does the Least Frequently Used (LFU) algorithm work?,It replaces the page with the smallest reference count.
What is a problem with the LFU algorithm?,"A page that was heavily used initially but is now unused might retain a high count, preventing its replacement."
How can the problem with LFU be mitigated?,"By shifting counts right periodically, creating an exponentially decaying average."
"How does the Most Frequently Used (MFU) algorithm replace pages, according to the main text?","It replaces the page with the smallest count, based on the assumption that a page with a small count was just brought in and is therefore likely to be used."
"Are LFU and MFU commonly used, and why or why not?",Neither LFU nor MFU are common because they are expensive to implement and do not approximate the optimal algorithm particularly well.
When are page-buffering algorithms used?,In addition to core page-replacement algorithms.
"Explain the ""pool of free frames"" page-buffering technique.","On a page fault, the desired page is read into a free frame from the pool immediately, allowing the process to restart faster. The victim frame is then added to the pool after its contents are written out."
"Explain the ""list of modified pages"" page-buffering technique.","When the paging device is idle, modified pages on this list are written to secondary storage, and their modify bits are reset. This increases the probability that a replacement will find a clean page, avoiding an immediate write-out."
"Explain the ""pool of free frames remembering old page"" page-buffering technique.","If a previously replaced page is needed again before its frame is reused, it can be retrieved directly from this pool without I/O. On a page fault, the system checks this pool first."
"Which operating system often uses page-buffering techniques, and with what page replacement algorithm?","UNIX systems often use page-buffering, specifically with the second-chance algorithm."
What type of applications might perform worse with standard OS virtual memory buffering?,"Some applications, such as databases."
Why might certain applications prefer to manage their own memory/storage?,Because they often understand their specific memory and storage usage patterns better than general-purpose OS algorithms.
"What is ""double buffering"" in the context of OS and application I/O?","It occurs when both the operating system and the application buffer I/O, leading to redundant buffering."
"How might data warehouse applications pose a challenge for LRU, and what alternative might be more efficient?","Data warehouses often perform sequential reads followed by computations/writes. LRU might remove older pages that will be needed again, making MFU potentially more efficient in such scenarios."
How do some operating systems allow special programs to bypass standard file system services for secondary storage?,"By allowing them to access secondary storage directly as a large sequential array of logical blocks, known as ""raw disk""."
What file-system services are bypassed when using raw I/O?,"Demand paging, locking, prefetching, allocation, names, and directories."
"For whom are raw partitions efficient, and for whom are regular file-system services generally better?","Raw partitions can be efficient for specific applications, but most applications perform better with regular file-system services."
Define over-allocating.,Providing access to more resources than physically available; allocating more virtual memory than physical memory.
Define page replacement.,The selection of a physical memory frame to be replaced when a new page is allocated.
What is a victim frame?,The frame selected by a page-replacement algorithm to be replaced.
What is a modify bit?,An MMU bit indicating a frame has been modified (meaning its contents must be saved to secondary storage before replacement).
What is a dirty bit?,An MMU bit indicating a frame has been modified (meaning its contents must be saved to secondary storage before replacement).
Define frame-allocation algorithm.,An OS algorithm for allocating physical memory frames among all competing demands (processes).
Define page-replacement algorithm.,An algorithm that chooses which victim frame will be replaced by a new data frame.
What is a reference string?,"A trace of accesses to a resource; specifically, a list of pages accessed over time."
What is Belady's anomaly?,A phenomenon where the page-fault rate may increase as the number of allocated physical frames increases for some page-replacement algorithms.
What is the optimal page-replacement algorithm?,"The algorithm with the lowest page-fault rate, which never suffers from Belady's anomaly."
Define Least Recently Used (LRU).,A page-replacement strategy that selects the item (or page in memory) that has not been accessed for the longest period of time.
What is a stack algorithm in the context of page replacement?,A class of page-replacement algorithms that do not suffer from Belady's anomaly.
What is a reference bit?,An MMU bit indicating that a page has been referenced (read or written to).
Define the second-chance page-replacement algorithm.,"A FIFO algorithm where, if the selected page's reference bit is set, the bit is cleared, and the page is given a ""second chance"" (not replaced immediately)."
"In the context of the second-chance algorithm, what does ""clock"" refer to?","A circular queue containing possible victim frames, used to implement the second-chance algorithm."
Define Least Frequently Used (LFU).,"A page-replacement strategy that selects the item (or page in virtual memory) that has been used least frequently, i.e., has the lowest access count."
Define Most Frequently Used (MFU).,"A page-replacement strategy that selects the item used most frequently; in virtual memory, page with highest access count."
What is raw disk?,"Direct access to secondary storage as a large sequential array of logical blocks, bypassing file system services."
What is the primary issue addressed by frame allocation?,How to allocate fixed free memory among processes.
"In a pure demand paging system, where are initial frames placed for user processes?",On the free-frame list.
"In pure demand paging, what happens when a page fault occurs?",Free frames are obtained from the free-frame list.
"In pure demand paging, what happens if the free-frame list is exhausted during a page fault?",A page-replacement algorithm is used.
"In pure demand paging, what happens to frames when a process terminates?",Frames are returned to the free-frame list.
How can OS-allocated buffer/table space contribute to user paging in frame allocation variations?,It can be used for user paging when not in use.
What is a common practice for reserving free frames in some allocation strategies?,"Keeping 3 free frames reserved: one for a page fault, and a replacement selected during a swap."
What is the basic strategy for allocating frames to a user process?,The user process is allocated any free frame.
What are the two main constraints on frame allocation?,"Cannot exceed total available frames (unless page sharing), and must allocate at least a minimum number of frames."
What is the impact of allocating fewer frames to a process on performance?,Higher page-fault rate and slower execution.
When might an instruction require a restart due to page faults?,If a page fault occurs before the instruction is complete.
What is the general rule for the minimum number of frames required for an instruction?,Enough frames for all pages an instruction can reference.
What is the minimum number of frames for a single memory-reference instruction?,1 frame for the instruction and 1 frame for the memory reference.
What is the minimum number of frames for a process with one-level indirect addressing?,At least 3 frames per process.
What primarily defines the minimum number of frames required for a system?,Computer architecture.
"For a ""move"" instruction straddling two frames with two indirect operands, how many frames might be needed?",6 frames.
What defines the maximum number of frames that can be allocated?,The total available physical memory.
"Define ""equal allocation"" in the context of virtual memory.","Assigns equal amounts of a resource to all requestors; in virtual memory, equal frames to each process."
"How are frames divided among 'n' processes using equal allocation, given 'm' frames?",Each process receives 'm/n' frames.
"Define ""proportional allocation"" in the context of virtual memory.","Assigns a resource in proportion to some aspect of the requestor; in virtual memory, page frames in proportion to process size."
When is proportional allocation used over equal allocation?,When processes need differing amounts of memory.
"How are frames allocated to process p_i with virtual memory size s_i using proportional allocation, given total size S and available frames m?","a_i ≈ (s_i/S) × m frames, ensuring a_i is an integer, greater than minimum, and sum ≤ m."
How does an increased multiprogramming level affect frame allocation to processes?,Processes lose frames.
How are high- and low-priority processes typically treated in standard equal or proportional allocation?,They are treated the same.
How can proportional allocation be adjusted to account for process priority?,"Allocate based on process priority, or a combination of size and priority."
"Define ""global replacement"" in page replacement algorithms.","Process selects replacement frame from all frames in system, even if allocated to another process."
What is a potential problem with global replacement regarding process performance?,Process performance depends on other processes' paging behavior (external circumstances).
What is the general impact of global replacement on system throughput compared to local replacement?,"Generally greater system throughput, and it is more common."
"Define ""local replacement"" in page replacement algorithms.",Process selects replacement frame only from its own allocated frames.
What is the impact of local replacement on a process's performance?,Performance depends only on its own paging behavior.
What is a page fault?,A condition where a referenced page has no valid mapping in memory.
"Define ""major fault"" (also known as ""hard fault"").",Page fault resolved by I/O to bring page from secondary storage.
What is required to resolve a major page fault?,Reading from backing store and updating the page table.
"Define ""minor fault"" (also known as ""soft fault"").",Page fault resolved without paging in data from secondary storage.
What is one reason for a minor page fault involving a shared library?,"A shared library is in memory but has no logical mapping for the process, requiring only a page table update."
What is another reason for a minor page fault involving a reclaimed page?,"A page was reclaimed to the free-frame list but not zeroed or reallocated, requiring the frame to be removed from the list and reassigned."
Which type of page fault is generally less time-consuming?,Minor page faults.
What Linux command can be used to observe minor and major page faults?,"`ps -eo min_flt,maj_flt,cmd`"
"Define ""reapers"" in the context of memory management.","Routines that scan memory, freeing frames to maintain minimum free memory."
"In a global page-replacement strategy, when are replacement routines triggered for the free-frame list?","When the list falls below a certain threshold, not necessarily at zero, to ensure sufficient free memory."
"When are kernel reaper routines triggered, and what do they do?","They are triggered when free memory falls below a minimum threshold, reclaiming pages from all processes (excluding the kernel) using page-replacement algorithms."
When is a reaper routine suspended and resumed?,"Suspended when free memory reaches a maximum threshold, and resumed when free memory falls below the minimum threshold."
Which page-replacement algorithm do reaper routines typically use?,LRU approximation.
What happens if a reaper routine is unable to maintain free frames effectively?,"It reclaims more aggressively, potentially using an algorithm like pure FIFO."
"Define ""out-of-memory (OOM) killer.""",Linux routine that terminates processes to free memory when free memory is very low.
"What is an OOM score, and how does it relate to process termination likelihood?",A score calculated by memory usage percentage; a higher score indicates a higher termination likelihood by the OOM killer.
How can OOM scores be viewed in Linux?,By checking `/proc/<pid>/oom_score` for a given process ID.
"Define ""non-uniform memory access (NUMA).""",Architecture where memory access time varies based on the CPU core.
Describe key characteristics of NUMA systems in terms of CPU access and performance.,"CPUs access local memory faster than remote memory; they are slower than uniform access systems but allow more CPUs, greater throughput, and parallelism."
What is critical for performance management in NUMA systems?,Managing page frame location.
How do NUMA-aware systems allocate frames during a page fault?,"Frames are allocated ""as close as possible"" to the CPU (minimum latency, same system board)."
How does the scheduler contribute to performance optimization in NUMA systems?,"It tracks the last CPU a process ran on and schedules the process on that CPU, then allocates frames close to it, improving cache hits and decreasing memory access latency."
Why do threads complicate memory allocation in NUMA systems?,"Process threads can run on different system boards, making memory allocation challenging."
How does Linux's CFS scheduler address thread migration in NUMA environments?,"The kernel identifies scheduling domains, and the CFS scheduler prevents thread migration across these domains to avoid memory access penalties."
How does Linux manage memory allocation for threads in NUMA environments?,"It maintains a separate free-frame list per NUMA node, ensuring threads are allocated memory from their running node."
"Define ""lgroups"" in the context of Solaris.",Solaris locality groups in the kernel that gather CPUs and memory for optimized access in NUMA systems.
How do Solaris lgroups function to optimize memory access?,"Each lgroup contains CPUs and memory where CPU access to memory within the group is within a defined latency; Solaris schedules threads and allocates memory within the lgroup, or nearby lgroups if not possible, to minimize latency and maximize cache hits."
"What is a key characteristic of a process experiencing ""thrashing"" in terms of frames?","A process without ""enough"" frames (minimum needed for its working set)."
What immediately happens when a process without enough frames tries to execute?,It quickly page-faults.
"In the context of thrashing, what happens when a page needed immediately is replaced?",The process faults again and again.
"What is ""thrashing""?",High paging activity; spending more time paging than executing.
What is the primary result of thrashing?,Severe performance problems.
"Describe the initial scenario that can lead to thrashing, involving OS monitoring CPU utilization.","OS monitors CPU utilization; if it's low, the OS increases multiprogramming by initiating a new process."
What type of page-replacement algorithm exacerbates thrashing?,A global page-replacement algorithm (replaces pages without regard to the process they belong to).
What happens when a process needs more frames and a global replacement algorithm is used?,It starts faulting and takes frames from other processes.
What is the consequence when other processes lose frames due to a global replacement algorithm?,"They also fault and take frames from others, leading to a chain reaction."
How do faulting processes impact the paging device and the ready queue?,"Faulting processes heavily use the paging device, causing the ready queue to empty as processes wait."
"What happens to CPU utilization as processes wait for the paging device, and how does the CPU scheduler react?","CPU utilization decreases, prompting the CPU scheduler to *increase* multiprogramming further."
How does the new process initiated by the CPU scheduler contribute to the thrashing cycle?,"It takes frames, leading to more page faults and a longer paging device queue."
What happens to system throughput when thrashing occurs?,System throughput plunges.
What happens to the page-fault rate and effective memory-access time during thrashing?,"The page-fault rate increases tremendously, causing effective memory-access time to increase."
What is the overall state of the system when thrashing occurs?,No work is done; processes spend all their time paging.
"Describe the relationship between CPU utilization and the degree of multiprogramming, especially when thrashing occurs.","As multiprogramming increases, CPU utilization increases (slower) until it reaches a maximum. Further increases lead to thrashing, causing CPU utilization to drop sharply."
What is the immediate action to stop thrashing once it has begun?,Decrease the degree of multiprogramming.
What types of page replacement algorithms can limit the effects of thrashing?,Local replacement algorithm or priority replacement algorithm.
"Define ""local replacement algorithm"".",Page replacement algorithm that avoids thrashing by not allowing a process to steal frames from other processes.
"Define ""priority replacement algorithm"".",Page replacement algorithm that avoids thrashing by not allowing a process to steal frames from other processes.
What is a remaining problem even with local or priority replacement algorithms when thrashing processes exist?,"Thrashing processes still queue for the paging device, increasing the average service time for page faults and thus increasing effective access time for all processes."
What is the fundamental way to prevent thrashing?,Provide each process with enough frames.
What model is used to determine how many frames a process needs?,The locality model.
"Define ""locality model"".",A model where a process moves from locality to locality during its execution.
"Define ""locality model"" (from glossary).",Model for page replacement based on the working-set strategy.
"What is a ""locality"" in the context of the locality model?",A set of pages actively used together.
Describe characteristics of localities in a running program.,"A running program has several overlapping localities, defined by program structure and data structures."
Give an example of a change in locality.,"A function call might create a new locality (function instructions, local variables, subset of global variables); exiting a function leaves that locality."
What common computing principle is reflected by the locality model?,"The principle behind caching, which states that accesses are patterned, not random."
When does thrashing occur in relation to a process's locality?,When a process does not have enough frames for its current locality.
What is the working-set model based on?,The locality assumption.
"Define ""working-set model"" (from glossary).",Memory access model based on tracking the set of most recently accessed pages.
What parameter is used in the working-set model to define a 'window'?,"Delta (Δ), which defines the working-set window."
"Define ""working-set window"" (from glossary).","Limited set of most recently accessed pages (a ""window"" view of the entire set of accessed pages)."
"What is the ""working set"" of a process?",The set of pages in the most recent Delta (Δ) references (within the working-set window).
"Define ""working set"" (from glossary).",The set of pages in the most recent page references.
How do pages enter and leave the working set?,A page in active use is in the working set. A page no longer used drops from the working set Delta (Δ) time units after its last reference.
What does the working set approximate?,A program's locality.
What determines the accuracy of the working set?,The selection of the Delta (Δ) (working-set window) parameter.
What is the consequence if Delta (Δ) is too small for the working-set model?,It won't encompass the entire locality.
What is the consequence if Delta (Δ) is too large for the working-set model?,It may overlap several localities.
What does an infinite Delta (Δ) mean for the working set?,The working set becomes all pages touched during execution.
What is the most important property of the working-set model?,The working-set size.
How is the total demand for frames (D) calculated in the working-set model?,"D = sum of WSS_i, where WSS_i is the working-set size for each process i."
"When does thrashing occur according to the working-set model, in relation to available frames?","If the total demand for frames (D) is greater than the total available frames (m), some processes will lack frames and thrash."
How does the OS utilize the working-set model for frame allocation?,The OS monitors the working set of each process and allocates enough frames for its working-set size.
When does the OS initiate a new process according to the working-set model?,When there are enough extra frames available beyond the current demand.
What action does the OS take if the sum of working-set sizes exceeds available frames?,The OS suspends a process.
What happens to a suspended process's pages and frames in the working-set model?,"Its pages are swapped out, and its frames are reallocated to other processes. It is restarted later."
What are the benefits of using the working-set model for memory management?,"It prevents thrashing, keeps multiprogramming high, and optimizes CPU utilization."
What is a primary difficulty in implementing the working-set model?,Tracking the constantly moving working-set window.
How is the working-set window often approximated in practice?,Using a fixed-interval timer interrupt combined with reference bits.
Provide an example of parameters for approximating the working set using timer interrupts and reference bits.,"Delta (Δ) = 10,000 references, with an interrupt every 5,000 references."
How are reference bits used in the working-set approximation at a timer interrupt or page fault?,"At a timer interrupt, reference bits are copied and cleared. At a page fault, the current reference bit and two in-memory bits are examined; if at least one bit is on, it means the page was used within the last 10,000-15,000 references, thus in the working set."
What are the limitations of the working-set approximation using reference bits?,"It is not entirely accurate, as it cannot tell the exact reference time within an interval. Uncertainty can be reduced by increasing history bits or interrupt frequency, but at a higher cost."
"What is the ""Page-fault frequency (PFF)"" strategy used for?","It's a more direct strategy for thrashing control, often used as an alternative to the working-set model."
"Define ""page-fault frequency"" (PFF).",The frequency of page faults.
What is the primary goal of the PFF strategy?,To prevent thrashing by controlling the page-fault rate.
"According to the PFF strategy, what action is taken if the page-fault rate is too high?",The process needs more frames.
"According to the PFF strategy, what action is taken if the page-fault rate is too low?",The process may have too many frames.
How does the PFF strategy enforce desired page-fault rates?,By establishing upper and lower bounds on the desired page-fault rate.
What action is taken in the PFF strategy if the actual PFF exceeds the upper limit?,Allocate another frame to the process.
What action is taken in the PFF strategy if the actual PFF falls below the lower limit?,Remove a frame from the process.
What happens if PFF increases but no free frames are available in the system?,"A process is selected and swapped out to backing store, and its freed frames are then distributed to high-PFF processes."
What is the performance impact of thrashing and swapping in modern systems?,High performance impact.
What is considered the best practice to avoid thrashing and swapping today?,Include enough physical memory to avoid them.
What is the main benefit of providing enough physical memory to avoid thrashing?,It provides the best user experience across various devices (smartphones to large servers).
What is memory compression an alternative to?,Paging.
Define memory compression.,An alternative to paging; compresses frame contents to decrease memory usage.
How does memory compression reduce memory usage?,"It compresses several frames into a single frame, reducing memory usage without swapping pages."
What is a primary benefit of memory compression?,It reduces memory usage without swapping pages.
"What triggers page replacement in the context of memory compression (e.g., Figure 10.7.1)?",The free-frame list falling below a certain threshold.
"After page replacement is triggered in memory compression (e.g., Figure 10.7.1), what happens to selected frames (e.g., 15, 3, 35, 26)?",They are placed on a modified-frame list.
"In memory compression (e.g., Figure 10.7.1), what is done with frames on the modified-frame list instead of writing them to swap space?","They are compressed (e.g., three frames) into a single page frame."
"In the memory compression process (e.g., Figure 10.7.2), which type of frame is typically removed from the free-frame list to store compressed data?","A frame (e.g., Frame 7)."
"Where are compressed frames (e.g., 15, 3, 35) stored after compression in the memory compression process (e.g., Figure 10.7.2)?","In a single frame (e.g., Frame 7), which is then stored in a list of compressed frames."
"What happens to the original frames (e.g., 15, 3, 35) after their contents are compressed into another frame?",They are moved to the free-frame list.
What happens if a compressed frame is referenced?,"A page fault occurs, the frame is decompressed, and the original pages are restored."
"Do mobile systems (Android, iOS) typically support standard swapping/paging?","No, they generally do not."
"What is integral to the memory-management strategy of mobile systems (Android, iOS)?",Memory compression.
Which desktop operating systems support memory compression?,Windows 10 and macOS.
Which type of applications are candidates for memory compression on Windows 10 mobile devices?,Universal Windows Platform (UWP) apps.
Define Universal Windows Platform (UWP).,Windows 10 architecture providing a common app platform for all devices running it.
How does macOS (Version 10.9+) utilize memory compression?,"It compresses LRU (Least Recently Used) pages when free memory is short, then pages if needed."
How does memory compression performance compare to paging to SSD on macOS?,Memory compression is faster than paging to SSD on macOS.
What does memory compression require for storing compressed pages?,Allocating free frames.
What is an example of significant memory saving possible with memory compression?,Compressing 3 frames into 1.
What are the two main factors that have contention in memory compression?,Compression speed and compression ratio.
Define compression ratio.,A measurement of compression effectiveness (ratio of compressed to uncompressed space).
What is the relationship between higher compression ratios and algorithm characteristics?,"Higher compression ratios typically require slower, more computationally expensive algorithms."
What do most memory compression algorithms aim to balance?,High compression ratios with fast algorithms.
How can memory compression performance be improved?,By parallel compression using multiple cores.
Name two examples of fast memory compression algorithms/implementations and their typical compression performance.,"Microsoft's Xpress and Apple's WKdm, which compress to 30-50% of the original size."
How are pages allocated for user-mode processes?,Pages are allocated from the kernel's free page frame list.
How is the kernel's free page frame list populated for user-mode processes?,"By page-replacement algorithms (e.g., Section 10.4)."
What is a characteristic of free pages for user-mode processes in physical memory?,They are scattered throughout physical memory.
What happens when a user-mode process requests a single byte of memory?,"An entire page frame is granted, leading to internal fragmentation."
What is one reason kernel memory is often allocated from a different free-memory pool than user-mode memory?,"Kernel requests varying data structure sizes, some less than a page, requiring conservative memory use and minimized fragmentation. Many OS do not subject kernel code/data to paging."
What is another reason kernel memory is often allocated from a different free-memory pool than user-mode memory?,"Hardware devices interact directly with physical memory (no virtual memory interface) and may require physically contiguous pages, which user-mode pages don't need."
What are two strategies for managing kernel free memory?,"The ""buddy system"" and ""slab allocation""."
How does the buddy system allocate memory?,From a fixed-size segment of physically contiguous pages.
What type of allocator does the buddy system use?,A power-of-2 allocator.
What happens if a memory request in the buddy system is not appropriately sized?,It is rounded up to the next highest power of 2.
Example: How is an 11 KB memory request satisfied by the buddy system?,With a 16-KB segment.
Describe the division process in the buddy system for a 21 KB request from an initial 256 KB segment.,"The 256 KB segment is divided into two buddies ($A_L$ and $A_R$), each 128 KB. One buddy ($A_L$) is divided into two 64-KB buddies ($B_L$ and $B_R$). The next-highest power of 2 for 21 KB is 32 KB. One 64-KB buddy ($B_L$) is divided into two 32-KB buddies ($C_L$ and $C_R$). One 32-KB buddy ($C_L$) is then used for the 21-KB request."
What is an advantage of the buddy system?,It can quickly combine adjacent buddies to form larger segments using coalescing.
Example: How does coalescing work in the buddy system when the kernel releases a $C_L$ unit?,"When the kernel releases a $C_L$ unit, $C_L$ and $C_R$ coalesce into a 64-KB segment ($B_L$). $B_L$ can then coalesce with $B_R$ to form a 128-KB segment ($A_L$), which can eventually form the original 256-KB segment."
What is a drawback of the buddy system?,Rounding up to the next highest power of 2 causes internal fragmentation.
Example: How does internal fragmentation occur in the buddy system for a 33-KB request?,"A 64-KB segment is allocated, meaning cannot guarantee less than 50% of the allocated unit is wasted."
What is the purpose of a single cache in slab allocation?,"A single cache is maintained for each unique kernel data structure (e.g., process descriptors, file objects, semaphores)."
What are caches populated with in slab allocation?,Objects (instantiations of kernel data structures).
Describe the initial state of objects when a cache is created in slab allocation.,Objects are initially marked as `free` and allocated to the cache.
How is the number of objects determined in a slab?,"It depends on the slab size (e.g., a 12-KB slab can hold six 2-KB objects)."
How does the slab allocator fulfill a request for a new object?,"The allocator assigns any `free` object from the cache, and the object is then marked `used`."
"Scenario: How does the slab allocator fulfill a kernel request for a process descriptor (`struct task_struct`, ~1.7 KB)?","The cache fulfills the request with a pre-allocated, free `struct task_struct` object."
What are the three states of slabs in Linux?,"Full, Empty, and Partial."
Define a 'Full' slab state in Linux.,All objects in the slab are `used`.
Define an 'Empty' slab state in Linux.,All objects in the slab are `free`.
Define a 'Partial' slab state in Linux.,The slab has both `used` and `free` objects.
In what order does the slab allocator attempt to satisfy a request?,"1. From a free object in a partial slab. 2. If none, from a free object from an empty slab. 3. If no empty slabs, a new slab is allocated from contiguous physical pages, assigned to the cache; object memory is allocated from the new slab."
What are the two main benefits of the slab allocator?,1. No memory wasted due to fragmentation. 2. Memory requests satisfied quickly.
Explain why the slab allocator prevents memory fragmentation.,"Each kernel data structure has an associated cache, which is made of slabs divided into object-sized chunks. When the kernel requests memory, the exact amount is returned."
Explain why the slab allocator satisfies memory requests quickly.,"It is effective for frequent object allocation/deallocation (common in the kernel). Objects are created in advance and quickly allocated from the cache. Released objects are marked free and returned to the cache, making them immediately available."
Where did the slab allocator first appear?,In the Solaris 2.4 kernel.
Which Linux kernel version adopted the slab allocator (referred to as SLAB)?,Linux Version 2.2+.
What are the recent Linux kernel memory allocators?,SLOB and SLUB.
What is the SLOB allocator designed for?,"Systems with limited memory (e.g., embedded systems)."
How does the SLOB allocator manage memory?,"It maintains three lists: `small` (<256 bytes), `medium` (<1,024 bytes), and `large` (other objects < page size), allocating from the appropriate list using a first-fit policy."
Which Linux kernel allocator is the default for Linux kernel (Version 2.6.24+)?,The SLUB allocator.
What did the SLUB allocator replace?,SLAB.
What are the improvements of SLUB over SLAB?,"Reduced SLAB overhead, stores metadata in the `page` structure (not with each slab), and has no per-CPU queues for objects (significant memory saving on multi-processor systems), leading to better performance with more processors."
Define 'power-of-2 allocator'.,Allocator in buddy system; satisfies memory requests in units sized as a power of 2.
Define 'buddies' in the context of the buddy system.,Pairs of equal size in buddy memory allocation.
Define 'coalescing'.,Combining freed memory in adjacent buddies into larger segments.
Define 'slab allocation'.,"Memory allocation method; slab split into object-sized chunks, eliminating fragmentation."
Define 'slab' in the context of slab allocation.,Section of memory (one or more contiguous pages) used in slab allocation.
Define 'cache' in the context of the slab allocator.,"In slab allocator, consists of one or more slabs."
Define 'object' in the context of the slab allocator.,Instance of a class or data structure.
What issue does pure demand paging face when a process starts?,A large number of page faults occur due to initial locality.
Define prepaging.,"Bringing pages into memory before they are requested, as an attempt to prevent high initial paging."
What is the strategy employed by prepaging?,To bring some or all needed pages into memory at once.
Provide an example of prepaging using the working-set model.,Remembering the working set for a suspended process and automatically bringing the entire working set back into memory before restarting.
What is the primary advantage of prepaging?,Comparing the cost of prepaging against the potential cost of servicing numerous page faults.
What is the main risk associated with prepaging?,"Many prepaged pages may not be used, leading to wasted memory and effort."
"In prepaging cost analysis, if 's' pages are prepaged and 'α' is the fraction of 's' pages actually used, what does 's · (1 - α)' represent?",The cost of unnecessary pages that were prepaged but not used.
Under what condition does prepaging generally result in a loss according to cost analysis?,If 'α' (the fraction of used prepaged pages) is approximately 0.
Under what condition does prepaging generally result in a win according to cost analysis?,If 'α' (the fraction of used prepaged pages) is approximately 1.
Why is prepaging executable programs difficult?,It is unclear what specific pages should be brought into memory.
Why is prepaging files more predictable?,Files are often accessed sequentially.
What Linux system call is used to prefetch file contents into memory?,`readahead()`.
What are typical characteristics of page sizes in new machine designs?,"They are invariably powers of 2, typically ranging from 4,096 ($2^{12}$) to 4,194,304 ($2^{22}$) bytes."
How does decreasing page size affect the page table size?,"It increases the number of pages, thereby increasing the overall page table size."
Why is a larger page size desirable for page table size?,"Each active process has its own copy of the page table, and larger pages mean fewer entries, reducing the physical memory consumed by the page tables."
How does page size impact memory utilization and internal fragmentation?,"Memory is better utilized with smaller pages, as processes are unlikely to end exactly on a page boundary, leading to unused allocated space (internal fragmentation) in larger pages."
Define internal fragmentation.,The phenomenon where a part of the final page allocated to a process is unused because the process does not require the entire page.
What is the average waste due to internal fragmentation?,Half of the final page allocated to a process.
What page size is required to minimize internal fragmentation?,A small page size.
What are the three components of I/O time when reading or writing a page?,"Seek time, latency, and transfer time."
Which component of I/O time is directly proportional to the page size?,Transfer time.
Why does a larger page size generally argue for minimizing total I/O time?,"Because seek and latency times often dwarf the transfer time, doubling the page size results in a minimal increase in total I/O time compared to performing multiple smaller I/Os."
How does a smaller page size affect locality and total I/O?,It can reduce total I/O and improve locality because each page can match a program's locality more accurately.
Define 'resolution' in the context of paging.,"The ability to isolate and bring into memory only the data that is actually needed, without transferring unneeded data within the same page."
How does page size influence the number of page faults?,Larger page sizes generally lead to a reduced number of page faults.
What are some significant overheads associated with each page fault?,"An interrupt, saving registers, replacing a page, queuing, and updating page tables."
What has been the historical trend in page size development?,"A trend toward larger page sizes, even for mobile systems, with modern systems supporting very large page sizes (e.g., Linux huge pages)."
Define TLB hit ratio.,The percentage of virtual address translations that are successfully resolved in the Translation Look-aside Buffer (TLB).
How can the TLB hit ratio be increased?,"By increasing the number of entries in the TLB, although this is expensive and power-hungry due to associative memory."
Define TLB reach.,"The amount of memory addressable by the Translation Look-aside Buffer (TLB), calculated as the number of entries multiplied by the page size."
What is the ideal goal for TLB reach?,The working set for a process should ideally be stored entirely within the TLB.
What happens if a process has insufficient TLB reach?,"The process spends more time resolving memory references by traversing the page table, rather than using the faster TLB."
"Besides increasing TLB entries, what other approach can increase TLB reach?",Increasing the page size or providing support for multiple page sizes.
What is the 'contiguous bit' in an ARM v8 TLB entry?,"A bit that, when set, indicates that the TLB entry maps contiguous (adjacent) blocks of memory rather than a single page."
How might an operating system manage the TLB when multiple page sizes are supported?,"The OS may manage the TLB in software, often with a TLB entry field indicating the page frame size or a contiguous block."
What is the primary purpose of inverted page tables?,To reduce the physical memory needed for virtual-to-physical address translations.
How do inverted page tables work?,"They have one entry per page of physical memory, indexed by a combination of process ID and page number (<process-id, page-number>)."
What is a main downside of using inverted page tables?,"They no longer contain complete information about the logical address space of a process, which is needed for demand paging."
How do inverted page tables compensate for the lack of complete logical address space information for demand paging?,"An external page table (one per process) is kept, which looks like a traditional per-process page table and contains virtual page location information."
When are external page tables (used with inverted page tables) referenced?,"Only on a page fault, meaning they don't need to be quickly available and can themselves be paged in/out as necessary."
What special case can occur with inverted page tables that requires careful kernel handling?,"A page fault may cause another page fault (a 'double fault') if the external page table itself needs to be paged in, leading to a delay in page-lookup processing."
"How can system performance be improved, even though demand paging is designed to be transparent to user programs?",By making the user or compiler aware of demand paging and structuring programs accordingly.
"In a 128x128 array initialized with 128-word pages, how many page faults occur for row-major order (outer loop 'j', inner loop 'i') if fewer than 128 frames are allocated?","16,384 page faults (128 × 128), because it zeros one word in each page, then another word in each page, leading to many page misses."
"In a 128x128 array initialized with 128-word pages, how many page faults occur for column-major order (outer loop 'i', inner loop 'j')?","128 page faults, because it zeros all words on one page before moving to the next, improving locality."
How can careful selection of data structures and programming structures improve paging performance?,"By increasing locality, lowering the page-fault rate, and resulting in a smaller working set."
Provide an example of a data structure with good locality.,"A stack, where access is almost always to the top."
Provide an example of a data structure with bad locality.,"A hash table, which tends to scatter memory references across different pages."
What are 'clean pages' in the context of program structure and paging?,"Code pages that are read-only and never modified; they are beneficial because they do not need to be paged out, saving I/O operations."
How can loaders optimize for paging performance?,By avoiding placing routines across page boundaries (keeping a routine in one page) and by packing frequently called routines into the same page.
What is the purpose of 'I/O interlock' and 'page locking' in demand paging?,"To allow pages to be fixed in memory (locked) and prevent them from being paged out, particularly during I/O operations to user memory."
Describe the problem scenario that page locking addresses during I/O.,"If a process initiates an I/O operation to a user memory buffer, and that page is subsequently swapped out by another process through global replacement, the pending I/O operation would then target an incorrect or different page frame."
"What is one solution to the I/O interlock problem, besides page locking?","Never executing I/O directly to user memory, instead copying data between system memory and user memory; however, this incurs potentially high overhead."
How does a 'lock bit' function in page locking?,"A 'lock bit' is associated with every frame, and when set, prevents that frame from being selected for replacement by the page replacement algorithm."
How are lock bits used when performing a disk write operation?,"The pages containing the block to be written are locked into memory before the I/O operation starts, and then unlocked once the I/O is complete."
Why are some or all OS kernel pages often locked into memory?,"Many operating systems cannot tolerate a page fault occurring within the kernel itself, as it could lead to system instability."
Define pinning in the context of page locking.,"The act of locking pages into memory, often initiated by user processes, to prevent them from being paged out."
Give an example of a user process that might utilize page pinning.,"A database process that manages a large chunk of memory and frequently moves blocks between secondary storage and memory, benefiting from persistent memory residency."
What is a potential danger or downside of using the lock bit feature?,"A bug could cause a lock bit to be turned on but never turned off, rendering the locked frame permanently unusable."
How does Solaris handle applications' requests for page locking (pinning)?,"Solaris allows applications to provide locking 'hints', but the OS can disregard these hints if the free-frame pool is too small or if a process requests an excessive number of locked pages."
What is the primary role of mass storage in computer systems?,To permanently store files and data.
What types of secondary storage do modern computers primarily use?,Hard disk drives (HDDs) and nonvolatile memory (NVM) devices.
List common variations among secondary storage devices.,"Transfer (character at a time vs. block), Access (sequentially vs. randomly), Data transfer (synchronously vs. asynchronously), Usage (dedicated vs. shared), Read-only vs. read-write, Speed (slowest major component)."
What are the key goals of an OS I/O subsystem?,"Provide the simplest interface possible to the rest of the system, and optimize I/O for maximum concurrency (devices are performance bottleneck)."
What is 'mass storage'?,The nonvolatile storage system of a computer.
What is the main mass-storage system?,Secondary storage (HDDs and NVM devices).
What types of tertiary storage exist for some systems?,"Slower, larger storage like magnetic tape, optical disks, and cloud storage."
What is the general term for all types of non-volatile storage?,"Non-volatile storage (NVS) or ""storage drives""."
hard disk drive (HDD),"Secondary storage device based on mechanical components (spinning magnetic media platters, moving read-write heads)."
nonvolatile memory (NVM),Persistent storage based on circuits and electric charges.
How is information stored and read on an HDD platter?,Information is stored by recording magnetically and read by detecting magnetic patterns.
platter,HDD component with a magnetic media layer for holding charges; flat circular shape (like a CD).
disk arm,"HDD component holding read-write heads, which moves all heads as a unit over cylinders of platters."
track,"On an HDD platter, the medium under the read-write head during platter rotation; logically a circular division of the platter surface."
sectors,"On an HDD platter, fixed-size sections that subdivide tracks."
cylinder,"On an HDD, the set of tracks under read-write heads on all platters in the device at a given arm position."
What is the smallest unit of transfer on an HDD?,A sector.
"What was the common sector size for HDDs until around 2010, and what is it migrating to?","Commonly 512 bytes until ~2010, then migrating to 4KB."
What are common rotation speeds (RPM) for HDD motors?,"5,400, 7,200, 10,000, 15,000 RPM."
transfer rate,The rate at which data flows between the drive and the computer.
positioning time,"On an HDD, the time for the read-write head to position over the desired track; also known as random-access time."
What are the two parts of positioning time (random-access time) for an HDD?,Seek time and rotational latency.
seek time,"On an HDD, the time for the disk arm to move the read-write head to the desired cylinder."
rotational latency,"On an HDD, the time for the desired sector to rotate to the disk head once the head is over the desired cylinder."
What are typical performance figures for modern disks in terms of transfer rate and latency?,"Tens to hundreds of MB/sec transfer, several milliseconds seek/rotational latency."
How do drive controllers improve HDD performance?,By using DRAM buffers.
head crash,"On an HDD, a mechanical problem where the read-write head touches the platter surface."
What is the consequence of a head crash on an HDD?,"It's normally irreparable, the entire disk is replaced, and data is lost unless backed up or RAID protected."
effective transfer rate,"The actual, measured transfer rate of data between two devices, which is typically lower than published rates."
What are NVM devices composed of?,A controller and flash NAND die semiconductor chips that store data.
solid-state disk (SSD),A disk-drive-like storage device using flash-memory-based nonvolatile memory.
USB drive,Nonvolatile memory in the form of a device that plugs into a USB port (also known as thumb drive/flash drive).
What are the advantages of NVM devices over HDDs?,"They are more reliable (no moving parts), faster (no seek/rotational latency), and consume less power."
What are the disadvantages of NVM devices compared to larger HDDs?,They are more expensive per MB and have less capacity.
What are the storage and reliability challenges with NAND semiconductors?,"Data cannot be overwritten directly (NAND cells must be erased first), and they deteriorate with every erase cycle (~100,000 program-erase cycles)."
"In what increments are NAND flash devices read/written, and in what increments are they erased?","Read/write in ""page"" increments (similar to a sector), and erasure occurs in ""block"" increments (several pages), which is much slower."
How is NVM lifespan measured?,In Drive Writes Per Day (DWPD).
flash translation layer (FTL),"For nonvolatile memory, a table maintained by the controller that maps physical pages to currently valid logical blocks."
What happens when a full SSD has a pending write and no free blocks?,"If no free blocks but individual pages hold invalid data, garbage collection occurs, where good data is copied to other locations, freeing blocks for erase/writes."
garbage collection,"The recovery of space containing no-longer-valid data, typically by copying valid data to new locations and then erasing the old block."
over-provisioning,"In non-volatile memory, space (e.g., 20% of total) set aside by the device as always available write area, not counted in device free space."
wear leveling,"In nonvolatile memory, an effort by the controller to select all NAND cells over time as write targets to avoid premature media failure due to frequently erased blocks."
How do NVM devices provide data protection?,They include error-correcting codes (ECC) calculated and stored with the data to detect and correct errors.
RAM drives,"Sections of a system's DRAM presented as secondary storage devices, created by device drivers."
What is a practical use of RAM drives despite their volatility?,"To allow users/programmers to place data in memory for temporary safekeeping using standard file operations, useful for temporary files or sharing data."
magnetic tape,"A nonvolatile magnetic media storage device (tape spooled on reels, passing over a read-write head); mostly used for backups, infrequently used info, or transferring info between systems."
How do magnetic tapes compare to main memory and drives in terms of random access time?,"Random access is very slow: ~1000x slower than HDDs, ~100,000x slower than SSDs."
"Once positioned correctly, how fast do tape drives read/write?",At speeds comparable to HDDs.
I/O bus,Physical connection of an I/O device to a computer system.
List some common bus types for connecting secondary storage.,"Advanced Technology Attachment (ATA), Serial ATA (SATA), eSATA, Serial Attached SCSI (SAS), Universal Serial Bus (USB), Fibre Channel (FC)."
Which bus type is most common for secondary storage?,SATA.
NVM express (NVMe),"A high-speed I/O bus specifically for NVM storage, which directly connects the device to the system PCI bus to increase throughput and decrease latency."
controller,A special processor managing I/O devices.
host bus adapter (HBA),A device controller installed in a host bus port to allow a device connection to the host.
host controller,"I/O-managing processors within a computer (e.g., inside an HBA)."
device controller,An I/O managing processor built into each storage device.
How does a mass storage I/O operation typically proceed?,"The computer places a command into the host controller (via memory-mapped I/O ports), which then sends the command via messages to the device controller. The device controller operates the drive hardware."
"Where does data transfer occur at the drive, and then to the host?","At the drive, data transfers between the device controller's built-in cache and the storage media. Data then transfers to the host between the cache and host DRAM via DMA at fast electronic speeds."
logical blocks,Logical addresses used to access blocks on storage devices; also the smallest unit of transfer.
How are storage devices logically addressed?,As large one-dimensional arrays of logical blocks.
What does a logical block map to physically on a device?,A physical sector or a semiconductor page.
"Why is converting LBA to old-style disk addresses (cylinder, track, sector) difficult in practice?","Because of defective sector substitution with spares, non-constant sectors per track on some drives, and internal management by disk manufacturers that hides the true physical mapping."
What assumption do algorithms dealing with HDDs make about logical and physical addresses?,That ascending logical addresses correspond to ascending physical addresses.
constant linear velocity (CLV),A device-recording method where bit density is uniform per track by varying the rotational speed of the medium.
Why do outer tracks in CLV have more sectors?,Because they are longer and bit density is uniform.
Where is CLV commonly used?,In CD-ROM and DVD-ROM drives.
constant angular velocity (CAV),"A device-recording method where the medium spins at a constant velocity, and bit density decreases from inner to outer tracks to maintain a constant data rate."
Where is CAV commonly used?,In hard disks.
"What is the OS's responsibility regarding hardware efficiency, especially for HDDs?","For HDDs, the OS is responsible for minimizing access time and maximizing data transfer bandwidth."
What is access time for HDDs/mechanical storage?,"It is the total time required to read or write data, consisting of seek time and rotational latency."
"Define ""seek time"" in the context of HDD access time.",The time for the device arm to move heads to the desired cylinder.
"Define ""rotational latency"" in the context of HDD access time.",The additional time for the platter to rotate the desired sector to the head.
"Define ""device bandwidth"" for HDDs.",Total bytes transferred divided by total time (from the first request to the last transfer completion).
How can access time and bandwidth be improved for HDDs?,By managing the order of storage I/O requests.
How does a process initiate an I/O request?,It issues a system call to the OS.
What information does an I/O request typically specify?,"Input/output direction, open file handle, memory address, and amount of data."
What happens if the drive/controller is available when an I/O request is made?,The request is serviced immediately.
What happens if the drive/controller is busy when a new I/O request is made?,New requests are placed in a queue.
What is common about device queues in a multiprogramming system?,They often have pending requests.
What is the benefit of having a queue of I/O requests for device drivers?,"It allows device drivers to improve performance by ordering requests (e.g., avoiding head seeks)."
How did past HDD interfaces differ from modern drives regarding control over physical locations?,"Past HDD interfaces allowed the host to specify track/head, leading to much effort on disk scheduling; modern drives do not expose these controls and map LBA to physical addresses internally."
What are the current goals of disk scheduling?,"Fairness, timeliness, and optimizations (e.g., bunching sequential reads/writes)."
When do drives perform best?,With sequential I/O.
Is absolute knowledge of head/physical block/cylinder locations possible on modern drives?,Generally not possible.
What approximation is used for LBA to physical address mapping on modern drives?,"Increasing LBAs generally mean increasing physical addresses, and close LBAs equate to physical proximity."
"What is the simplest disk scheduling algorithm, and what are its alternative names?","First-Come, First-Served (FCFS) or FIFO."
Is FCFS disk scheduling fair?,"Yes, it is intrinsically fair."
Does FCFS provide the fastest service?,"Generally, no."
"What is a common problem with FCFS disk scheduling, especially illustrated by a ""wild swing"" example?","It can lead to inefficient head movement (e.g., a ""wild swing"" where the head moves far for one request, then far back for another, then far forward again), not optimizing for seek time."
Define the SCAN algorithm for HDD I/O scheduling.,"The disk arm starts at one end, moves to the other, servicing requests, and when it reaches the other end, its direction is reversed, and servicing continues."
What is another name for the SCAN algorithm?,Elevator algorithm.
"In the SCAN algorithm, what happens to a request just in front of the head?",It is serviced almost immediately.
"In the SCAN algorithm, what happens to a request just behind the head?","It waits until the arm moves to the end, reverses, and comes back."
"With a uniform distribution of requests, where is the heaviest density of requests in SCAN, and where are there few requests after reversal?",Few requests are immediately in front of the head after reversal (recently serviced); the heaviest density of requests is at the other end of the disk (where requests have waited longest).
Define Circular SCAN (C-SCAN) scheduling.,"A variant of SCAN where the head moves from one end to the other, servicing requests, but upon reaching the other end, it immediately returns to the beginning of the disk without servicing requests on the return trip."
What is the primary goal or benefit of C-SCAN scheduling compared to SCAN?,To provide more uniform wait time.
How does C-SCAN conceptually treat cylinders?,As a circular list.
Are many disk-scheduling algorithms commonly used?,"No, many exist but are rarely used."
"Is an optimal order definable for any request list, and is its computation always justified?","Yes, an optimal order can be defined, but its computation may not justify the savings over simpler algorithms like SCAN."
What heavily influences the performance of a disk-scheduling algorithm?,The number and types of requests.
How do all disk scheduling algorithms behave if there is only one outstanding request in the queue?,They all behave like FCFS.
Which disk scheduling algorithms are generally better for heavy disk load and less likely to cause starvation?,SCAN and C-SCAN.
Which Linux scheduler was created to prevent starvation?,The deadline scheduler.
What type of queues does the Linux deadline scheduler maintain?,Separate read and write queues.
Why does the Deadline scheduler give reads priority?,Because processes are more likely to block on read operations.
How are the queues in the Deadline scheduler sorted?,"In LBA (Logical Block Addressing) order, which implements a C-SCAN like behavior."
How are I/O requests typically sent in the Deadline scheduler?,"In batches, sorted by LBA order."
"How many queues does the Deadline scheduler specifically keep, and how are they organized?","Four queues: two read queues and two write queues (one of each sorted by LBA, the other by FCFS)."
How does the Deadline scheduler check for and prevent starvation of FCFS requests?,"After each batch, it checks if any FCFS requests are older than a configured age (default 500 ms); if so, the LBA queue (read/write) containing the old request is selected for the next batch."
What was the default I/O scheduler in Linux RedHat 7?,The Deadline I/O scheduler.
"Besides Deadline, what other I/O schedulers were included in RHEL 7?",NOOP and Completely Fair Queueing scheduler (CFQ).
What are the characteristics of the NOOP scheduler and when is it preferred?,"It is preferred for CPU-bound systems using fast storage (NVM devices), as it does minimal work, effectively acting as FCFS."
What type of drives is the Completely Fair Queueing scheduler (CFQ) the default for?,SATA drives.
"How many queues does CFQ maintain, and how are they sorted?","Three queues (insertion sort, LBA order): real time, best effort (default), and idle."
"What are the priorities among CFQ's queues, and what is a potential issue?","Real time has the highest priority, then best effort, then idle (real time > best effort > idle); starvation is possible."
How does CFQ use historical data to optimize?,"It anticipates if a process will issue more I/O requests soon; if so, it idles waiting for new I/O, ignoring other queued requests to minimize seek time, assuming locality of reference per process."
"Define ""bandwidth"" (from table definition).",Total amount of data transferred divided by total time between first request for service and completion of last transfer.
"Define ""SCAN algorithm"" (from table definition).","HDD I/O scheduling algorithm: disk head moves from one end to other, performing I/O; then reverses."
"Define ""Circular SCAN (CSCAN) scheduling"" (from table definition).","HDD I/O scheduling algorithm: disk head moves from one end to other, performing I/O; then returns to beginning without servicing."
"Define ""Completely Fair Queueing (CFQ)"" (from table definition).","In Linux, default I/O scheduler in kernel 2.6 and later."
What is the primary goal of disk-scheduling algorithms for Hard Disk Drives (HDDs)?,To minimize disk head movement.
What distinguishes NVM devices from HDDs in terms of physical components?,NVM devices have no moving disk heads.
What common scheduling policy do NVM devices typically use?,"Simple FCFS (First-Come, First-Served)."
Define the Linux NOOP scheduler.,A Linux NVM scheduling algorithm that uses an FCFS policy and merges adjacent requests.
Describe the characteristic of read service time in NVM devices.,Uniform.
Describe the characteristic of write service time in NVM devices and why it behaves that way.,"Not uniform, due to flash memory properties."
How do some SSD schedulers handle write requests?,They merge only adjacent write requests.
How do some SSD schedulers handle read requests?,They service all read requests in FCFS order.
What are the two main types of I/O access?,Sequential and random.
For which types of devices is sequential access I/O optimal?,Mechanical devices such as HDDs and tapes.
Why is sequential access optimal for mechanical devices?,"Because the data is located near the read/write head, minimizing movement."
Define Input/Output Operations Per Second (IOPS).,"A measure of random access I/O performance, representing the number of input and output operations per second."
What effect does random access I/O have on HDDs?,It causes disk head movement.
Compare random access I/O performance between NVM (SSDs) and HDDs.,"Random access I/O is much faster on NVM (e.g., HDD hundreds of IOPS, SSD hundreds of thousands of IOPS)."
Do NVM devices offer a significant advantage for raw sequential throughput compared to HDDs?,"Less advantage, because HDD head seeks are minimized during sequential operations."
What is the performance advantage of NVM over HDD for sequential reads?,Equivalent to a 10x advantage.
How does writing to NVM compare to reading from NVM in terms of speed?,Writing is slower than reading.
How does writing impact NVM's performance advantage over HDDs?,It decreases the advantage.
Describe the consistency of write performance for HDDs throughout their lifespan.,Consistent throughout device life.
What factors cause NVM write performance to vary?,"Device fullness (due to garbage collection and over-provisioning) and ""wear""."
How does the performance of a worn NVM device compare to a new one?,Much worse performance.
How can NVM lifespan and performance be improved through file system interaction?,"The file system informs the NVM device when files are deleted, allowing the device to erase blocks."
Under what general conditions does garbage collection typically occur in an NVM device?,"When the NVM device is under a random read/write load, is full, but has free space."
What is the primary purpose of garbage collection in NVM devices?,To reclaim space from invalid data.
Describe the sequence of operations a single write might trigger due to garbage collection in an NVM device.,"Read of pages, write of good data to overprovisioning space, erase of an all-invalid-data block, and placement of the block into overprovisioning space."
Summarize the I/O operations triggered by a single write request due to garbage collection.,One page write (for the new data) + one or more page reads (by GC) + one or more page writes (for good data from GC blocks).
Define write amplification.,"Creation of I/O requests by NVM devices (GC, space management), impacting write performance."
What is the worst-case scenario for write amplification?,Several extra I/Os are triggered with each write request.
What is the fundamental importance of error detection and correction?,"They are fundamental to memory, networking, and storage."
Define: Error detection.,"Determining if a problem has occurred (e.g., data corruption)."
What actions can a system take upon detecting an error?,"Halt operation, report the error, or warn of a failing/failed device."
How do memory systems typically detect errors?,Using parity bits.
"What is a parity bit, and what is its association?","A bit associated with each byte, recording the even or odd number of 1s within that byte."
How does parity detect a single-bit error?,The calculated parity changes and does not match the stored parity.
How is an error detected if the stored parity bit itself is damaged?,It will not match the newly computed parity.
What types of errors are guaranteed to be detected by parity?,All single-bit errors.
What is a limitation of parity regarding multiple-bit errors?,Double-bit errors might go undetected.
How is a parity bit calculated?,By XORing the bits of the associated data.
What is the memory overhead required for parity?,An extra bit of memory per byte.
What general term encompasses parity as a form of error detection?,Checksums.
Define: Checksum.,A general term for an error detection and correction code.
How do checksums operate to detect errors?,"They use modular arithmetic to compute, store, and compare values on fixed-length words."
What is another error-detection method besides parity?,Cyclic redundancy check (CRCs).
Define: Cyclic redundancy check (CRCs).,An error-detection method using a hash function to detect multiple-bit errors.
Define: Error-correcting code (ECC).,"A value calculated from data bytes, recalculated later to check for changes, and capable of correcting errors."
How does ECC perform error correction?,It uses algorithms and extra storage.
What factors cause ECC codes to vary?,The extra storage needed and the number of errors correctable.
In what unit is ECC typically applied in disk drives?,Per-sector ECC.
In what unit is ECC typically applied in flash drives?,Per-page ECC.
Describe the ECC process during a write operation.,"When a controller writes a sector/page, the ECC is calculated from the data and written along with it."
Describe the ECC process during a read operation.,"When a sector/page is read, the ECC is recalculated from the data and compared with the stored ECC value."
What does an ECC mismatch during a read operation indicate?,"The data is corrupted, and the storage media may be bad."
How is ECC able to identify and correct corrupted bits?,"It contains enough information to identify changed bits and calculate correct values, provided only a few bits are corrupted."
Define: Soft error.,"A recoverable error, often by retrying the operation."
When does ECC signal a non-correctable error?,When there are too many changes for ECC to correct.
Define: Hard error.,"An unrecoverable error, possibly resulting in data loss."
Who is responsible for performing ECC processing on read/write operations?,The controller automatically.
How do error detection and correction influence product markets?,They frequently serve as differentiators between consumer and enterprise products.
What are additional applications of ECC in some systems?,DRAM error correction and data path protection.
What are the OS responsibilities related to storage device management?,"Drive initialization, booting from drive, and bad-block recovery."
What is the initial state of a new storage device?,"A blank slate (e.g., platter of magnetic material, uninitialized semiconductor cells)."
What must happen to a storage device before data can be stored on it?,It must be divided into sectors that the controller can read/write.
What must happen to NVM pages before data can be stored?,They must be initialized and an FTL (Flash Translation Layer) created.
What is the process of dividing a device into sectors or initializing NVM pages?,Low-level formatting or physical formatting.
What is low-level formatting?,The process that fills a device with a special data structure for each storage location.
What components typically make up the data structure for a sector or page?,"A header, a data area, and a trailer."
What information is contained in the header and trailer of a sector/page data structure?,"Controller information, such as sector/page number and error detection/correction code."
When are most drives low-level formatted?,"At the factory, as part of the manufacturing process."
What does factory low-level formatting enable the manufacturer to do?,Test the device and initialize the mapping from logical block numbers to defect-free sectors/pages.
What are the common choices for sector sizes?,512 bytes and 4KB.
What is the advantage of a larger sector size?,"Fewer sectors per track, fewer headers/trailers, and more space available for user data."
How do some OSes handle sector sizes?,They may only handle one specific sector size.
What are the three steps an OS typically takes to record its own data structures on a device?,"1. Partitioning, 2. Volume creation and management, 3. Logical formatting (or creation of a file system)."
What is partitioning (storage device management)?,Dividing a device into one or more groups of blocks/pages.
What is the purpose of partitioning?,To allow the OS to treat each partition as a separate device.
Give examples of how partitions might be used.,"One partition for OS executable code (file system), another for swap space, and another for user files."
How do some OS/file systems handle partitioning?,They partition automatically when the entire device is managed by the file system.
Where is partition information typically stored?,In a fixed format at a fixed location on the storage device.
Which Linux command manages partitions?,`fdisk`.
What happens after an OS recognizes a device and reads its partition info?,"The OS creates device entries (e.g., `/dev` in Linux)."
Which configuration file tells the OS to mount partitions in Linux?,`/etc/fstab`.
What is mounting a file system?,Making it available for use by logically attaching it to the root file system.
How is a volume implicitly created?,"When a file system is placed directly within a partition, making it ready to be mounted."
How are volumes explicitly created?,"When multiple partitions/devices are used as a RAID set, allowing one or more file systems to spread across devices."
Which Linux tool provides volume management features?,`lvm2`.
Which file system integrates volume management?,ZFS.
"What can the term ""volume"" also refer to in a broader sense?","Any mountable file system, such as a CD image file."
What is logical formatting (storage management)?,"The process where the OS stores initial file-system data structures onto the device, or creation of a file system."
What data structures are created during logical formatting?,Maps of free/allocated space and an initial empty directory.
What does partition information indicate regarding booting?,If a partition contains a bootable file system (OS).
What is established by a partition labeled for boot?,The root of the file system.
What happens once the boot partition is mounted?,Device links for all other devices/partitions are created.
"What constitutes a computer's ""file system""?",All mounted volumes.
How are Windows file systems typically named?,"Separately via letters (e.g., C:, D:, E:)."
How are Linux file systems typically structured?,"The boot file system is mounted at boot, and other file systems are mounted within its tree structure."
What is the difference in file system interfaces between Windows and Linux regarding device usage?,"In Windows, the file system interface is clear when a device is used. In Linux, a single file access might traverse many devices."
What larger chunks do file systems group blocks into?,Clusters.
How does device I/O differ from file system I/O?,"Device I/O occurs via blocks, while file system I/O occurs via clusters."
What is the benefit of grouping blocks into clusters for I/O?,It assures more sequential-access and fewer random-access characteristics.
How do file systems optimize for HDD head seeks?,They group file contents near metadata to reduce head seeks.
What is it called when special programs use a partition as a large sequential array of logical blocks without file-system data structures?,Raw disk.
What is the I/O type for a raw disk?,Raw I/O.
What are common use cases for raw disk access?,Swap space and some database systems that need to control exact record location.
What file-system services does raw I/O bypass?,"Buffer cache, file locking, prefetching, space allocation, file names, and directories."
What is the purpose of allowing applications to use raw partitions?,To implement their own special-purpose storage services.
How do most applications interact with storage devices?,They use the provided file system.
How does Linux provide access similar to raw I/O?,Via the `DIRECT` flag to the `open()` system call.
What must a computer have when it starts (powered up/rebooted)?,An initial program to run.
What is the initial bootstrap loader like and where is it stored?,It tends to be simple and is stored in NVM (Non-Volatile Memory) flash memory firmware on the motherboard.
Where is the bootstrap NVM firmware mapped?,To a known memory location.
What does the bootstrap NVM firmware initialize?,"CPU registers, device controllers, and main memory contents."
What does the tiny bootstrap loader do?,It brings in the full bootstrap program from secondary storage.
Where is the full bootstrap program stored?,"In ""boot blocks"" at a fixed location on the device."
What is a device with a boot partition called?,A boot disk or system disk.
What does the bootstrap NVM code instruct the storage controller to do?,"Read boot blocks into memory (without device drivers loaded), then execute the code."
How does the full bootstrap program differ from the initial bootstrap loader?,It is more sophisticated; it loads the entire OS from a non-fixed location and starts the OS.
"In Windows, what does the boot partition contain?",The OS and device drivers.
What is the Windows boot code in the first logical block of a hard disk/NVM device termed?,Master Boot Record (MBR).
What runs when Windows booting begins?,Code in the system's firmware.
What does the firmware code do during Windows boot?,Directs the system to read boot code from the MBR (as it understands enough about the controller/device to load the sector).
What does the MBR contain?,"Boot code, a table listing partitions, and a flag for the boot partition."
What happens after the system identifies the boot partition during Windows boot?,"It reads the first sector/page, known as the boot sector."
What does the boot sector do in the Windows boot process?,Directs to the kernel.
What happens after the kernel is loaded in the Windows boot process?,"The boot process continues, loading subsystems and system services."
Why are disks prone to failure?,Due to moving parts and small tolerances.
What happens in case of complete disk failure?,"The disk is replaced, and contents are restored from backup."
What type of disk failure is more frequent than complete failure?,One or more sectors becoming defective.
Do new disks come with defects?,Most disks come from the factory with bad blocks.
"How were bad blocks handled on older disks (e.g., IDE controllers)?",Manually.
What was the manual strategy for handling bad blocks during formatting?,"Scanning the disk for bad blocks, then flagging discovered bad blocks as unusable so the file system wouldn't allocate them."
How were blocks that went bad during operation handled manually?,"A special program (e.g., Linux `badblocks`) was run manually to search for and lock away the bad blocks."
What typically happened to data on bad blocks that were manually locked away?,The data was usually lost.
How do more sophisticated disks handle bad-block recovery?,They are smarter; the controller maintains a list of bad blocks.
When is the controller's list of bad blocks initialized and updated?,Initialized at the factory and updated over the life of the disk.
What does low-level formatting do on sophisticated disks to aid bad-block recovery?,It sets aside spare sectors that are not visible to the OS.
What does the controller do when it encounters a bad sector on a sophisticated disk?,It logically replaces each bad sector with a spare sector.
What are the names for the scheme where a controller replaces a bad sector with a spare sector?,Sector sparing or forwarding.
Describe the steps in a typical bad-sector transaction with sector sparing.,"1. OS tries to read logical block 87. 2. Controller calculates ECC, finds sector bad, reports I/O error to OS. 3. Device controller replaces bad sector with a spare. 4. After that, any system request for logical block 87 is translated to the replacement sector's address by the controller."
What is a potential issue with bad sector redirection by the controller?,It could invalidate OS disk-scheduling optimization.
Where are spare sectors typically allocated on most disks?,"A few spare sectors in each cylinder, and a spare cylinder."
"When a bad block is remapped, where does the controller try to use a spare sector from first?",From the same cylinder if possible.
What is an alternative to sector sparing?,Sector slipping.
How does sector slipping work?,"It remaps all sectors from a defective one onwards down one spot (e.g., if logical block 17 is defective, it remaps all sectors from 17 to 202, moving them down one spot, freeing up space of sector 18, and mapping sector 17 to it)."
What might recoverable soft errors trigger?,"Device activity, such as copying block data or sparing/slipping the block."
What is the consequence of an unrecoverable hard error?,"Lost data. The file using the block must be repaired (e.g., from backup), which requires manual intervention."
What can become nonfunctional or go bad in NVM devices?,"Bits, bytes, or pages."
Why is bad block management simpler for NVM devices compared to HDDs?,There is no seek time performance loss.
How are bad pages handled in NVM devices?,"Multiple pages are set aside as replacement locations, or space from an over-provisioning area is used."
What does the NVM controller do regarding bad pages?,It maintains a table of bad pages and never sets them as available to write to.
low-level formatting,Initialization of a storage medium for computer storage.
physical formatting,Initialization of a storage medium for computer storage.
partition,Logical segregation of storage space into multiple areas.
mounting,Making a file system available for use by logically attaching it to the root file system.
volume,Container of storage; often a device with a mountable file system.
logical formatting,Creation of a file system in a volume to ready it for use.
cluster,"In Windows storage, a power-of-2 number of disk sectors collected for I/O optimization."
raw disk,Direct access to a secondary storage device as an array of blocks with no file system.
bootstrap,Steps taken at computer power-on to bring system to full operation.
boot disk,Disk with a boot partition and kernel to load for booting.
system disk,"Storage device with a boot partition, can store OS for booting."
boot partition,Storage device partition containing an executable operating system.
master boot record (MBR),"Windows boot code, stored in the first sector of a boot partition."
boot sector,"First sector of a Windows boot device, containing bootstrap code."
bad block,Unusable sector on an HDD.
sector sparing,Replacement of unusable HDD sector with another sector elsewhere on device.
sector slipping,Renaming of sectors to avoid using a bad sector.
What is swapping?,The process of moving entire processes between secondary storage and main memory.
When does swapping typically occur?,"When physical memory is critically low, processes are moved to swap space to free memory."
How do modern operating systems typically implement swapping?,"Modern OS combine swapping with virtual memory, swapping individual pages rather than entire processes."
"Are the terms ""swapping"" and ""paging"" used interchangeably in modern OS contexts?",Yes.
Define swap-space management.,A low-level OS task of managing space on secondary storage for swapping and paging.
How does virtual memory utilize secondary storage?,As an extension of main memory.
What is the impact of swap space on system performance?,It significantly decreases system performance because drive access is much slower than main memory access.
What is the main goal for swap space design and implementation?,To provide the best throughput for the virtual memory system.
How does swap space usage vary between operating systems?,It is used differently by various OS depending on their memory-management algorithms.
What do traditional swapping systems hold in swap space?,The entire process image (code and data segments).
What do paging systems store in swap space?,Pages that have been pushed out of main memory.
What is the typical range for the amount of swap space needed?,It varies from a few megabytes (MB) to gigabytes (GB).
What factors determine the amount of swap space needed?,"Physical memory, virtual memory backing, and virtual memory usage."
Is it safer to overestimate or underestimate the amount of swap space required?,It is safer to overestimate.
What are the consequences of running out of swap space?,The system may abort processes or crash.
What is the consequence of overestimating swap space?,It wastes secondary storage space but causes no other harm.
How did Solaris traditionally determine the recommended swap space amount?,Swap space was set equal to the virtual memory exceeding pageable physical memory.
What was the past Linux recommendation for swap space amount?,Double the physical memory.
Why has the Linux recommendation for swap space changed today?,Today's Linux uses considerably less swap space due to changes in paging algorithms.
"Can some operating systems, like Linux, utilize multiple swap spaces?","Yes, they can allow multiple swap spaces, which can be files or dedicated partitions."
Where are multiple swap spaces typically located when used?,Usually on separate storage devices.
What is the purpose of having multiple swap spaces on separate storage devices?,To spread the I/O load from paging/swapping over the system's I/O bandwidth.
What are the two primary locations where swap space can reside?,Carved out of a normal file system (as a large file) or in a separate raw partition.
How is swap space managed when it resides as a file within a normal file system?,"Normal file-system routines are used to create, name, and allocate its space."
Define raw partition.,A partition within a storage device not containing a file system.
What is a key characteristic of swap space located in a raw partition regarding file systems?,It has no file system or directory structure.
Who manages block allocation and deallocation for swap space in a raw partition?,A separate swap-space storage manager.
What is the primary optimization goal for the swap-space manager in a raw partition?,"Speed, not storage efficiency."
How does the frequency of access to swap space compare to file systems?,Swap space is accessed more frequently than file systems.
"What potential issue may increase with swap space in raw partitions, and why is it considered an acceptable trade-off?","Internal fragmentation may increase, which is acceptable because the data life in swap space is shorter."
How is fragmentation in raw swap partitions typically managed or resolved?,It is short-lived because the swap space is reinitialized at boot time.
How is the amount of swap space determined when using the raw-partition approach?,It is a fixed amount determined during disk partitioning.
How can additional swap space be added when using the raw-partition approach?,By repartitioning the device (which might involve moving or destroying other partitions) or by adding another swap space elsewhere.
Are all operating systems rigid in their swap space location choices?,"No, some OS (like Linux) are flexible and can swap in both raw partitions and file-system space."
What is the trade-off involved when choosing between file system-based swap space and raw partitions?,The convenience of file system allocation/management versus the performance of raw partitions.
How did traditional UNIX kernels manage swapping?,They copied entire processes between contiguous disk regions and memory.
How did later UNIX systems evolve their swapping mechanisms with the advent of paging hardware?,They evolved to a combination of swapping and paging.
What significant change did Solaris 1 introduce regarding standard UNIX swap methods?,It changed them for improved efficiency and to adapt to technological developments.
"In systems like Solaris, what happens to text-segment pages (code) during execution and if selected for pageout?","They are brought from the file system, accessed in memory, and then thrown away if selected for pageout."
Why is it more efficient to reread a text-segment page from the file system than to write it to swap and then reread it?,It is more efficient to reread the page from the file system than to write it to swap and then reread it.
For what type of memory is swap space primarily used as a backing store in modern systems like Solaris and Linux?,Pages of anonymous memory.
Define anonymous memory.,Memory not associated with a file; dirty pages paged out are stored in swap space.
What was a significant change in later Solaris versions regarding swap space allocation timing?,"It allocates swap space only when a page is forced out of physical memory, not when the virtual memory page is first created."
Why does the later Solaris swap allocation scheme offer better performance on modern computers?,"Because modern computers have more physical memory, leading to less frequent paging."
How is Linux's use of swap space similar to Solaris's?,"Linux, like Solaris, uses swap space primarily for anonymous memory."
What types of swap areas does Linux allow?,"One or more swap areas, which can be a swap file on a regular file system or a dedicated swap partition."
"What does each swap area in Linux consist of, and what do these components hold?","A series of 4-KB page slots, which hold swapped pages."
Define page slot (in Linux swap-space management).,"In Linux swap-space management, a part of the data structure tracking swap-space use."
What data structure is associated with each swap area in Linux?,"A swap map, which is an array of integer counters."
Define swap map (in Linux swap-space management).,"In Linux swap-space management, a part of the data structure tracking swap-space use."
"In a Linux swap map, what does each counter correspond to?",A page slot.
"In a Linux swap map, what does a counter value of 0 indicate?",The corresponding page slot is available.
"In a Linux swap map, what does a counter value greater than 0 indicate?",The corresponding page slot is occupied by a swapped page.
What specific information does the value of a counter in a Linux swap map provide?,"It indicates the number of mappings to the swapped page (e.g., 3 means it's mapped to 3 processes if it's shared memory)."
What are the three main ways computers access secondary storage?,"Host-attached, network-attached, and cloud storage."
Define host-attached storage.,"Storage accessed through local I/O ports, directly attached to a computer."
What is the most common local I/O port for host-attached storage?,SATA (Serial Advanced Technology Attachment).
How can more host-attached storage be accessed beyond a few SATA ports?,"Through an individual storage device, a device in a chassis, or multiple drives in a chassis connected via USB, FireWire, or Thunderbolt."
What type of I/O architectures do high-end workstations and servers use for extensive host-attached storage?,Sophisticated I/O architectures for more or shared storage.
Define Fibre Channel (FC).,A high-speed serial architecture (using optical fiber or copper cable) used as a storage I/O bus in data centers to connect computers to storage arrays.
What are the benefits of Fibre Channel (FC)?,"Large address space, switched communication, and flexibility in I/O communication by allowing multiple hosts and storage devices to attach to the fabric."
List examples of devices suitable for host-attached storage.,"HDDs, NVM devices, CD/DVD/Blu-ray/tape drives, and Storage-Area Networks (SANs)."
How are I/O commands for host-attached storage directed?,Reads/writes of logical data blocks are directed to specifically identified storage units (via bus ID or target logical unit).
Define Network-Attached Storage (NAS).,"Storage that provides access across a network, typically accessed from a computer over a network."
What can a NAS device be?,A special-purpose storage system or a general computer system providing storage to other hosts.
How do clients access NAS?,Via a remote-procedure-call (RPC) interface.
Name common RPC interfaces used for NAS and their typical operating systems.,NFS (UNIX/Linux) and CIFS (Windows).
How are NAS RPCs typically carried?,"Via TCP/UDP over an IP network, usually the same LAN."
How is a NAS unit usually implemented?,As a storage array with RPC interface software.
What important feature do CIFS and NFS provide for file sharing on NAS?,"Locking features, allowing file sharing between hosts accessing the NAS."
What is a key convenience benefit of NAS for LAN computers?,It provides a convenient way for LAN computers to share a storage pool due to ease of naming and access.
What is a downside of NAS compared to some direct-attached storage?,It is less efficient and offers lower performance.
Define iSCSI.,The latest network-attached storage protocol that uses the IP network protocol to carry the SCSI protocol for distant storage access.
How does iSCSI enable hosts to treat storage?,"Hosts treat storage as directly attached, even if physically distant, by using networks (not SCSI cables) as interconnects between hosts and storage."
What is the key difference in data transfer between NFS/CIFS and iSCSI?,"NFS/CIFS present a file system and send parts of files, while iSCSI sends logical blocks across the network, allowing the client to use blocks directly or create a file system."
Define cloud storage.,"Similar to network-attached storage, it is accessed across a network, specifically over the Internet/WAN to a remote, shared data center, often for a fee or free."
What is a primary difference between cloud storage and NAS in terms of location?,"Unlike NAS, cloud storage is accessed over the Internet/WAN to a remote data center."
What is a primary difference between cloud storage and NAS in terms of how storage is accessed/presented?,"NAS is accessed as another file system (CIFS/NFS) or raw block device (iSCSI), with the OS integrating these protocols. Cloud storage is API based, requiring programs to use APIs for access."
Provide examples of cloud storage services.,"Amazon S3, Dropbox, Microsoft OneDrive, Apple iCloud."
Why do cloud storage services primarily use APIs instead of existing NAS protocols?,Due to WAN latency and the higher likelihood of failure scenarios. NAS protocols are designed for LANs with lower latency and less connectivity loss.
"How do LAN connection failures (e.g., with NFS/CIFS) typically affect a system?",The system might hang.
How does cloud storage typically handle failures?,"Failures are more likely, and the application pauses access until connectivity is restored."
"What is a major drawback of Network-Attached Storage (NAS), especially in large client-server installations?","Storage I/O consumes data network bandwidth and increases network communication latency, leading to server-client communication competing with server-storage communication."
Define Storage-Area Network (SAN).,"A private local-area storage network that connects servers and storage units using storage protocols (not general networking protocols), allowing multiple computers to connect to storage devices."
What is a key advantage or 'power' of a SAN?,Flexibility.
How is storage allocated in a SAN?,Storage is dynamically allocated to hosts that are attached to the SAN.
What types of drives can storage arrays in a SAN contain?,RAID protected or unprotected drives (Just a Bunch of Disks (JBOD)).
Define Just a Bunch of Disks (JBOD).,Unprotected drives in a storage array.
What is the function of a SAN switch?,It allows or prohibits access between hosts and storage.
How do SANs facilitate sharing among server clusters?,"Clusters of servers can share the same storage, and storage arrays can include multiple direct host connections."
Compare SANs to storage arrays in terms of ports and cost.,SANs typically have more ports and higher cost than storage arrays.
What are typical characteristics of SAN connectivity?,"Short distances, typically no routing. NAS can have more connected hosts than SAN."
Define a storage array.,"A purpose-built device that includes drives to store data and one or more controllers to manage storage and access. It can include SAN or network ports, or both."
What components and functions do storage array controllers typically have?,"CPUs, memory, and software that implement array features such as network protocols, UIs, RAID, snapshots, replication, compression, deduplication, and encryption."
How do storage arrays utilize SSDs?,"Some arrays include SSDs for maximum performance (though smaller capacity if only SSDs), or they mix SSDs and HDDs, using SSDs as cache or for selected data while HDDs serve as bulk storage."
What is the most common interconnect for SANs?,Fibre Channel (FC).
Which SAN interconnect is increasing in use due to its simplicity?,iSCSI.
Define InfiniBand (IB).,"A special-purpose bus architecture that provides hardware and software support for high-speed interconnection networks between servers and storage units, also used as a SAN interconnect."
What is the primary purpose of Redundant Arrays of Independent Disks (RAIDs)?,"RAIDs are disk-organization techniques used to improve data read/write rates through parallel operation and to enhance data storage reliability through redundant information, preventing data loss upon a single drive failure."
Define Redundant Arrays of Independent Disks (RAID).,"RAID is a disk organization technique where two or more storage devices work together, usually with protection from device failure."
How has the meaning of the 'I' in RAID changed historically?,"Originally, 'I' stood for 'inexpensive' as RAIDs were a cost-effective alternative to large, expensive disks. Today, it stands for 'independent'."
What is Mean Time Between Failures (MTBF)?,MTBF is the statistical mean time a device is expected to work correctly before failing.
Why is redundancy introduced in storage systems like RAID?,"Redundancy is introduced to store extra information not normally needed, which is then used to rebuild lost information if a disk fails, preventing data loss."
Define Mirroring in the context of RAID.,"Mirroring is a storage RAID protection scheme where two physical devices contain the same content; if one fails, data is read from the other."
What is a Mirrored Volume?,"A mirrored volume is a logical disk composed of two physical drives, where every write operation is performed on both drives."
What factors determine the Mean Time to Data Loss for a mirrored volume?,The Mean Time to Data Loss for a mirrored volume depends on the MTBF of individual drives and the mean time to repair (average time to replace a failed drive and restore data).
Define Mean Time to Repair.,Mean time to repair is the statistical mean of time required to replace a failed drive and restore data.
Define Mean Time to Data Loss.,Mean time to data loss is the statistical mean of the time until data is irrecoverably lost in a redundant storage system.
How do RAID systems mitigate inconsistent states caused by power failures during writes?,"Solutions include writing one copy first then the next, or adding a solid-state nonvolatile cache (NVRAM) to the RAID array, which protects write-back cache data from loss (assuming the cache itself has error protection/correction)."
How does mirroring improve performance in a RAID system?,"Mirroring doubles the read request rate because reads can be sent to either drive. While the transfer rate per individual read remains the same as a single drive, the number of reads per unit time is doubled."
What is Data Striping?,Data striping is the technique of splitting data across multiple drives to improve the overall transfer rate.
Define Bit-Level Striping.,"Bit-level striping is the simplest form of data striping where bits of each byte are split and stored on separate drives. For example, in an 8-drive array, bit 'i' of each byte would go to drive 'i'."
Define Block-Level Striping.,"Block-level striping is a form of data striping where blocks of a file are distributed across multiple drives. With 'n' drives, block 'i' of a file typically goes to drive (i modulo n) + 1."
What are the two main goals of parallelism via striping in storage systems?,"The goals are to increase the throughput of multiple small accesses (e.g., page accesses) by load balancing, and to reduce the response time of large accesses by reading from all disks in parallel."
What are RAID Levels?,"RAID levels are classifications for various disk-organization schemes that offer different cost-performance trade-offs by combining striping with various methods of redundancy (e.g., parity bits or mirroring)."
Describe RAID Level 0.,RAID Level 0 consists of drive arrays with block-level striping but no redundancy. It offers high performance but no data protection against drive failure.
Describe RAID Level 1.,RAID Level 1 is drive mirroring. It provides high reliability by duplicating data but is expensive due to doubling the required storage.
Describe the core concept of RAID Level 4.,"RAID Level 4 uses a memory-style Error-Correcting-Code (ECC) organization. Data blocks are striped across N drives, and an ECC calculation result (parity block) is stored on a dedicated N+1 drive. It allows recovery from a single drive failure."
What is a Read-Modify-Write Cycle in the context of RAID?,"A read-modify-write cycle occurs when a write operation is smaller than a full block. The entire block must be read, modified with the new data, and then written back along with an updated parity block."
"What are the advantages of RAID Level 4 over RAID Level 1, given equal data protection?",RAID 4 offers reduced storage overhead (one parity drive for several data drives vs. a mirror for every drive) and faster transfer rates for reads/writes of block series due to N-way striping.
"What is the main performance problem with RAID Level 4 and other parity-based RAIDs, and how is it mitigated?","The main performance problem is the expense of computing and writing XOR parity, which can slow down small writes (requiring a read-modify-write cycle). This is mitigated by fast modern CPUs, dedicated parity hardware in controllers, and NVRAM caches that buffer writes and avoid most read-modify-write cycles."
Describe RAID Level 5.,"RAID Level 5 is characterized by block-interleaved distributed parity. Instead of a single dedicated parity drive (like RAID 4), data and parity blocks are spread among all N+1 drives, avoiding the performance bottleneck of a single parity drive."
What is the P + Q redundancy scheme in RAID?,"The P + Q redundancy scheme, used in RAID Level 6, involves storing extra redundant information beyond simple XOR parity. It typically uses advanced error-correcting codes, like Galois field math, to calculate two distinct parity blocks (P and Q) for a set of data blocks, enabling tolerance of up to two drive failures."
What is Galois field math used for in RAID?,"Galois field math is used in RAID Level 6 (P + Q redundancy) to calculate the second independent parity block (Q), allowing the system to tolerate two drive failures by using advanced error-correcting codes."
What is Multidimensional RAID Level 6?,"Multidimensional RAID Level 6 amplifies the protection of standard RAID 6 by logically arranging hundreds of drives into rows and columns and implementing RAID 6 both horizontally (across rows) and vertically (down columns), allowing recovery from multiple failures in any location."
What are RAID Levels 0+1 and 1+0?,RAID 0+1 (often called RAID 10) and RAID 1+0 (often called RAID 01) are combined RAID levels that merge the performance benefits of RAID 0 (striping) with the reliability benefits of RAID 1 (mirroring). They are expensive as they double the number of drives needed.
What is the key difference in resilience between RAID 0+1 and RAID 1+0 after a single drive failure?,"In RAID 0+1, a single drive failure makes the entire striped set (sub-array) inaccessible, leaving only the mirrored copy available. In RAID 1+0, a single drive failure makes only that specific drive unavailable, with its mirror taking over, and the rest of the striped drives remain available, offering better resilience."
List common implementations of RAID.,"RAID can be implemented via: Volume-management software (kernel/system layer), Host Bus Adapter (HBA) hardware, Storage array hardware, or at the Storage Area Network (SAN) interconnect layer via drive virtualization devices."
Define Snapshot in file systems.,"A snapshot in file systems is a read-only view of a file system at a particular point in time, showing its state before the last update."
Define Replication in file systems.,"Replication in file systems is the automatic duplication and synchronization of data over a network to another system, primarily for redundancy and disaster recovery."
What is the difference between synchronous and asynchronous replication?,"Synchronous replication requires a block to be written both locally and remotely before the write is considered complete, ensuring no data loss. Asynchronous replication groups writes and sends them periodically, which is faster and has no distance limitations, but may incur data loss if the primary site fails before writes are replicated."
What is a Hot Spare drive?,A hot spare is an unused storage device that is configured as an automatic replacement for a failed drive in a RAID set. It is not used for data storage but is ready to be used to recover data and reestablish the RAID level without human intervention.
Why is rebuild performance an important consideration when selecting a RAID level?,"Rebuild performance, the time it takes to rebuild data if a drive fails, is crucial for systems requiring continuous data supply (e.g., high-performance databases). It also influences the overall Mean Time Between Failures (MTBF) of the system."
Which RAID level is easiest for rebuilding after a drive failure?,"RAID Level 1 (mirroring) is the easiest for rebuilding, as data can simply be copied from the healthy mirrored drive."
For what type of applications is RAID Level 0 typically used?,"RAID Level 0 is used in high-performance applications where data loss is not critical, such as scientific computing, due to its lack of redundancy."
For what type of applications is RAID Level 1 typically used?,"RAID Level 1 is popular for applications requiring high reliability with fast recovery, such as small databases."
What is the primary disadvantage of RAID Level 1?,"The primary disadvantage of RAID Level 1 is its high space overhead, as it requires double the storage capacity due to mirroring."
Which RAID level is often preferred for moderate data volumes?,"RAID Level 5 is often preferred for moderate data volumes, balancing performance, protection, and storage efficiency."
Which RAID levels are most common in storage arrays and why?,RAID Level 6 and Multidimensional RAID 6 are most common in storage arrays because they offer good performance and protection (tolerating multiple drive failures) without incurring the large space overhead of mirroring.
What is unique about how the InServ storage array (HP 3Par) handles RAID?,"The InServ storage array does not require drives to be configured at a specific RAID level. Instead, each drive is broken into 256-MB 'chunklets,' and RAID is applied at the chunklet level. A single drive can participate in multiple, various RAID levels across different volumes."
"Define Utility Storage, as featured in the InServ array.","Utility storage is an InServ feature that allows an administrator to configure a host with a large logical storage space, but initially allocate only a small amount of physical storage. As the host uses more storage, unused drives (or 'chunklets' in InServ's case) are dynamically allocated up to the original logical level, without the file system noticing changes."
List types of problems that RAID does NOT protect against.,"RAID does not protect against logical errors such as wrong file pointers, wrong pointers within file structures, incomplete writes ('torn writes'), accidental overwrites of file system structures, or total data loss due to hardware RAID controller failure or software RAID bugs."
What is Solaris ZFS?,"Solaris ZFS is an advanced file system, first included as part of Solaris, known for its innovative approach to data integrity using checksums and its integrated file system and volume management capabilities."
How does Solaris ZFS enhance data consistency and error correction?,"ZFS maintains internal checksums for all blocks (data and metadata), storing these checksums with the pointers to the blocks (e.g., in the inode for data blocks). If a block's checksum is incorrect, ZFS detects the problem and, if mirrored, automatically updates the bad block with the good one, providing higher consistency, error detection, and error correction than standard RAID or file systems."
Define Inode in file systems.,"An inode (index node) is a per-file data structure in many file systems that holds most of the metadata of the file, such as permissions, ownership, timestamps, and pointers to the file's data blocks."
What is a Pool in Solaris ZFS?,"In ZFS, a pool is a logical grouping of drives, partitions, or RAID sets that can contain one or more ZFS file systems. The entire pool's free space is available to all file systems within that pool, providing flexible storage allocation."
How does ZFS address the flexibility limitations of traditional RAID and volume management?,"ZFS combines file system management and volume management, allowing drives or partitions to be gathered into flexible 'pools' of storage. File systems within a pool can dynamically use the pool's free space without artificial limits, relocation, or resizing, avoiding the need to pre-allocate fixed-size volumes."
What is Object Storage?,"Object storage is an alternative data storage approach, typically computer-oriented and designed for programs, where data is placed as self-describing 'objects' into a flat storage pool. Objects are accessed, created, or deleted via unique object IDs rather than through a traditional navigable file system hierarchy."
How does Object Storage differ from traditional file systems?,"Unlike file systems, object storage provides no way to navigate the storage pool or find objects directly; instead, objects are accessed via unique object IDs. It is designed for programs rather than direct user interaction and often runs on commodity hardware without traditional RAID arrays, relying on software for data protection."
Define Hadoop File System (HDFS).,"HDFS is an example of object storage management software, commonly used in Hadoop clusters, which determines where to store objects and manages their protection, often by storing copies of objects on different computers."
Define Ceph.,"Ceph is a brand of object storage management software known for its distributed, scalable object, block, and file storage capabilities."
"What is the primary advantage of Object Storage, especially in large-scale deployments?","The primary advantage of object storage is horizontal scalability, meaning capacity can be increased simply by adding more computers with internal or external disks to the storage pool, allowing for petabytes of storage."
Define Horizontal Scalability.,"Horizontal scalability is the ability to increase capacity (e.g., storage capacity or processing power) by adding more items (such as individual computers or storage nodes) to a system, rather than expanding the capabilities of a single, existing item."
"What is another term for Object Storage, and why is it called that?","Another term for object storage is Content-Addressable Storage, because objects can be retrieved based on their contents or properties rather than a fixed path or name."
Define Unstructured Data in the context of object storage.,"Unstructured data refers to data that does not have a fixed format but is rather free-form, a characteristic often found in objects stored in object storage systems."
List common real-world examples of data stored in object stores.,"Common examples include Google search contents, Dropbox files, Spotify songs, Facebook photos, and data objects for customer applications in cloud computing services like Amazon S3."
What are the major secondary storage I/O units?,Hard disk drives (HDDs) and nonvolatile memory (NVM) devices.
How is modern secondary storage structured?,As large one-dimensional arrays of logical blocks.
List the ways drives can be attached to a computer.,"1. Through local I/O ports on host.
2. Directly connected to motherboards.
3. Through communications network or storage network connection."
What generates requests for secondary storage I/O?,File system and virtual memory system.
What does each secondary storage I/O request specify?,Device address as a logical block number.
What is the purpose of disk-scheduling algorithms for HDDs?,"To improve HDD effective bandwidth, average response time, and variance in response time."
How do SCAN and C-SCAN algorithms improve HDD performance?,Via disk-queue ordering strategies.
How does HDD performance vary with scheduling algorithms?,It varies greatly.
What is a key physical characteristic of Solid-state disks (SSDs)?,They have no moving parts.
How does SSD performance vary among scheduling algorithms?,Performance varies little among algorithms.
What is the typical scheduling strategy used by SSDs?,"Simple FCFS (First-Come, First-Served)."
What is a common outcome of complex data storage and transmission?,Errors frequently result.
Define Error detection.,"Attempts to spot problems, alert the system for corrective action, and avoid error propagation."
Define Error correction.,"Detects and repairs problems, depending on correction data and the amount of corruption."
How are storage devices typically partitioned?,Into one or more chunks of space.
What can each partition on a storage device hold?,A volume or be part of a multidevice volume.
Where are file systems created?,In volumes.
Who manages a storage device's blocks?,The OS (Operating System).
What is the typical state of new storage devices when they arrive?,Typically pre-formatted.
What steps are taken after a device is pre-formatted for use?,"The device is partitioned, and file systems are created."
"What are boot blocks allocated for, if a device contains an OS?",To store the system's bootstrap program.
What action must the system take if a block or page becomes corrupted?,The system must lock out or logically replace it with a spare.
What is key to good performance in some systems regarding memory management?,Efficient swap space.
How do some systems implement swap space?,They dedicate a raw partition to swap space.
How do other systems implement swap space?,They use a file within the file system.
What options do some systems provide for swap space implementation?,"They provide both dedicating a raw partition or using a file within the file system, leaving the decision to the user or administrator."
Why are secondary storage devices in large systems frequently made redundant?,Via RAID algorithms.
What benefits do RAID algorithms provide?,"They allow more than one drive for operation, and enable continued operation/automatic recovery from drive failure."
How are RAID algorithms organized?,Into different levels.
What does each RAID level provide?,A specific combination of reliability and high transfer rates.
What is Object storage used for?,"Big data problems, such as Internet indexing and cloud photo storage."
How are objects in object storage characterized and addressed?,"They are self-defining collections of data, addressed by an object ID (not a file name)."
What is typically used for data protection in object storage?,Replication.
Where do computes based on data happen in object storage systems?,On systems where a copy of the data exists.
What is a key scalability characteristic of object storage?,It is horizontally scalable for vast capacity and easy expansion.
What are a computer's two main jobs?,I/O and computing.
"In many cases, which of a computer's main jobs is considered primary?","I/O is often primary, with computing being incidental (e.g., browsing, editing)."
What is the operating system's role concerning I/O?,To manage and control I/O operations and devices.
What aspect of I/O hardware is studied regarding OS internal facilities?,"I/O hardware basics, specifically constraints on OS internal facilities."
What two main areas concerning OS and applications are covered regarding I/O?,OS I/O services and application I/O interface.
"What ""gap"" is a key topic in I/O study?",Bridging the gap between hardware and application interfaces.
What is the UNIX System V STREAMS mechanism?,A mechanism for dynamic driver code pipelines.
What OS design principles are covered regarding I/O performance?,I/O performance and OS design principles for improvement.
What is a major concern in operating system design related to hardware?,Device control.
Why do I/O devices require varied control methods?,"Due to wide variations in their function and speed (e.g., mouse, hard disk, flash drive, tape robot)."
What OS component is formed by the varied I/O control methods?,The kernel's I/O subsystem.
What is a key benefit of the kernel's I/O subsystem?,It separates the kernel from device management complexities.
What is a trend in I/O device technology regarding software and hardware interfaces?,Increasing standardization of software/hardware interfaces.
What is the benefit of increasing standardization of I/O software/hardware interfaces?,It helps incorporate new device generations.
What is another trend in I/O device technology related to the diversity of devices?,An increasingly broad variety of I/O devices.
What challenge does the increasing variety of I/O devices pose?,"Incorporating new, unlike devices."
How do basic I/O hardware elements accommodate diverse devices?,"Elements like ports, buses, and device controllers are designed to accommodate diverse devices."
How is the kernel structured to manage diverse device details?,With device-driver modules that encapsulate device details.
What do device drivers provide to the I/O subsystem?,A uniform device-access interface.
What is the uniform device-access interface provided by device drivers analogous to?,System calls for applications.
Define: Device driver,An OS component providing uniform access and managing I/O to various devices.
What are the main types of device categories computers operate?,"Storage (disks, tapes), transmission (network, Bluetooth), and human-interface (screen, keyboard, mouse, audio)."
How do devices communicate with a computer?,Via signals over cable or air.
What is a 'port' in I/O hardware?,A connection point for devices to attach to computers.
What is a 'bus' in computer systems?,"A communication system connecting computer components (CPU, I/O devices) for data/command transfer; a set of wires with a rigidly defined protocol for messages (electrical voltages, timings)."
What is a 'daisy chain'?,"Devices connected in a string (A to B, B to C), usually operating as a bus."
How do buses vary?,"In signaling, speed, throughput, and connection."
What is the 'PCIe bus' and its purpose?,A common computer I/O bus connecting the processor-memory subsystem to fast devices.
What is an 'expansion bus' and its purpose?,"A computer bus for connecting slow devices (e.g., keyboards)."
What is 'Serial-attached SCSI (SAS)'?,"A common type of I/O bus, used to connect disks to a SAS controller."
What is 'SAS'?,Common type of I/O bus.
Describe a 'PCIe lane'.,Two signaling pairs (receive/transmit) forming a full-duplex byte stream.
What is the data packet format for PCIe?,"Eight-bit byte format, with simultaneous transmission in both directions."
What are the common physical link configurations for PCIe?,"1, 2, 4, 8, 12, 16, or 32 lanes (e.g., x8)."
Give an example of PCIe throughput based on generation and lanes.,PCIe gen3 x8 offers 8 GB/s throughput.
What is a 'controller' in I/O hardware?,"Electronics operating a port, bus, or device; a special processor managing I/O devices."
What is 'Fibre Channel (FC)'?,A storage I/O bus used in data centers to connect computers to storage arrays.
What is a 'host bus adapter (HBA)'?,A device controller installed in a host bus port for device connection.
What components does an HBA typically contain?,"A processor, microcode, and private memory for its specific protocol (e.g., FC protocol)."
Do all devices require external controllers?,"No, some devices, like disk drives, have built-in controllers."
What functions does a disk controller typically perform?,"It implements disk-side protocols (like SAS, SATA), contains microcode, and has a processor for tasks such as bad-sector mapping, prefetching, buffering, and caching."
How does a processor communicate with a controller?,"Via registers (data, control)."
What are the two primary methods for processor-controller communication?,Special I/O instructions and Memory-mapped I/O.
What is 'memory-mapped I/O'?,A device I/O method where device-control registers are mapped into the processor address space.
How does the CPU interact with device-control registers in memory-mapped I/O?,It uses standard data-transfer instructions to read or write to the registers at their mapped memory locations.
Why is memory-mapped I/O generally more efficient than special I/O instructions for large data transfers?,"Writing millions of bytes to memory-mapped regions (e.g., graphics memory) is faster than executing millions of individual I/O instructions."
What is the modern trend regarding I/O communication methods?,"Systems have moved towards memory-mapped I/O for efficiency, with most I/O today occurring via device controllers using this method."
What are the four typical I/O device control registers?,"Status, control, data-in, and data-out registers."
What is a 'data-in register'?,A device I/O register read by the host to get input data.
What is a 'data-out register'?,A device I/O register written by the host to send output data.
What is a 'status register'?,"A device I/O register whose bits are read by the host to indicate states such as command complete, byte available, or error."
What is a 'control register'?,"A device I/O register written by the host to start a command or change a device's mode (e.g., full/half-duplex, parity, word length, speed)."
What is the typical size of data registers?,1-4 bytes.
What are FIFO chips used for in controllers?,"They hold several bytes, expanding capacity and buffering data bursts."
What is 'handshaking' in host-controller interaction?,A coordinated exchange of signals or register bits between the host and controller to manage communication.
Describe the polling process for a host sending a byte to a device.,"1. Host reads `busy` bit until clear. 2. Host sets `write` bit in `command` register, writes byte to `data-out` register. 3. Host sets `command-ready` bit. 4. Controller notices `command-ready` set, sets `busy` bit. 5. Controller reads command, reads `data-out` byte, performs I/O. 6. Controller clears `command-ready`, clears `error` bit, clears `busy` bit."
What is 'busy waiting' or 'polling'?,"A thread or process continuously uses the CPU while waiting for an event; in I/O, it's an I/O loop where the I/O thread continuously reads the status waiting for I/O completion."
When is polling an efficient method for I/O?,"If the controller and device are very fast, or for basic operations (e.g., 3 CPU cycles)."
When is polling inefficient for I/O?,"If the wait is long and other CPU tasks are pending, or if the device is rarely ready, leading to wasted CPU cycles."
What is a risk associated with polling if the host waits too long?,Data loss due to buffer overflow.
What is the alternative to polling for a hardware controller to notify the CPU?,An interrupt.
What is an 'interrupt' in computer systems?,A hardware mechanism for a device to notify the CPU that it needs attention.
What is an 'interrupt-request line'?,A hardware connection to the CPU for signaling interrupts.
What is an 'interrupt-handler routine'?,An OS routine called when an interrupt signal is received.
Describe the basic mechanism of an interrupt.,"The CPU senses the interrupt-request line after each instruction. If a controller asserts the signal, the CPU saves its current state, jumps to an interrupt-handler routine at a fixed address. The handler determines the cause, processes it, restores the CPU state, and executes a 'return from interrupt' instruction."
What verbs describe the interrupt process from device to handler?,"A device controller *raises* an interrupt, the CPU *catches* and *dispatches* it, and the handler *clears* it."
What is 'interrupt-controller hardware'?,Computer hardware components for interrupt management.
What are the two types of interrupt request lines on a CPU?,Nonmaskable interrupt and Maskable interrupt.
What is a 'nonmaskable interrupt'?,"An interrupt that cannot be delayed or blocked (e.g., unrecoverable memory error)."
What does 'maskable' mean in the context of interrupts?,Describes an interrupt that can be delayed or blocked.
What is an 'interrupt vector'?,"An OS data structure indexed by interrupt address, pointing to handlers. Its purpose is to reduce the need for a single handler to search all sources."
What is 'interrupt chaining'?,A mechanism where an interrupt vector element points to a list of handlers; handlers are called until one services the request. This avoids a huge table and improves dispatch efficiency.
What is an 'interrupt priority level'?,"Prioritization of interrupts for handling order, allowing deferral of low-priority and urgent response to high-priority interrupts."
What is an 'exception'?,"A software-generated interrupt by error (e.g., division by zero, protected memory access) or a user program's request for an OS service."
What is a 'first-level interrupt handler (FLIH)'?,"An interrupt handler responsible for reception and queuing of interrupts, including context switch and state storage."
What is a 'second-level interrupt handler (SLIH)'?,An interrupt handler that performs the actual handling and processing of the interrupt.
What is a 'software interrupt'?,A software-generated interrupt; also called a trap.
What is a 'trap'?,"A software interrupt. It saves user state, switches to kernel mode, and dispatches to a kernel routine."
How are traps prioritized compared to device interrupts?,"Traps typically have low priority compared to device interrupts, as they are generally less urgent."
How are interrupts used in virtual memory paging?,"A page fault raises an interrupt, suspending the current process. The handler fetches the required page, and then schedules another process to run."
How do system calls typically initiate kernel routines?,Library routines build a data structure and then execute a software interrupt (trap) to switch to kernel mode and dispatch to the appropriate kernel routine.
What is 'programmed I/O (PIO)'?,A data transfer method where the CPU transfers data one byte at a time.
Why is PIO wasteful for large data transfers?,"It monopolizes the CPU for data movement, which is inefficient for large transfers like those to/from a disk."
What is 'direct memory access (DMA)'?,An operation allowing device controllers to transfer large data directly to/from main memory without involving the main CPU.
How is a DMA transfer initiated by the host CPU?,"The host writes a DMA command block (containing source, destination, and byte count) to memory, then provides the command block's address to the DMA controller and continues with other work."
What is 'scatter-gather' I/O?,An I/O method specifying multiple sources/destinations in one command block for a single transfer operation.
How does a DMA controller perform data transfers without the main CPU?,"It operates the memory bus directly, seizing it to perform transfers."
What is a challenge when DMA targets user space memory?,"Risk of modification during transfer, often leading to inefficient 'double buffering'."
What is 'double buffering'?,"Copying data twice (e.g., device to kernel, then kernel to process), or using two buffers."
What is the trend for direct I/O between devices and user address space?,Operating systems have moved to using memory-mapping to enable direct I/O transfers.
What are the handshaking signals between a DMA controller and a device controller?,DMA-request and DMA-acknowledge wires.
Describe the handshaking process during a DMA transfer.,"1. Device places a signal on the DMA-request line when a word is available. 2. DMA controller seizes the memory bus, places the address, and signals DMA-acknowledge. 3. Device receives DMA-acknowledge, transfers the word, and removes DMA-request. This repeats until done, then DMA controller interrupts the CPU."
What is 'cycle stealing'?,"A device (e.g., DMA controller) using the bus, temporarily preventing the CPU from accessing main memory."
What is the overall impact of cycle stealing on system performance?,"While it momentarily slows the CPU, DMA generally improves total system performance by offloading large data transfers."
What is 'direct virtual memory access (DVMA)'?,"DMA using virtual addresses as transfer sources/destinations, which are translated to physical addresses. It can also transfer between memory-mapped devices without CPU/main memory."
What are the key concepts of I/O hardware?,"Bus, Controller, I/O port and its registers, Handshaking (host and device controller), Handshaking execution (polling or interrupts), and Offloading large transfers to DMA controller."
What are the main challenges for OS implementers regarding I/O hardware?,"Managing the wide variety of devices with unique capabilities, control-bit definitions, and protocols; attaching new devices without OS rewrites; and providing a uniform I/O interface to applications."
What is a 'PHY'?,Physical hardware component connecting to a network (OSI layer 1).
What is the primary goal when designing the Application I/O interface?,To treat I/O devices uniformly.
What are the key approaches used to achieve uniform treatment of I/O devices?,"Abstraction, encapsulation, and software layering."
How are device differences abstracted away in the I/O system?,"By identifying general kinds of devices, each accessed via standardized functions."
What is an 'interface' in the context of I/O device access?,Standardized functions through which each kind of device is accessed.
What encapsulates device differences in the kernel?,Device drivers.
What are 'device drivers'?,Kernel modules custom-tailored to specific devices that export standard interfaces.
What is the purpose of the device-driver layer?,To hide differences among device controllers from the kernel I/O subsystem.
How do I/O system calls relate to hardware differences?,"They encapsulate device behavior in generic classes, hiding hardware differences from applications."
"What are the benefits of the I/O structuring techniques (abstraction, encapsulation, layering)?",Simplifies OS development and allows hardware manufacturers to design compatible devices or write drivers for popular OS.
List the dimensions along which I/O devices vary.,"Character-stream or block; sequential or random access; synchronous or asynchronous; sharable or dedicated; speed of operation; read-write, read only, or write once."
Define 'Character-stream' device.,A device that transfers bytes one by one.
Define 'Block' device.,A device that transfers a block of bytes as a unit.
Define 'Sequential access' for devices.,Data access in a fixed order determined by the device.
Define 'Random-access' for devices.,User can seek to any storage location directly.
Define 'Synchronous' I/O operation.,"Predictable response times, coordinated with the caller."
Define 'Asynchronous' I/O operation.,"Irregular/unpredictable response times, not coordinated with the caller."
Define 'Sharable' device.,A device that can be used concurrently by several processes/threads.
Define 'Dedicated' device.,A device that cannot be used concurrently by multiple processes/threads.
What are the different data transfer directions for I/O devices?,"Read-write (both input/output), Read only (only one data transfer direction), Write once (written once, then read-only)."
How does the OS simplify device access for applications?,"It hides many differences and groups devices into conventional types, providing major access conventions."
What are the major application access conventions for I/O?,"Block I/O, Character-stream I/O, Memory-mapped file access, Network sockets."
What is an 'escape' or 'back door' in the context of device interfaces?,A method of transparently passing arbitrary commands to a device driver when the interface lacks a standard method for specific functionality.
What UNIX system call serves as an 'escape' or 'back door'?,`ioctl()`.
What are the three arguments of the UNIX `ioctl()` system call?,"Device identifier (major/minor numbers), command integer, and a pointer to a data structure."
"What does the 'major number' signify in a device identifier (e.g., for `ioctl()`)?","The device type, which routes I/O requests to the appropriate driver."
"What does the 'minor number' signify in a device identifier (e.g., for `ioctl()`)?",The device instance (which specific device of that type).
What is a 'block-device interface'?,An interface for I/O to disk drives and other block-oriented devices.
What are common commands for a block-device interface?,"`read()`, `write()`, and `seek()` (for random-access devices)."
How do applications usually access block devices?,Via the file-system interface.
What is 'raw I/O'?,"Direct access to secondary storage as a linear array of blocks, bypassing the file system."
When might raw I/O be preferred?,By the OS or special applications like Database Management Systems (DBMS) to avoid extra buffering and redundant locking.
What is 'direct I/O'?,"A compromise mode on a file (e.g., in UNIX) that allows block I/O bypassing OS block features like buffering and locking."
How does memory-mapped file access work?,"It layers on block-device drivers, allowing access to disk storage via a byte array in main memory. A system call maps a file to memory and returns a virtual address."
What is a key efficiency of memory-mapped file access?,Data transfers only occur when needed (demand-paged virtual memory access).
What are the benefits of memory-mapped file access for programmers?,"It is convenient, allowing simple read/write operations to memory instead of using explicit I/O calls."
How is memory-mapped file access used by the kernel?,For kernel services such as executing programs and accessing swap space.
What is a 'character-stream interface'?,"An interface for I/O to character devices such as keyboards, mice, modems, printers, and audio boards."
What are the basic system calls for a character-stream interface?,`get()` and `put()` (one character at a time).
How is network I/O different from disk I/O?,It typically uses a different interface than `read()`-`write()`-`seek()`.
What is a 'network socket'?,"A common interface for network I/O, serving as an endpoint for communication (used in UNIX, Windows)."
List common system calls associated with the socket interface.,"Create socket, connect local socket to remote address, listen for remote application connection, send/receive packets, and `select()`."
What is the purpose of the `select()` system call in network I/O?,"It manages a set of sockets, returning information on which sockets are ready (e.g., packet waiting, room to send), eliminating polling/busy waiting."
What are the benefits of the socket interface?,It facilitates distributed applications using any network hardware/protocol.
"Besides sockets, what are other approaches for Inter-Process Communication (IPC) and network communication?","Half-duplex pipes, full-duplex FIFOs, full-duplex STREAMS, and message queues."
What functionalities do hardware clocks and timers provide?,"Current time, elapsed time, and the ability to set a timer for operation X at time T."
What is a 'programmable interval timer'?,"Hardware (often provided by CPUs) used to measure elapsed time and trigger operations, typically by generating an interrupt (once or periodically)."
How is a programmable interval timer used by the OS?,"By the scheduler to preempt processes, by disk I/O to flush dirty caches, and by the network to cancel slow operations."
How does the OS manage more timer requests than available hardware channels?,It simulates virtual clocks by maintaining a sorted list of wanted interrupts and setting the hardware timer for the earliest pending request.
What is a 'high-performance event timer (HPET)'?,"A modern hardware timer (provided by some CPUs on PCs) often operating in the 10-megahertz range, with comparators that trigger interrupts when a value matches the HPET."
What limits precision in timer operations?,Timer resolution and virtual clock overhead.
What is 'network time protocol (NTP)'?,A network protocol used for synchronizing system clocks.
Define a 'blocking' system call.,"An I/O request that suspends the calling thread, moving it to a wait queue, until the I/O operation completes."
What is the nature of physical I/O actions in terms of timing?,"They are generally asynchronous, with varying and unpredictable completion times."
Why does the OS typically provide blocking I/O calls for applications?,Because they are easier for application developers to write and manage.
When is nonblocking I/O needed by user processes?,"For applications that cannot afford to halt, such as user interfaces or video applications, to overlap execution with I/O."
How can multithreaded applications overlap execution with I/O?,Some threads can block on I/O while others continue to execute.
What is a 'nonblocking' I/O system call?,"An I/O request that returns quickly, indicating the bytes transferred (full, fewer, or none), and does not halt the calling thread."
What is an 'asynchronous' system call?,"An I/O request that returns immediately without waiting for the I/O to complete, allowing the calling thread to continue execution."
How is I/O completion communicated for asynchronous system calls?,"Via variable setting, a signal/software interrupt, or a callback mechanism."
What is the key difference between a nonblocking `read()` and an asynchronous `read()`?,"A nonblocking `read()` returns immediately with any available data (full, fewer, or none), while an asynchronous `read()` requests the full transfer to complete later, returning immediately without data."
How does OS buffering optimize I/O performance?,"The OS buffers I/O and returns control to the application, completing the request later, which can hide I/O latency."
How is data consistency ensured when the OS buffers I/O?,"The kernel reads from its buffers before performing I/O, ensuring that the correct data is returned to the reader."
What mechanism can be used with `select()` for nonblocking behavior in network sockets?,"Specifying a maximum waiting time, with '0' indicating polling behavior (returning immediately if no I/O is possible)."
What is the overhead associated with using `select()`?,`select()` only checks if I/O is possible; subsequent `read()`/`write()` calls are still needed to perform the actual data transfer.
What is 'Vectored I/O'?,One system call that performs multiple I/O operations involving multiple memory locations.
What UNIX system call implements Vectored I/O?,"`readv` (and `writev`), which accepts a vector of multiple buffers to read to or write from."
What is another name for Vectored I/O?,Scatter-gather.
What are the benefits of Vectored I/O?,"Avoids context-switching and system-call overhead, eliminates the need to transfer data to a larger contiguous buffer first, and can provide atomicity (all I/O done without interruption, avoiding data corruption)."
What are some key I/O-related services provided by the kernel's I/O subsystem?,"Scheduling, buffering, caching, spooling, device reservation, and error handling."
On what infrastructure does the kernel's I/O subsystem build?,Hardware and device-driver infrastructure.
What is a key responsibility of the I/O subsystem regarding processes and users?,Protecting itself from errant processes and malicious users.
What is the primary purpose of I/O scheduling?,To determine a good execution order for I/O requests.
"Why is I/O scheduling necessary, considering the order of application system calls?",The order of application system calls is rarely the best for overall system performance.
What are the benefits of I/O scheduling?,"Improve overall system performance, fair device access, and reduce average waiting time."
How is I/O scheduling typically implemented?,By maintaining a wait queue for each I/O device.
What happens when a blocking I/O system call is made?,The request is placed on the device's wait queue.
How does the I/O scheduler optimize the device queue?,It rearranges the queue for efficiency and to reduce average response time.
Can the OS prioritize certain I/O requests? Provide an example.,"Yes, the OS may prioritize delay-sensitive requests, such as those from the virtual memory subsystem over applications."
What is a 'device-status table'?,A kernel data structure that tracks the status and queues of operations for I/O devices.
What information is typically stored in an entry of a device-status table?,"Device type, address, and state (e.g., not functioning, idle, busy)."
What additional information is stored in a device-status table entry if the device is busy?,The request type and parameters of the current operation.
"Besides improving efficiency, what else does scheduling I/O operations enable the OS to use?","Storage space in main memory/storage hierarchy via buffering, caching, and spooling."
Define 'buffer'.,A memory area storing data transferred between devices or between a device and an application.
What is the primary reason for using buffering?,To cope with speed mismatches between a producer and a consumer of data.
Provide an example of speed mismatch buffering.,A network (slow producer) sending data to an SSD (fast consumer); a buffer accumulates network bytes before writing them to the SSD in a single operation.
Define 'double buffering'.,"The use of two buffers to decouple a producer and a consumer, allowing for relaxed timing requirements; while one buffer is being filled, the other is being processed/written."
What other reason is there for buffering beyond speed mismatch?,"To provide adaptations for different data-transfer sizes (e.g., network fragmentation/reassembly)."
What are 'copy semantics' in the context of application I/O?,"The guarantee that data written to disk is the version present at the time the system call was made, independent of any subsequent changes to the application's buffer."
How does the OS typically guarantee copy semantics for a `write()` system call?,"The `write()` system call copies application data to a kernel buffer before returning to the application, and the disk write occurs from this kernel buffer."
"Why is copying data between kernel and application space common, despite its overhead?",Due to the clean semantics it provides.
What more efficient techniques can be used instead of direct data copying for copy semantics?,Virtual memory mapping and copy-on-write page protection.
Define 'cache'.,A region of fast memory that holds copies of data.
What is the primary benefit of accessing cached data?,Access to a cached copy is more efficient than accessing the original data.
Provide an example of caching in a computer system.,"Process instructions stored on disk are cached in physical memory, and further copied into CPU caches for faster access."
What is the key difference between a buffer and a cache?,"A buffer may hold the *only* copy of data being transferred, while a cache holds a *copy* of an item that also resides elsewhere (the original source)."
Can a memory region serve as both a buffer and a cache?,"Yes, caching and buffering are distinct concepts, but a single memory region can serve both purposes."
How do OS main memory buffers for disk data relate to caching?,"These buffers, used for copy semantics and efficient scheduling, also act as a cache to improve I/O efficiency for shared files or rapid write/reread operations."
"What action does the kernel take when a file I/O request is made, in relation to the buffer cache?","The kernel checks the buffer cache first; if the requested data is available, it avoids or defers physical disk I/O."
How do disk writes utilize the buffer cache?,"Disk writes are accumulated in the buffer cache for several seconds, allowing for more efficient write schedules."
Define 'spool' in the context of I/O.,"A buffer holding output for a device, such as a printer, that cannot accept interleaved data streams."
Why is spooling necessary for devices like printers?,"Printers can only serve one job at a time, but multiple applications may attempt to print concurrently. Spooling coordinates this concurrent output."
How does the OS implement spooling for printer output?,The OS intercepts each application's printer output and spools it to a separate file on secondary storage.
What happens after an application finishes its printing task in a spooling system?,The spooling system queues the spooled file for output to the printer.
How are spooled files then sent to the printer?,The spooling system copies the queued files to the printer one at a time.
Who manages the spooling system?,It is typically managed by a system daemon process or an in-kernel thread.
What control interfaces does the OS provide for spooling?,"It allows users to display the queue, remove jobs, or suspend printing."
"What type of devices cannot multiplex I/O requests, making spooling or exclusive access necessary?",Devices like tape drives and printers.
How do operating systems provide explicit coordination facilities for devices that cannot multiplex I/O requests?,They support exclusive device access.
Give examples of how OSes implement exclusive device access.,VMS allows a process to allocate an idle device and deallocate it when done; other OSes limit only one open file handle to such a device; Windows provides functions like `wait` for device objects or `OpenFile()` parameters for specific access types.
What is the responsibility of applications when coordinating exclusive device access?,Applications are responsible for avoiding deadlock.
What is a key role of a protected memory OS regarding errors?,"To guard against hardware and application errors, preventing system failure from minor malfunctions."
What are the two main types of I/O failure?,"Transient failures (e.g., network overloaded) and permanent failures (e.g., defective controller)."
How does the OS typically compensate for transient I/O failures?,"By retrying operations, such as a disk `read()` retry or a network `send()` resend."
What is the OS's capability to recover from a permanent failure of an important component?,The OS is unlikely to recover from such failures.
What does an I/O system call typically return to indicate its outcome?,A success/failure bit.
How does UNIX provide detailed error information from system calls?,"Through an integer variable called `errno`, which returns an error code (hundreds of possible values)."
What is the 'sense key' in SCSI protocol error reporting?,"A value in the status register indicating the general nature of a failure (e.g., hardware error, illegal request)."
What does the 'additional sense code' convey in SCSI protocol error reporting?,"The category of the failure (e.g., bad command parameter, self-test failure)."
What does the 'additional sense-code qualifier' provide in SCSI protocol error reporting?,"More specific details about the failure (e.g., which parameter was bad, which subsystem failed)."
What potential disruption can a user process cause related to I/O?,It may disrupt the system by attempting illegal I/O instructions.
How does the OS prevent user processes from issuing illegal I/O instructions?,"All I/O instructions are privileged, meaning users cannot issue them directly."
How do user programs perform I/O operations?,They must use the OS by executing a system call.
What is the OS's role when a user program makes an I/O system call?,"The OS, operating in monitor mode, checks the request's validity, performs the I/O, and then returns control to the user."
How are memory-mapped and I/O port memory locations protected from user access?,By the memory-protection system.
Are there exceptions where the kernel might allow user access to I/O memory? Provide an example.,"Yes, the kernel cannot deny all user access; for example, graphics games need direct access to memory-mapped graphics memory."
How might the kernel manage direct user access to I/O memory for specific components?,"It might provide a locking mechanism to allocate a section of memory (e.g., graphics memory) to one process at a time."
How does the kernel keep state information on I/O components?,Via in-kernel data structures.
Give an example of an in-kernel data structure used for I/O.,An open-file table structure.
What types of I/O activities do kernel data structures track?,"Network connections, character-device communications, and other I/O activities."
"How does UNIX handle differing `read()` semantics for various entities (user files, raw devices, process address spaces)?",It encapsulates these differences using an object-oriented technique.
What does an open-file record contain in UNIX to handle different file types?,A dispatch table with pointers to appropriate routines based on the file type.
"How do some operating systems, like Windows, handle I/O requests at a high level?",They use message-passing.
Describe the flow of an I/O request using message-passing in the kernel.,"An I/O request is converted into a message, sent through the kernel to the I/O manager, and then to the device driver."
What are the benefits of using message-passing for I/O in terms of system design?,"It simplifies the I/O system structure and design, and adds flexibility."
Why is power management important in data centers?,"Due to power costs, greenhouse gas emissions, and significant heat generation (cooling can use twice as much electricity as powering equipment)."
What is the OS's role in power usage in cloud computing environments?,"Adjusting processing loads, evacuating user processes, and idling/powering off systems."
How does the OS manage power for individual hardware components?,"It analyzes load and can power off hardware-enabled components (e.g., CPUs, external I/O devices) if their usage is low."
How does the OS manage power for CPU cores specifically?,"CPU cores can be suspended and resumed based on load, with their state saved and restored."
Why is power management a high priority in mobile computing?,To maximize battery life.
What is 'power collapse' in Android power management?,"A deep sleep state that uses marginally more power than being off, but responds to external stimuli and allows for quick wake-up."
How is Android's power collapse achieved?,"By powering off individual components (like screen, speakers, I/O subsystem) and placing the CPU in its lowest sleep state."
Describe Android's component-level power management.,"An infrastructure that understands component relationships and usage, where each component's device driver tracks its usage, allowing unused components to be powered off. If all components on a bus are unused, the bus is turned off; if all components in the device tree are unused, the system enters power collapse."
What is a 'device tree' in Android's power management?,A representation of the physical-device topology of the system.
Define 'wakelocks' in Android power management.,A mechanism allowing applications to temporarily prevent the system from entering a power collapse state.
How do wakelocks work?,"Applications acquire and release wakelocks, and the kernel prevents power collapse while a wakelock is held."
What role does firmware play in power management during boot time?,"Firmware analyzes hardware and creates a device tree in RAM, which the kernel then uses to load drivers and manage devices."
What is ACPI?,"Advanced Configuration and Power Interface (ACPI) is an industry-standard firmware used in modern computers that provides callable routines for the kernel to manage device state, errors, and power."
Give an example of how ACPI is utilized by the kernel for device management.,"The kernel calls a device driver, which in turn calls ACPI routines, which then communicate with the device."
What is the primary role of the I/O subsystem?,To coordinate extensive services for applications and the kernel.
List some key areas supervised by the kernel I/O subsystem.,"Management of name space for files/devices, access control, operation control (e.g., a modem cannot `seek()`), file-system space allocation, device allocation, buffering, caching, spooling, I/O scheduling, device-status monitoring, error handling, failure recovery, device-driver configuration and initialization, and power management of I/O devices."
How do the upper levels of the I/O subsystem access devices?,Via a uniform interface provided by device drivers.
"How does an OS connect an application's request (e.g., reading a file) to hardware operations (e.g., disk sector access)?",The OS maps the application's file name reference through the file system's directory structure to specific space allocation on disk.
What is the initial step for an application referring to data?,It refers to data by file name.
What is the role of the file system in connecting application requests to hardware?,"The file system maps the file name, through directories, to space allocation on the disk."
How does MS-DOS (FAT) map a file name to disk blocks?,"The file name maps to a number, which indicates an entry in the file-access table, telling which disk blocks are allocated."
How does UNIX map a file name to disk blocks?,"The file name maps to an inode number, and the inode contains the space-allocation information."
How does MS-DOS identify hardware devices within a file name?,"The first part of the file name (before the colon), e.g., `C:`, identifies the hardware device."
"In MS-DOS, how is a device name like `C:` mapped to hardware?",It is mapped to a specific port address via a device table.
What is a key characteristic of the device name space in MS-DOS compared to its file-system name space?,"It is separate from the file-system name space, indicated by the colon separator (e.g., `C:`)."
What is an advantage of MS-DOS's separate device name space?,"It makes it easy to associate extra functionality, such as spooling for printer files."
How is the device name space handled in UNIX?,"It is incorporated into the regular file-system name space, with no clear separation of the device portion in the path name."
What is the purpose of the mount table in UNIX?,It associates path name prefixes with specific device names.
How does UNIX resolve a path name involving a device?,It looks up the path name in the mount table for the longest matching prefix.
What does a mount table entry provide after a successful lookup in UNIX?,A device name (which is also in the file-system name space).
What is found when a device name is looked up in UNIX?,"A `<major, minor>` device number, not an inode."
What does the major device number indicate?,It identifies the specific device driver responsible for handling I/O for that device.
What is the purpose of the minor device number?,It is passed to the device driver to index into its internal device table.
What information does a device-table entry provide?,The port address or memory-mapped address of the device controller.
How do modern operating systems enhance flexibility in device lookup?,They use multiple stages of lookup tables.
What is a benefit of the general mechanisms for passing requests between applications and drivers in modern OS?,New devices and drivers can be introduced without requiring kernel recompilation.
How do some operating systems manage loading device drivers?,They load device drivers on demand.
Describe driver loading during boot time.,"The system probes buses and loads necessary drivers, either immediately or upon the first request for the device."
How are drivers loaded for devices added after system boot?,"The device is detected, often by an error, the kernel inspects it, and then loads the driver dynamically."
What are some challenges associated with dynamic loading/unloading of drivers?,"It requires more complex kernel algorithms, careful device-structure locking, and robust error handling."
What is the first step in the life cycle of a blocking `read()` system call?,A process issues a blocking `read()` system call to a file descriptor of an opened file.
What is the kernel's first action after receiving a `read()` system call?,"The kernel system-call code checks parameters. If the data is already in the buffer cache, the data is returned and I/O completed."
What happens if the requested data is not in the buffer cache during a `read()` system call?,"Physical I/O is performed. The process is removed from the run queue and placed on a wait queue for the device, and the I/O request is scheduled."
What happens after an I/O request is scheduled by the kernel?,The I/O subsystem sends the request to the device driver (via subroutine call or in-kernel message).
What is the device driver's role in a blocking `read()` request after receiving the request?,"The device driver allocates kernel buffer space, schedules the I/O, and sends commands to the device controller by writing to device-control registers."
What is the device controller's responsibility during a `read()` request?,It operates the device hardware to perform the data transfer.
How can a driver handle data transfer completion from the device controller?,"The driver may poll for status/data, or set up a DMA transfer to kernel memory, where the DMA controller generates an interrupt upon transfer completion."
What happens immediately after a DMA transfer completes and an interrupt is generated?,"The correct interrupt handler receives the interrupt via the interrupt-vector table, stores the data, signals the device driver, and then returns."
What does the device driver do upon receiving a signal from the interrupt handler?,"It determines the completed I/O request and its status, then signals the kernel I/O subsystem."
What is the kernel's final action regarding data transfer and process state after being signaled by the device driver?,The kernel transfers the data and return codes to the requesting process's address space and moves the process from the wait queue to the ready queue.
What happens to the process after it is moved to the ready queue following a `read()` system call?,"Moving the process to the ready queue unblocks it. The scheduler then assigns the CPU, allowing the process to resume at the system call completion."
"Define ""mount table"".","An in-memory data structure containing information about each mounted volume, used to track file systems and access."
Which UNIX systems typically include the STREAMS mechanism?,UNIX System V and subsequent releases.
Define STREAMS.,A UNIX I/O feature allowing dynamic assembly of driver code pipelines.
"Define ""Stream"" in the context of STREAMS.",A full-duplex connection between a device driver and a user-level process.
What are the main components of a STREAMS mechanism?,"Stream head, driver end, and zero or more stream modules."
Define stream head.,The interface between STREAMS and user processes.
Define driver end.,The interface between STREAMS and the controlled device.
Define stream modules.,Modules of functionality loadable into a STREAM.
Where are stream modules positioned within a STREAMS connection?,Between the stream head and the driver end.
What type of queues does each component of a STREAMS mechanism contain?,A pair of queues: a read queue and a write queue.
How is data transferred between components in a STREAMS mechanism?,Via message passing between queues.
What is the function of modules in STREAMS?,They provide STREAMS processing functionality.
How are modules added (pushed) onto a stream?,Using the `ioctl()` system call.
Provide an example of using a module in STREAMS.,Opening a USB device (like a keyboard) via a stream and pushing a module for input editing.
Define flow control in STREAMS.,"A method to pause a sender of I/O, which limits the data flow rate."
How does a STREAMS queue behave when *without* flow control?,It accepts all messages and immediately sends them to the adjacent queue without buffering.
How does a STREAMS queue behave when *with* flow control?,It buffers messages and does not accept new messages without sufficient buffer space.
What is involved in flow control between adjacent STREAMS module queues?,Control message exchanges.
What system calls can a user process use to write data to a device via STREAMS?,`write()` or `putmsg()`.
What type of data does the `write()` system call handle in STREAMS?,Raw data (unstructured byte stream).
What additional capability does the `putmsg()` system call provide compared to `write()` in STREAMS?,It allows the user to specify a message.
What happens after a user process writes data to a stream head?,The stream head copies the data into a message and delivers it to the next module's queue.
What is the path of data after it leaves the stream head when writing to a device?,"Copying continues from module to module until it reaches the driver end, and then the device."
What system calls can a user process use to read data from a stream head?,`read()` or `getmsg()`.
How does the `read()` system call process data received from a stream head?,The stream head gets the message and returns ordinary data (an unstructured byte stream).
What type of data does the `getmsg()` system call return to the process?,The entire message.
What is the general nature of STREAMS I/O?,"Asynchronous (or nonblocking), except when communicating directly with the stream head."
When does a user process block when writing to a stream?,If the next queue in the stream uses flow control and there is no room to copy the message.
When does a user process block when reading from a stream?,Until data becomes available.
What is a key responsibility of the driver end regarding device interaction?,"It must respond to interrupts (e.g., when a frame is ready from a network device)."
How does the driver end's handling of incoming data differ from the stream head's?,"Unlike the stream head, which may block, the driver end *must* handle all incoming data."
Must drivers support flow control within the STREAMS mechanism?,"Yes, drivers must support flow control."
What often happens if a device's buffer becomes full when interacting via STREAMS?,"The device typically drops incoming messages (e.g., a network card dropping frames)."
What is a major benefit of the STREAMS mechanism?,It provides a framework for modular and incremental development of device drivers and network protocols.
What is an advantage regarding the reusability of STREAMS modules?,"Modules are reusable by different streams or devices (e.g., a networking module for Ethernet and 802.11 wireless)."
"What type of data and information can STREAMS support transfer between modules, beyond just unstructured byte streams?",Message boundaries and control information.
How widely adopted is STREAMS in UNIX variants and for what purpose?,Most UNIX variants support STREAMS; it is preferred for implementing protocols and device drivers.
Provide an example of a system where the socket mechanism is implemented using STREAMS.,System V UNIX and Solaris.
What is a major factor in system performance?,I/O
How do heavy I/O demands affect the CPU?,They place heavy demands on the CPU by requiring execution of device-driver code and scheduling processes (blocking/unblocking).
What is the impact of context switches during I/O on system components?,They stress the CPU and hardware caches.
What can be exposed by heavy I/O demands in the kernel?,Inefficiencies in the kernel's interrupt-handling mechanisms.
How does I/O activity load the memory bus?,Through data copies between controllers/physical memory and kernel buffers/application space.
What is a major concern for computer architects regarding I/O?,Graceful coping with I/O demands.
Why is interrupt handling considered relatively expensive?,"It involves a state change, execution of the handler, and restoration of the state."
Under what condition can Programmed I/O (PIO) be more efficient than interrupt-driven I/O?,If busy waiting is minimized.
What overhead results from I/O completion unblocking a process?,Full context switch overhead.
What is a characteristic of network traffic in terms of system overhead?,High context-switch rate.
"In a remote login, what is the sequence of events on the local machine when a character is typed?",Character typed → keyboard interrupt → interrupt handler → device driver → kernel → user process.
"In a remote login, describe the network I/O system call process initiated by the user on the local machine.",User process issues network I/O system call → local kernel → network layers (packet construction) → network device driver.
"In a remote login, what happens after the network device driver processes the packet on the local machine?","Network device driver transfers packet to controller → sends character, generates interrupt."
How does the network I/O system call complete on the local machine in a remote login scenario?,Interrupt back up through kernel → network I/O system call completes.
"In a remote login, what is the initial event on the remote system when a packet is received?",Network hardware receives packet → interrupt generated.
"In a remote login, what happens on the remote system after an interrupt is generated for a received packet?",Character is unpacked from protocols → passed to the appropriate network daemon.
"In a remote login, how does the network daemon process the character on the remote system?",Network daemon identifies session → passes packet to subdaemon.
What type of overhead is consistently observed throughout the remote login character example?,Context switches and state switches.
What is the impact of the receiver echoing a character in a remote login scenario?,It doubles the work.
What is one way some systems reduce the main CPU's interrupt burden for terminal I/O?,By using separate front-end processors.
What is the primary job of an I/O channel?,To offload I/O work from the main CPU and keep data flowing smoothly.
How do I/O channels process programs?,They process more general/sophisticated programs tuned for specific workloads.
List a principle to improve I/O efficiency related to context switches.,Reduce the number of context switches.
List a principle to improve I/O efficiency related to data copies in memory.,Reduce data copies in memory (between device/application).
List a principle to improve I/O efficiency related to interrupt frequency.,"Reduce interrupt frequency by using large transfers, smart controllers, or polling (if busy waiting is minimal)."
List a principle to improve I/O efficiency related to increasing concurrency.,Increase concurrency by using DMA-knowledgeable controllers/channels to offload data copying from the CPU.
List a principle to improve I/O efficiency related to hardware processing.,Move processing primitives into hardware to enable concurrent operation with the CPU/bus.
List a principle to improve I/O efficiency related to system component balance.,"Balance CPU, memory subsystem, bus, and I/O performance, as overload in one area causes idleness in others."
How does the complexity of I/O devices vary?,"It varies significantly, from simple (e.g., mouse) to very complex (e.g., Windows disk driver)."
"What are some functions of a complex I/O device driver, such as a Windows disk driver?","Manages individual disks, implements RAID arrays, converts requests to disk I/O, performs error handling, data recovery, and optimizes performance."
What are the three main locations where I/O functionality can be implemented?,"Device hardware, device driver (kernel), or application software."
Where were experimental I/O algorithms initially implemented?,At the application level.
What are the advantages of implementing experimental I/O algorithms at the application level?,"Flexible, bugs are unlikely to crash the system, and no reboot/reload of drivers is needed after code changes."
What are the disadvantages of implementing experimental I/O algorithms at the application level?,"Innefficient due to context switch overhead, and lacks access to internal kernel data/functionality (e.g., messaging, threading, locking)."
What is an example of a system interface that allows user-mode file systems?,FUSE (Filesystem in Userspace).
Where are I/O algorithms typically reimplemented once they are proven?,In the kernel.
What is the main advantage of reimplementing I/O functionality in the kernel?,Improves performance.
What are the challenges of reimplementing I/O functionality in the kernel?,"More challenging development (due to large, complex kernel), and must be thoroughly debugged to avoid data corruption or system crashes."
Where is the highest I/O performance achieved?,Through specialized implementation in hardware (device or controller).
What are the disadvantages of implementing I/O functionality in hardware?,"Difficulty/expense of improvements/bug fixes, increased development time (months vs. days), and decreased flexibility (e.g., hardware RAID controller may not allow the kernel to influence I/O order/location)."
What is the current trend observed in I/O device speed?,"I/O devices are increasing in speed, with NVM devices nearing DRAM speed."
What is the impact of increasing I/O device speeds on system design and OS algorithms?,It increases pressure on I/O subsystems and OS algorithms to leverage the faster read/write speeds.
List the major components that affect the I/O performance of storage and network latency.,"CPU, caches, DRAM, NVM, PCIe, SSD, SAA, HDD."
"Define ""front-end processors.""",Small computers performing tasks in an overall system; they manage I/O and offload the main CPU.
"Define ""terminal concentrator.""",A type of front-end processor that multiplexes traffic from hundreds of remote terminals into one port.
"Define ""I/O channel.""","A dedicated, special-purpose CPU found in large systems (mainframes/high-end systems) used for I/O or offloading the main CPU."
What are the basic I/O hardware elements?,"Buses, device controllers, and devices."
How is data movement handled in an I/O system?,By the CPU (using programmed I/O) or by a DMA controller.
What is a device driver?,A kernel module responsible for controlling a specific hardware device.
What basic hardware categories does the system-call interface handle?,"Block devices, character-stream devices, memory-mapped files, network sockets, and programmed interval timers."
What is the typical behavior of system calls regarding processes?,System calls usually block processes.
When are nonblocking or asynchronous system calls used?,"By the kernel or applications that must not sleep (i.e., cannot afford to block)."
What services does the kernel's I/O subsystem provide?,"I/O scheduling, buffering, caching, spooling, device reservation, and error handling."
What is the purpose of name translation in I/O?,It connects hardware devices to symbolic file names.
What are the multiple mapping levels involved in I/O name translation?,"Character-string names map to device drivers/addresses, which then map to physical addresses (I/O ports/bus controllers)."
Where can I/O device mapping occur?,"Within the file-system name space (e.g., UNIX) or in a separate device name space (e.g., MS-DOS)."
What are STREAMS in UNIX?,A UNIX mechanism for the dynamic assembly of driver code pipelines.
How do drivers behave in a STREAMS pipeline?,"Drivers can be stacked, and data passes sequentially and bidirectionally through them."
Why are I/O system calls considered costly?,"Due to context switching (crossing the kernel protection boundary), signal/interrupt handling, and CPU/memory load for data copying between kernel buffers and application space."
What is a file?,A collection of related information defined by its creator; the smallest allotment of logical secondary storage from a user's perspective; a named collection of related information recorded on secondary storage.
What is the primary role of the OS regarding files and physical storage?,The OS maps files onto physical mass-storage devices.
Define 'file system'.,Describes how files map to physical devices and how they are accessed/manipulated.
What are the key design goals for file systems?,"Efficient access (as physical storage can be slow), file sharing support, and remote access."
Why is the file system considered the most visible OS aspect for users?,It provides a mechanism for online storage and access to OS data/programs and user data.
What are the two main components of a file system?,1. A collection of files (storing related data). 2. A directory structure (organizes and provides information about files).
What kind of storage do most file systems reside on?,"Nonvolatile, persistent storage devices."
How does the OS provide a uniform logical view of stored information?,It abstracts physical properties to define a logical storage unit called the file.
What types of information do files commonly represent?,Programs (source/object) and data.
What are common categories of data files?,"Numeric, alphabetic, alphanumeric, and binary."
How can files be structured in terms of formatting?,They can be free form (like text) or rigidly formatted.
What is a 'text file'?,A file containing text (alphanumeric characters); a sequence of characters organized into lines.
What is a 'source file'?,"A file containing program source code; a sequence of functions (declarations, executable statements)."
What is an 'executable file'?,A file containing a program ready for loading/execution; a series of code sections a loader can bring into memory and execute.
How are files typically named for human users?,"They are named symbolically, e.g., `example.c`."
What does it mean for a file to be 'independent'?,"A file is independent of its creator process, user, and system. A copy remains independent if no sharing or synchronization is involved."
List the typical attributes of a file.,"Name, Identifier, Type, Location, Size, Protection, Timestamps and user identification."
What is the 'Name' attribute of a file?,"The symbolic, human-readable name of the file."
What is the 'Identifier' attribute of a file?,"A unique tag (number), non-human-readable, that identifies the file within the file system."
What is the 'Type' attribute of a file?,An attribute used in systems supporting different file types to categorize the file.
What is the 'Location' attribute of a file?,A pointer to the device and the file's location on that device.
What is the 'Size' attribute of a file?,"The current size of the file (in bytes, words, or blocks), possibly including the maximum allowed size."
What is the 'Protection' attribute of a file?,"Access-control information, specifying permissions like read, write, or execute."
"What information is included in 'Timestamps and user identification' attributes of a file, and what is its purpose?","Includes creation time, last modification time, and last use time. Useful for protection, security, and monitoring."
What are 'extended file attributes'?,"Extended metadata supported by newer file systems, such as character encoding and file checksums."
What is a 'file info window'?,A GUI view of file metadata.
Where is information about all files stored within the file system structure?,"In the directory structure, typically on the same device as the files themselves."
What does a directory entry contain?,"The file name and a unique identifier, which in turn locates other attributes of the file."
How does the OS typically handle file operations?,"It provides system calls for operations like create, write, read, reposition, delete, and truncate files."
What are the two steps involved in creating a file?,1. Find space in the file system. 2. Make an entry for the new file in the directory.
What is typically required before performing most file operations (except create/delete)?,The `open()` system call.
What is a 'write pointer'?,"The location in a file for the next write operation, maintained by the system during sequential writes."
What is a 'read pointer'?,"The location in a file for the next read operation, maintained by the system during sequential reads."
What is the 'current-file-position pointer'?,"A per-process pointer that indicates the next read/write location within a file, shared by read and write operations."
What is the 'seek' operation in the context of files?,"The operation of changing the current-file-position pointer to a given value, without actual I/O involved."
What are the steps involved in deleting a file?,1. Search the directory for the named file. 2. Release all file space for reuse. 3. Erase or mark the directory entry as free.
What are 'hard links'?,File-system links where a file has two or more names pointing to the same inode. The actual content is deleted only when the last link is deleted.
What does it mean to 'truncate' a file?,"To erase the contents of a file but keep its attributes, resetting the file length to zero and releasing its allocated space."
Why does the `open()` system call exist for files?,To avoid constant directory searching for file location and attributes during repeated operations on the same file.
What is an 'open-file table'?,"An OS data structure that contains information about all currently open files, allowing files to be specified by an index into the table rather than requiring directory searches."
What happens when a file is closed?,The OS removes its entry from the open-file table and releases any associated locks.
What are the two levels of internal tables the OS uses for open files when multiple processes are involved?,A per-process table and a system-wide open-file table.
What information does the per-process open-file table track?,"Files a specific process has open, its current file pointer, access rights, and accounting information."
What information does the system-wide open-file table contain?,"Process-independent information about open files, such as disk location, access dates, and size."
What is an 'open count' in the context of file management?,"A counter in the system-wide open-file table that tracks the number of processes that currently have a file open. When it reaches zero, the entry is removed."
What is the purpose of file locks?,"They allow one process to lock a file or sections of a file to prevent other processes from accessing them, useful for shared files like system logs."
What is a 'shared lock'?,"A type of file lock that allows multiple processes to acquire it concurrently, similar to a reader lock."
What is an 'exclusive lock'?,"A type of file lock that allows only one process to acquire it at a time, similar to a writer lock."
Differentiate between 'mandatory' and 'advisory' file-locking mechanisms.,"In mandatory locking, the OS prevents other processes from accessing a locked file (e.g., Windows). In advisory locking, the OS does not prevent access; applications must manually acquire and respect the locks (e.g., UNIX)."
What is a common technique for indicating file types?,"Including the type as part of the file name, often as an extension (e.g., `name.extension`)."
How do operating systems typically use file extensions?,"The OS uses the extension to indicate the file type and the allowed operations, such as identifying executable files (.com, .exe, .sh)."
What is a 'shell script'?,"A file containing a set series of commands specific to a shell, often identified by a `.sh` extension."
How does macOS handle file types and application associations?,"Each file has a type (e.g., `.app`) and a 'creator attribute' (the program that created it). The OS sets and enforces the creator attribute, allowing double-clicking a file to open the associated application."
What is a 'magic number' in the context of file types?,"A specific number or byte sequence at the beginning of some binary files that indicates their data type, such as an image format. Text files can also have magic numbers to indicate their shell language, for example."
How does UNIX generally treat file structures?,Each file is a sequence of 8-bit bytes with no OS interpretation. This provides maximum flexibility but requires applications to interpret the file structure themselves.
What is the universal file structure that all operating systems must support?,"The executable file structure, necessary for loading and running programs."
What is the unit of disk I/O?,"All disk I/O operates in units of one block (or sector), and all blocks are typically the same size."
What is 'internal fragmentation' in file systems?,"Wasted disk space that occurs in the last allocated block of a file because the block size is fixed, and the file's data may not perfectly fill the last block."
How does block size relate to internal fragmentation?,A larger block size generally leads to greater internal fragmentation.
What is the fundamental requirement for using information stored in files?,Information stored in files must be accessed and read into memory.
How many access methods do systems typically provide for file information?,"Some systems provide only one access method, while others provide many."
What is a major design problem related to file access methods?,Choosing the right access method.
Define: sequential access,"File-access method: contents read in order, beginning to end."
What is the simplest file access method?,Sequential access.
Describe how information is processed using sequential access.,"Information is processed in order, one record after another."
What are common applications that use sequential access?,Editors and compilers.
What is the function of the `read_next()` operation in sequential access?,It reads the next portion of the file and automatically advances the file pointer.
What is the function of the `write_next()` operation in sequential access?,It appends new material to the end of the file and advances the file pointer to the end of the newly written material.
Can a file using sequential access be reset?,"Yes, a file using sequential access can be reset to the beginning."
Do all sequential access systems allow skipping records?,Some systems allow skipping forward or backward `n` records.
What model of file is sequential access based on?,The tape model of a file.
On what types of devices does sequential access work?,It works on both sequential-access devices and random-access ones.
Define: direct access,File-access method: contents read in random order.
Define: relative access,File-access method: contents read in random order.
What is another name for direct access?,Relative access.
Define: logical records,File contents logically designated as fixed-length structured data.
How are files structured in direct access?,Files are structured as fixed-length logical records.
How do programs read/write records using direct access?,Programs can read or write records rapidly in no particular order.
What model of file is direct access based on?,The disk model of a file (as disks allow random access).
How is a file viewed in direct access?,As a numbered sequence of blocks or records.
Are there restrictions on read/write order in direct access?,"No, there are no restrictions on read/write order."
What is a significant use case for direct access?,"Immediate access to large amounts of information, such as in databases (e.g., an airline reservation system)."
How do direct-access file operations typically specify the target location?,"They include a block number as a parameter (e.g., `read(n)`, `write(n)`)."
What is an alternative way to implement direct access operations while retaining sequential-like operations?,Retain `read_next()` and `write_next()` and add a `position_file(n)` operation.
Define: relative block number,Index relative to beginning of file (first is block 0).
What is the starting index for relative block numbers in many systems?,"0 (the first block is 0, the next is 1, etc.). Some systems start at 1."
Define: allocation problem,OS determination of where to store file blocks.
What is a benefit of the OS handling the allocation problem for direct access?,It prevents the user from accidentally accessing non-file portions of the file system.
How does the OS satisfy a request for a record `N` in a direct-access file?,It turns it into an I/O request for `N` bytes starting at `N * (logical record length)`.
"What makes it easy to read, write, or delete a record in direct access?",Logical records are of fixed size.
Do all operating systems support both sequential and direct access?,"No, not all operating systems support both."
When might a file's access method be defined?,Some systems require a file to be defined as sequential or direct at its creation.
How can sequential access be simulated on a direct-access file?,By keeping a `cp` variable for the current position.
What is the efficiency of simulating direct access on a sequential-access file?,It is extremely inefficient and clumsy.
What are other file access methods typically built upon?,Direct-access method.
Define: index (in the context of file access),"An access method built on direct access, where a file contains an index with pointers to its contents."
What is a core component of file access methods built on direct access?,An index constructed for the file.
What does an index contain in the context of file access?,"Pointers to various blocks of the file, similar to a book index."
How is a record found using an index-based access method?,"Search the index, then use the pointer to access the file directly."
Give an example of how an index might be structured for a sorted file.,"For a retail-price file sorted by UPC, the index might contain the first UPC in each block."
What is the process for finding a block using a simple index kept in memory?,"Perform a binary search on the index to find the relevant block, then access that block."
What challenge arises when index files become too large?,The index file itself might be too large to fit in memory.
How is the challenge of large index files addressed?,"By creating an index for the index file (e.g., a primary index pointing to secondary indexes, which then point to the data)."
What is an example of an indexed sequential-access method?,IBM ISAM (Indexed Sequential-Access Method).
Describe the structure of IBM ISAM indexes.,"A small main index points to disk blocks of a secondary index, and the secondary index blocks then point to the actual file blocks."
What is a requirement for the file data in IBM ISAM?,The file must be sorted on a key.
Outline the steps to find an item using IBM ISAM.,1. Perform a binary search on the main index. 2. Get the appropriate secondary index block. 3. Perform a binary search on the secondary index. 4. Find the block containing the record. 5. Perform a sequential search within that block.
What is the maximum number of direct-access reads typically required to locate any record using an indexed sequential-access method like ISAM?,At most two direct-access reads.
What is a directory in a file system context?,A symbol table translating file names to file control blocks.
What are the essential organizational capabilities a directory must allow?,"Insert entries, Delete entries, Search for named entry, List all entries."
List the common operations on a directory.,"Search for a file, Create a file, Delete a file, List a directory, Rename a file, Traverse the file system."
What is the purpose of the 'Search for a file' operation on a directory?,To find the entry for a particular file or find files matching a pattern.
What is the purpose of the 'Create a file' operation on a directory?,To add new files to the directory.
"What is the purpose of the 'Delete a file' operation on a directory, and what can be a consequence?","To remove a file from the directory; it may leave a hole, requiring defragmentation."
What is the purpose of the 'List a directory' operation?,To list files and their entry contents.
What is the purpose of the 'Rename a file' operation on a directory?,To change a file name when its contents/use changes; it may also change the file's position.
What is the purpose of the 'Traverse the file system' operation on a directory?,"To access every directory/file, typically for backup or space release."
Describe the simplest directory structure.,"Single-level directory, where all files are in the same directory."
What are the advantages of a single-level directory?,Easy to support and understand.
What are the limitations of a single-level directory?,Files must have unique names (name collision problem for multiple users) and it's difficult for a single user to remember many file names.
How is a two-level directory structured?,"It separates directories for each user, with each user having their own User File Directory (UFD)."
What is a User File Directory (UFD)?,Per-user directory of files in two-level directory implementation.
What is the Main File Directory (MFD) in a two-level directory system?,"The system's main file directory, indexed by user name/account, which points to each User File Directory (UFD)."
How does file searching work when a user refers to a file in a two-level directory system?,Only their User File Directory (UFD) is searched.
What is an advantage of the two-level directory structure regarding file names?,"Different users can have the same file names, as names are unique only within each User File Directory (UFD)."
Who typically handles the creation and deletion of User File Directories (UFDs)?,"Special system programs, restricted to administrators."
What are the disadvantages of a two-level directory structure?,"It isolates users, which is a disadvantage for cooperation; to access another user's file, one must specify both the user name and the file name."
How can a two-level directory be visualized as a tree structure?,"The Main File Directory (MFD) is the root, User File Directories (UFDs) are descendants, and files are the leaves."
What is a path name in the context of a two-level directory?,"The combination of the user name and the file name used to locate a file (e.g., '/userb/test.txt' or 'C:\userb\test')."
What problem arises with system files in a two-level directory structure?,Copying system files to each User File Directory (UFD) wastes space.
What is the solution to the system file problem in two-level directories?,"Create a special user directory for system files (e.g., user 0)."
Describe the search sequence for files in a two-level directory system with system file optimization.,"The OS first searches the local User File Directory (UFD), then the special system directory."
What is a search path?,A sequence of directories searched for an executable file when a command is executed.
Can search paths be customized in a two-level directory system?,"Yes, the search path can be extended, and users can have their own search paths."
What is the relationship between tree-structured directories and two-level directories?,Tree-structured directories are a generalization of two-level directories to arbitrary height.
What is the most common directory structure?,Tree-structured directories.
"In a tree-structured directory, what are the characteristics of files and path names?","There is a root directory, and every file has a unique path name."
What can a directory (or subdirectory) contain in a tree-structured directory system?,Files or subdirectories.
"How is a directory often treated in a tree-structured directory system, and how is its type distinguished?",A directory is often treated as a special file; one bit defines whether an entry is a file (0) or a subdirectory (1).
How are directories created and deleted in a tree-structured system?,Using special system calls.
What is a 'current directory' in a tree-structured file system?,Each process has a directory that is currently being referenced.
How is a file referenced in a tree-structured directory system if it's not in the current directory?,One must specify its path name or change the current directory.
Where does the initial current directory come from?,From an accounting file.
Define 'absolute path name' in a tree-structured directory.,"A path name that begins at the root (e.g., '/') and follows the path down to the file."
Define 'relative path name' in a tree-structured directory.,A path name that defines the path from the current directory.
Provide an example of a relative path name.,"If the current directory is '/spell/mail', then 'prt/first' refers to '/spell/mail/prt/first'."
How do users typically organize their files in a tree-structured directory system?,By defining subdirectories based on topic or information type.
How is a directory deleted if it is empty in some tree-structured systems?,The entry is simply deleted.
How do some systems (like UNIX with 'rm -r') handle the deletion of a non-empty directory in a tree-structured system?,"They delete the directory and all its files/subdirectories recursively, which is convenient but dangerous."
How do other systems handle the deletion of a non-empty directory in a tree-structured system if they do not support recursive deletion?,"They only delete the directory if it's empty, requiring the user to delete contents recursively first."
How can users access other users' files in a tree-structured directory system?,By specifying the full path name or by changing their current directory to the relevant path.
What is the primary feature of acyclic-graph directories?,They allow directories to share subdirectories and files.
What constraint defines an acyclic-graph directory structure?,There are no cycles (loops) in the graph.
What happens when a shared file is modified in an acyclic-graph directory system?,"Changes are visible to all directories sharing the file, as only one actual file exists."
What happens when new files are added to a shared subdirectory in an acyclic-graph directory system?,The new files appear in all shared subdirectories.
What are two main ways to implement shared files/subdirectories in an acyclic-graph directory system?,Using links (pointers) or by duplicating all information in both sharing directories.
What is a 'link' in the context of acyclic-graph directories?,"A file that has no contents but points to another file or subdirectory (e.g., using an absolute or relative path name)."
What does it mean to 'resolve' a link?,To follow the path name in the link to locate the real target file.
How does the OS typically handle links during directory traversal in acyclic-graph systems?,It ignores links to preserve the acyclic structure.
What is a disadvantage of implementing sharing by duplicating all information in both sharing directories?,It leads to consistency issues if the original file/subdirectory is modified.
What are the main problems associated with acyclic-graph directories?,"Multiple absolute path names for the same file (aliasing), issues with traversing the entire file system efficiently, and complexity in determining when space can be deallocated upon deletion."
What is the 'aliasing' problem in acyclic-graph directories?,The same file can have multiple absolute path names.
What is a challenge when traversing an entire file system structured as an acyclic graph?,Avoiding traversing shared structures more than once.
What problem does deleting a file linked in an acyclic-graph directory present?,Deleting the original file may leave dangling pointers if other links to it still exist.
How do symbolic links behave upon deletion of the original file?,"Deletion of the link itself doesn't affect the original file, but if the original file is deleted, the symbolic links dangle."
How can the dangling pointer problem be avoided when deleting files in an acyclic-graph directory system?,"By preserving the file until all references to it are deleted, typically by using a reference count."
How does a 'reference count' work for file deletion?,The count is incremented on a new link/entry creation and decremented on deletion; the file is deleted only when the count reaches 0.
What type of links in UNIX use a reference count for deletion?,Hard links.
What are hard links?,File-system links where a file has two or more names pointing to the same inode.
What is the main characteristic of a general graph directory structure?,It allows cycles (loops) in the directory structure.
What is the primary advantage of acyclic graph directories over general graph directories?,Simpler traversal and deletion algorithms.
What are the problems associated with cycles in a general graph directory structure?,Infinite loops during search/traversal and reference counts may not reach 0 even if a file/directory is inaccessible.
What is required to determine when space can be reallocated in a general graph directory system with cycles?,Garbage collection.
What is 'garbage collection' in the context of file systems?,The recovery of space containing no-longer-valid data.
Why is garbage collection problematic for disk-based file systems?,It can be very time-consuming.
What is a simpler approach to dealing with cycles in general graph directories?,Bypass links during directory traversal.
Define 'user file directory (UFD)'.,Per-user directory of files in two-level directory implementation.
Define 'main file directory (MFD)'.,Index pointing to each UFD in two-level directory implementation.
Define 'path name'.,"File-system name for a file, containing mount-point and directory-entry info to locate it (e.g., 'C:/foo/bar.txt')."
Define 'search path'.,Sequence of directories searched for an executable file when a command is executed.
Define 'absolute path name'.,Path name starting at the top of the file system hierarchy.
Define 'relative path name'.,"Path name starting at a relative location (e.g., current directory)."
Define 'acyclic graph' in the context of directory structures.,Directory structure implementation that contains no cycles (loops).
Define 'link' (in the context of acyclic-graph directories).,File that has no contents but points to another file.
Define 'resolve' (in relation to links).,To follow a link and find the target file.
Define 'hard links'.,File-system links where a file has two or more names pointing to the same inode.
Define 'garbage collection' (in context of file systems).,Recovery of space containing no-longer-valid data.
What aspect of information safety addresses physical damage?,"Reliability (e.g., duplicate copies, backups)."
What aspect of information safety addresses improper access?,Protection.
List common protection mechanisms.,"User name/password authentication, encrypting secondary storage, firewalling network access, and advanced mechanisms for valid data access in multiuser systems."
What is the 'Read' operation in file access control?,Reading from a file.
What is the 'Write' operation in file access control?,Writing or rewriting a file.
What is the 'Execute' operation in file access control?,Loading and executing a file.
What is the 'Append' operation in file access control?,Writing new information at the end of a file.
What is the 'Delete' operation in file access control?,Deleting a file and freeing its space.
What is the 'List' operation in file access control?,Listing a file's name and attributes.
What is the 'Attribute change' operation in file access control?,Changing a file's attributes.
"Where is protection typically implemented for higher-level functions (like rename, copy, edit) that use lower-level calls?",At the lower-level calls.
"What is the most general scheme for access control, where access depends on user identity?",Access-control list (ACL).
Define 'Access-control list (ACL)'.,A list that specifies user names and the allowed access types for a file. The OS checks this list to determine access.
What are the advantages of using an Access-control list (ACL)?,It allows for complex access methodologies.
What are the disadvantages of using an Access-control list (ACL)?,"Lengthy lists (tedious to construct, especially if users are unknown) and variable-size directory entries (complicating space management)."
"In condensed Access-control lists (ACLs), what are the three user classifications?","Owner, Group, and Other."
Who is classified as the 'Owner' in condensed ACLs?,The user who created the file.
Who is classified as the 'Group' in condensed ACLs?,A set of users sharing a file and needing similar access.
Who is classified as 'Other' in condensed ACLs?,All other users not classified as Owner or Group.
What is a common approach to access control combining ACLs and user classifications?,"Combining ACLs with owner, group, and universe schemes (e.g., as seen in Solaris)."
"How many fields do UNIX permissions typically use, and what are they?","Three fields: owner, group, and universe (or others)."
How are UNIX permissions represented for each field?,"Three bits, typically 'rwx'."
What does 'r' stand for in UNIX 'rwx' permissions?,Read.
What does 'w' stand for in UNIX 'rwx' permissions?,Write.
What does 'x' stand for in UNIX 'rwx' permissions?,Execution.
"In a UNIX file listing, what does 'd' as the first character indicate?",It indicates a subdirectory.
How does Solaris show that optional ACLs are present on a file?,"A '+' is appended to the regular permissions string (e.g., '-rw-r--r--+')."
What commands are used to manage ACLs in systems like Solaris?,setfacl and getfacl.
How does Windows manage ACLs?,Through a Graphical User Interface (GUI).
"When both ACLs and group permissions are present, which typically takes precedence?",ACLs typically take precedence over group permissions due to specificity priority.
What is an alternative protection approach involving passwords and files?,Associating a password with each file.
When is associating a password with each file effective?,If passwords are random and changed often.
What are the disadvantages of associating a password with each file?,"Users have too many passwords to remember, or using a single password for all files results in all-or-none protection."
What is a more common approach than associating passwords with individual files for large-scale protection?,"Encryption of partitions or files, managed with a key password."
What aspects does directory protection in multilevel structures control?,Control over creation/deletion of files in the directory and control over a user's ability to determine file existence (listing directory contents).
"If a path name refers to a file, what access is needed for the user to access that file?",The user needs access to both the directory (or directories in the path) and the file itself.
"In acyclic/general graphs (referring to directory structures), how do access rights vary?",Access rights can be different depending on the path name used to refer to a file.
What is memory mapping in the context of file access?,An alternative file access method where a file's I/O is treated as routine memory accesses using virtual memory techniques.
What is a key benefit of using memory-mapped files?,It can lead to significant performance increases by simplifying and speeding up file access.
How does memory mapping conceptually link a file to memory?,It maps a disk block to one or more pages in memory.
What happens upon initial access to a memory-mapped file?,"It triggers demand paging, resulting in a page fault."
What is read into physical memory during the initial access of a memory-mapped file?,A page-sized portion of the file is read into a physical page.
How are subsequent reads/writes to a memory-mapped file handled?,They are handled as routine memory accesses.
How do memory-mapped files simplify and speed up file access?,By avoiding `read()` and `write()` system call overhead.
Are writes to a memory-mapped file immediately reflected on secondary storage?,"No, writes to a memory-mapped file are not necessarily immediate to secondary storage."
When are updates to a memory-mapped file generally written back to secondary storage?,"Generally, updates are written back when the file is closed."
What can happen to intermediate changes in a memory-mapped file under memory pressure?,"Under memory pressure, intermediate changes may go to swap space."
"How do some operating systems, like Solaris, handle all file I/O?","Some OS (e.g., Solaris) memory-map all file I/O, even with standard calls, to kernel address space."
Can multiple processes map the same file concurrently using memory mapping?,"Yes, multiple processes can map the same file concurrently for data sharing."
"If multiple processes map the same memory-mapped file, are writes by one process visible to others?","Yes, writes by one process are visible to others mapping the same section."
How is data sharing via memory-mapped files implemented at a low level?,It is implemented by a virtual memory map pointing to the same physical page.
Does memory mapping support copy-on-write functionality?,"Yes, it supports copy-on-write, allowing processes to share read-only data and get their own copies for modification."
What mechanism should processes use for shared data coordination when using memory-mapped files?,Processes should use mutual exclusion for shared data coordination.
How is shared memory often implemented?,Shared memory is often implemented by memory mapping files.
What are the two main steps to implement shared memory using memory-mapped files in the Windows API?,1. Create a file mapping for the file. 2. Establish a view of the mapped file in the process's virtual address space.
How does a second process access shared memory established via memory-mapped files in Windows?,The second process opens and creates a view of the same mapped file.
What does a mapped file act as in the context of Windows API shared memory?,A mapped file acts as a shared-memory object for inter-process communication.
Provide an example use case for shared memory implemented with memory-mapped files.,"Producer writes, Consumer reads (classic inter-process communication example)."
What Windows API call is used to open a file when setting up memory-mapped shared memory?,`CreateFile()` (returns `HANDLE`)
What Windows API call is used to create a file mapping object after opening the file?,`CreateFileMapping()` (uses file `HANDLE`)
What Windows API call is used to establish a view of the mapped file in the process's virtual address space?,`MapViewOfFile()` (uses mapped object `HANDLE`)
What does `CreateFileMapping()` create that allows multiple processes to access the shared memory?,"A named shared-memory object (e.g., `SharedObject`)."
"What does `MapViewOfFile()` return, and how are accesses to it handled?",It returns a pointer to the shared-memory object; accesses to this memory are accesses to the file.
"Can only the entire file be mapped using memory mapping, or just a portion?",The entire file or a portion can be mapped.
How are mapped files handled with respect to paging?,A mapped file may be demand-paged.
What Windows API call is used to remove a view of a memory-mapped file?,`UnmapViewOfFile()`.
"Define ""memory mapping"" (file access method).",File-access method where a file is mapped into a process's memory space for direct memory access.
"In Windows, what is ""file mapping""?",The first step in memory-mapping a file.
"In Windows, define ""view"" in the context of memory-mapped files.",An address range mapped in shared memory; the second step in memory-mapping a file.
"In the Windows API, what is a ""named shared-memory object""?",A section of a memory-mapped file accessible by name from multiple processes.
What is a file?,"An abstract data type representing a sequence of logical records (e.g., byte, line, complex data)."
What is the operating system's role regarding file record types?,The OS may support specific record types or leave the management of record types to the application.
What is a primary task of the operating system concerning files?,"To map the logical file concept to physical storage (e.g., hard disk, NVM)."
How does the OS handle logical records in relation to physical storage?,The OS may order logical records into physical records for storage.
What is the purpose of directories?,To organize files.
What is a single-level directory?,A directory structure where all files reside in a single directory.
What is a problem with single-level directories in multiuser systems?,"Naming problems, as unique file names are required across all users."
What is a two-level directory?,A directory structure where each user has a separate directory.
What problem do two-level directories solve?,Naming problems in multiuser systems.
What information might a two-level directory list for files?,"File name, location, length, type, owner, and times."
What is a tree-structured directory?,"A generalization of the two-level directory, allowing subdirectories for further organization."
What is an acyclic-graph directory?,A directory structure that allows sharing of subdirectories and files.
What are the complications of an acyclic-graph directory?,It complicates searching and deletion processes.
What is a general graph directory structure?,A directory structure offering complete flexibility in sharing.
What is a potential requirement for a general graph directory structure?,It may require garbage collection for unused space.
What are the main challenges associated with remote file systems?,"Reliability, performance, and security."
What do distributed information systems manage in the context of remote file systems?,"User, host, and access information for shared state."
Why is file protection needed?,It is needed on multiuser systems.
What are common types of access control for file protection?,"Read, write, execute, append, delete, and list directory."
What techniques can be used for file protection?,"Access lists, passwords, and other techniques."
What is a Multiprogramming Environment?,A system where multiple threads compete for finite resources.
What happens when a thread requests a resource that is unavailable?,The thread enters a waiting state.
Define Deadlock (conceptual).,A situation where a waiting thread can never change state because its requested resources are held by other waiting threads.
Provide the formal definition of Deadlock.,Every process in a set is waiting for an event that can only be caused by another process in the set. (This definition also applies to threads.)
What constitutes the System Composition in the context of resource management?,A finite number of resources distributed among competing threads.
How are Resource Types organized in a system model?,"Resources are partitioned into types (classes), each with identical instances."
Give examples of Resource Types.,"CPU cycles, files, I/O devices."
Explain the concept of identical instances for a resource type.,"Any instance of a resource type should satisfy a request (e.g., if a system has 4 CPUs, any one CPU can satisfy a request for a CPU)."
Which Synchronization Tools are common sources of deadlock?,Mutex locks and semaphores.
How are individual lock instances typically classified as resources?,"Each lock instance is typically its own resource class (e.g., one lock for a queue, another for a linked list)."
What are the three steps in the Resource Utilization Sequence for a thread?,"1. Request
2. Use
3. Release"
"Describe the ""Request"" step in resource utilization.",A thread requests a resource and waits if it is not immediately available.
"Describe the ""Use"" step in resource utilization.",A thread operates on the requested resource.
"Describe the ""Release"" step in resource utilization.",A thread releases the resource after use.
How do threads typically perform resource requests and releases at the system level?,"Via system calls (e.g., `request()`, `release()`, `open()`, `close()`, `allocate()`, `free()`) or via semaphore operations (`wait()`, `signal()`) and mutex locks (`acquire()`, `release()`)."
How does the Operating System (OS) manage resource allocation?,The OS checks for resource allocation via a system table.
What information does the OS's system table track for resource management?,Free/allocated resources and the owning thread for allocated resources.
What happens to threads waiting for requested resources?,They are queued for those resources.
What are the main events that threads in a deadlocked state wait for?,Resource acquisition and release.
Provide an example illustrating a Deadlocked State.,"The Dining-philosophers problem, where each philosopher holds one chopstick and waits for another, creating a circular wait."
What is the developer's responsibility regarding deadlock?,Developers must be aware of deadlock possibilities.
How do locking tools relate to race conditions and deadlocks?,Locking tools prevent race conditions but require careful management of lock acquisition/release to avoid deadlocks.
What concept is illustrated by the Pthreads example in multithreaded applications?,Deadlock with POSIX mutex locks.
What is the function of `pthread_mutex_init()`?,Initializes an unlocked mutex.
What is the function of `pthread_mutex_lock()`?,Acquires a lock; blocks if the lock is held.
What is the function of `pthread_mutex_unlock()`?,Releases a lock.
Which two mutex locks are used in the Pthreads deadlock scenario?,"`first_mutex`, `second_mutex`."
Which two threads are involved in the Pthreads deadlock scenario?,"`thread_one`, `thread_two`."
"In the Pthreads deadlock scenario, in what order does `thread_one` acquire locks?","(1) `first_mutex`, (2) `second_mutex`."
"In the Pthreads deadlock scenario, in what order does `thread_two` acquire locks?","(1) `second_mutex`, (2) `first_mutex`."
Describe the condition under which deadlock is possible in the Pthreads scenario with `first_mutex` and `second_mutex`.,"If `thread_one` acquires `first_mutex` and `thread_two` acquires `second_mutex`, both threads will block waiting for the other's lock."
What is a significant characteristic of deadlock occurrence?,It is intermittent.
Why might a potential deadlock not always occur?,If one thread acquires and releases both locks before the other thread starts.
What system component can influence whether a deadlock occurs?,The CPU scheduler.
What difficulty does the intermittent nature of deadlocks present?,Makes identifying and testing for deadlocks difficult.
What kind of failure is livelock classified as?,A liveness failure.
How is livelock similar to deadlock?,Both are liveness failures.
What is a key distinction between livelock and deadlock regarding thread state?,"In livelock, threads are not blocked, unlike deadlock."
Define livelock.,A condition in which a thread continuously attempts an action that fails.
Provide an analogy that illustrates livelock.,"Two people trying to pass in a hallway, repeatedly moving into each other's way. They are active but make no progress."
"Which Pthreads function is used in the livelock example, and what is its purpose?",`pthread_mutex_trylock()` attempts to acquire a lock without blocking.
Describe the livelock scenario involving `pthread_mutex_trylock()` and two threads.,"`thread_one` acquires `first_mutex`, `thread_two` acquires `second_mutex`. Both then call `trylock` on the other mutex, which fails. They release their locks and repeat indefinitely."
What often causes livelock to occur?,When threads retry failing operations at the same time.
How can livelock be avoided?,By having threads retry failing operations at random times.
What real-world example demonstrates a mechanism similar to livelock avoidance?,Ethernet: Hosts involved in a network collision backoff for a random period before retransmitting.
"How common is livelock compared to deadlock, and is it still a concern?","Less common than deadlock, but still a challenge in concurrent application design."
What are the four necessary conditions for a deadlock to occur?,"Mutual exclusion, Hold and wait, No preemption, Circular wait."
"Define ""Mutual exclusion"" as a necessary condition for deadlock.",At least one resource must be held in a nonsharable mode.
"Define ""Hold and wait"" as a necessary condition for deadlock.",A thread must be holding at least one resource and waiting to acquire additional resources held by other threads.
"Define ""No preemption"" as a necessary condition for deadlock.",A resource can be released only voluntarily by the thread holding it.
"Define ""Circular wait"" as a necessary condition for deadlock.","A set of waiting threads {T0, T1, ..., Tn} must exist such that T0 is waiting for a resource held by T1, T1 is waiting for a resource held by T2, ..., Tn-1 is waiting for a resource held by Tn, and Tn is waiting for a resource held by T0."
"For a deadlock to occur, must all four necessary conditions hold simultaneously?",Yes.
"Which of the four necessary deadlock conditions implies the ""Hold and wait"" condition?",The circular-wait condition.
What is a system resource-allocation graph?,"A directed graph used for precise description of deadlocks, consisting of a set of vertices V and a set of edges E."
How are the vertices (V) in a system resource-allocation graph partitioned?,"Into two types: T (set of all active threads: {T1, T2, ..., Tn}) and R (set of all resource types: {R1, R2, ..., Rm})."
"In a system resource-allocation graph, what is a ""request edge""?","A directed edge from thread Ti to resource type Rj (Ti → Rj), signifying that Ti has requested an instance of Rj."
"In a system resource-allocation graph, what is an ""assignment edge""?","A directed edge from resource type Rj to thread Ti (Rj → Ti), signifying that an instance of Rj has been allocated to Ti."
What does it signify if a system resource-allocation graph contains no cycles?,No thread is deadlocked.
"If a system resource-allocation graph contains a cycle, what does that imply about a deadlock?",A deadlock may exist.
When does a cycle in a system resource-allocation graph *imply* that a deadlock has occurred?,If each resource type has exactly one instance.
"If a system resource-allocation graph contains a cycle, but each resource type has *several* instances, what is the implication regarding deadlock?",A cycle is a necessary but not a sufficient condition for deadlock.
What are the three main ways to deal with deadlock?,"Ignore problem, Prevent/Avoid, and Detect/Recover."
"Describe the ""Ignore problem"" approach to deadlock handling.","Pretend deadlocks never occur (most OS, e.g., Linux, Windows)."
"Describe the ""Prevent/Avoid"" approach to deadlock handling.",Use a protocol to ensure the system *never* enters a deadlocked state.
"Describe the ""Detect/Recover"" approach to deadlock handling.","Allow the system to enter a deadlocked state, then detect and recover (some systems, e.g., databases)."
"Why is the ""ignoring problem"" solution for deadlocks common in operating systems?",Due to the infrequency of deadlocks and the cost of other methods.
Can different deadlock handling approaches be combined?,"Yes, basic approaches can be combined for an optimal solution per resource class."
"Define ""deadlock prevention.""",Methods to ensure at least one necessary condition for deadlock cannot hold.
How does Deadlock Prevention prevent deadlocks?,By constraining resource request methods.
What information must the OS be given in advance for Deadlock Avoidance?,Information on resources a thread will request/use.
What factors does the OS consider when deciding to satisfy or delay a resource request using Deadlock Avoidance?,"Currently available resources, resources allocated to each thread, and future requests/releases of each thread."
When is the Deadlock Detection and Recovery approach applied?,"If no prevention or avoidance mechanisms are in place, allowing deadlock to arise."
What two main functions do system algorithms provide in Deadlock Detection and Recovery?,"Examine system state to determine if deadlock occurred, and recover from deadlock."
What are the consequences of an undetected deadlock?,"System performance deteriorates, resources remain held, more threads deadlock, eventually requiring a manual restart."
"Can manual recovery methods used for other liveness failures (e.g., livelock) be applied to deadlock recovery?",Yes.
"Define ""deadlock avoidance.""",OS method where processes inform OS of resource use; system approves/denies requests to avoid deadlock.
How does deadlock occur?,Deadlock occurs if all four necessary conditions for deadlock hold simultaneously.
How can deadlock be prevented?,Prevent deadlock by ensuring at least one of the four necessary conditions cannot hold.
When is the mutual-exclusion condition necessary for deadlock?,The mutual-exclusion condition must hold for deadlock to occur.
What types of resources do not require mutual exclusion and thus cannot be involved in deadlock?,"Sharable resources (e.g., read-only files) do not require mutual exclusion."
Why can't deadlocks generally be prevented by denying mutual-exclusion?,"Some resources are intrinsically nonsharable (e.g., mutex locks), so mutual exclusion cannot always be denied."
What is Protocol 1 to prevent 'hold and wait'?,A thread requests and allocates all resources it needs before beginning its execution.
What is a major impracticality of Protocol 1 for preventing 'hold and wait'?,"It is impractical for dynamic resources, as a thread may not know all resources it will need at the outset."
What is Protocol 2 to prevent 'hold and wait'?,A thread requests resources only when it is holding none. It must release all current resources before requesting more.
What are the disadvantages of both protocols for preventing 'hold and wait'?,Low resource utilization and possible starvation.
What is 'low resource utilization' in the context of 'hold and wait' prevention?,"Resources are allocated but remain unused for long periods, leading to inefficient use of resources."
What is 'starvation' in the context of 'hold and wait' prevention?,"A thread waits indefinitely for popular resources, potentially never acquiring them."
What is Protocol 1 to prevent 'no preemption'?,"If a thread requests a resource and must wait, all resources it currently holds are preempted (implicitly released). The thread restarts when its old and new requested resources are all available."
What is Protocol 2 to prevent 'no preemption'?,"If a requested resource is unavailable, check if it's held by a waiting thread. If so, preempt it and allocate to the requesting thread. If not, the requesting thread waits, and its resources may be preempted by other requests. The thread restarts when new resources are allocated and preempted resources are recovered."
To what types of resources is 'no preemption' prevention often applied?,"Resources whose state can be saved and restored (e.g., CPU registers, database transactions)."
To what types of resources can 'no preemption' prevention generally not be applied?,"Mutex locks and semaphores, where deadlocks commonly occur."
What is a practical solution to prevent 'circular wait'?,Impose a total ordering of all resource types.
How does imposing a total ordering of resource types prevent 'circular wait'?,Require threads to request resources in increasing order of their enumeration (based on the total ordering).
What is an alternative rule for requesting resources when using a total ordering to prevent 'circular wait'?,"A thread requesting resource R_j must have released any resource R_i such that F(R_i) ≥ F(R_j), where F is the ordering function."
How are multiple instances of the same resource type handled when using total ordering to prevent 'circular wait'?,A single request for all needed instances of that resource type must be issued.
Does the protocol of total ordering resource types prevent 'circular wait'?,"Yes, this protocol prevents circular wait (proof by contradiction)."
What is a challenge in developing a total ordering for resources to prevent 'circular wait'?,Developing an effective ordering can be difficult for systems with many locks.
What method does Java use for lock acquisition ordering?,`System.identityHashCode(Object)`.
Does lock ordering guarantee deadlock prevention if locks are acquired dynamically?,"No, lock ordering does not guarantee deadlock prevention if locks are acquired dynamically (e.g., within a `transaction()` function)."
Define: deadlock prevention,A set of methods intended to ensure that at least one of the necessary conditions for deadlock cannot hold.
Define: deadlock avoidance,An operating system method in which processes inform the operating system of which resources they will use during their lifetimes so the system can approve or deny requests to avoid deadlock.
What is the primary characteristic of deadlock-prevention?,"It limits how resource requests are made, ensuring no necessary condition for deadlock occurs."
What are the side effects of deadlock-prevention?,Low device utilization and reduced system throughput.
What information is required by deadlock avoidance algorithms regarding resource requests?,"Additional information on resource requests, specifically the maximum resources each thread may need (a priori information)."
How does a deadlock-avoidance algorithm prevent deadlocks?,By dynamically examining the resource-allocation state to prevent the occurrence of a circular-wait condition.
"How is the ""resource-allocation state"" defined in the context of deadlock avoidance?","It is defined by the available resources, allocated resources, and maximum demands of the threads."
"What defines a ""safe state"" in a system?",A state is safe if the system can allocate resources to each thread (up to its maximum declared need) in some specific order and thus avoid deadlock.
When is a system considered to be in a safe state?,"A system is considered safe if a ""safe sequence"" exists."
"Define ""safe sequence"".","A sequence of threads <T_1, T_2, …, T_n> where for each thread T_i, its resource requests can be met by the currently available resources plus the resources held by all preceding threads T_j (j < i)."
What is the relationship between a safe state and a deadlocked state?,"A safe state is not deadlocked, while a deadlocked state is inherently unsafe."
Are all unsafe states considered deadlocks?,"No, not all unsafe states are deadlocks, but they **may** lead to deadlock."
What is the operating system's strategy regarding unsafe states?,The OS avoids unsafe states as long as the system state is safe.
What is the OS's capability in an unsafe state regarding deadlock prevention?,"In an unsafe state, the OS cannot prevent deadlocks; thread behavior controls the outcome of unsafe states."
What type of systems is the Resource-allocation-graph algorithm suitable for?,Systems with only one instance of each resource type.
"Define ""claim edge"" in a resource-allocation graph.",A dashed line (T_i → R_j) indicating that thread T_i may request resource R_j in the future.
When is a claim edge (T_i → R_j) converted to a request edge in the Resource-allocation-graph algorithm?,When thread T_i actually requests resource R_j.
When is a request edge (T_i → R_j) converted to an assignment edge in the Resource-allocation-graph algorithm?,When resource R_j is successfully allocated to thread T_i.
When is an assignment edge (R_j → T_i) converted back to a claim edge in the Resource-allocation-graph algorithm?,When resource R_j is released by thread T_i.
What is the requirement for claiming resources in the Resource-allocation-graph algorithm?,Resources must be claimed a priori (in advance).
Under what condition is a resource request granted in the Resource-allocation-graph algorithm?,A request is granted only if no cycle is formed in the graph (which is checked by a cycle-detection algorithm).
What does the formation of a cycle in a resource-allocation graph indicate?,It indicates an unsafe state.
What is the time complexity of the Resource-allocation-graph algorithm?,"O(n^2) operations, where n is the number of threads."
What type of systems is the Banker's algorithm applicable to?,Systems with multiple instances of each resource type.
How does the efficiency of the Banker's algorithm compare to the Resource-allocation graph scheme?,The Banker's algorithm is less efficient.
What must a new thread declare when it enters the system under the Banker's algorithm?,"The maximum instances of each resource type it may need, which cannot exceed the total system resources."
Under what condition is a resource request granted by the Banker's algorithm?,Only if the allocation of the requested resources leaves the system in a safe state.
"Describe the ""Available"" data structure in the Banker's algorithm.",A vector of length m (number of resource types) indicating the number of available resources of each type.
"Describe the ""Max"" data structure in the Banker's algorithm.","An n × m matrix (n threads, m resource types) defining the maximum demand of each thread for each resource type."
"Describe the ""Allocation"" data structure in the Banker's algorithm.","An n × m matrix (n threads, m resource types) indicating the number of resources of each type currently allocated to each thread."
"Describe the ""Need"" data structure in the Banker's algorithm and its calculation.","An n × m matrix (n threads, m resource types) representing the remaining resource need of each thread. It is calculated as Need[i][j] = Max[i][j] - Allocation[i][j]."
What are the initial steps for the Banker's Safety Algorithm?,"Initialize Work = Available and set Finish[i] = false for all threads i = 0, 1, …, n-1."
What is the iterative step (Step 2) in the Banker's Safety Algorithm?,"Find an index i such that Finish[i] == false and the thread's Need_i is less than or equal to Work. If no such i exists, proceed to Step 4."
What updates are performed (Step 3) in the Banker's Safety Algorithm when a suitable thread T_i is found?,"Work = Work + Allocation_i and Finish[i] = true. Then, return to Step 2."
What is the final condition (Step 4) for a system to be in a safe state according to the Banker's Safety Algorithm?,"If Finish[i] == true for all threads i, the system is in a safe state."
What is the time complexity of the Banker's Safety Algorithm?,"O(m × n^2) operations, where m is the number of resource types and n is the number of threads."
What is Request_i in the Banker's Resource-request algorithm?,A vector representing the current resource request for thread T_i.
What is the first check performed in the Banker's Resource-request algorithm when T_i makes a request?,"Check if Request_i ≤ Need_i. If not, it means the thread has exceeded its maximum claimed resources, resulting in an error."
What is the second check performed in the Banker's Resource-request algorithm if Request_i ≤ Need_i?,"Check if Request_i ≤ Available. If not, thread T_i must wait because the requested resources are not currently available."
"What ""pretend allocation"" steps are taken in the Banker's Resource-request algorithm before checking for safety?","Available = Available - Request_i, Allocation_i = Allocation_i + Request_i, and Need_i = Need_i - Request_i."
"What is the final decision process after a ""pretend allocation"" in the Banker's Resource-request algorithm?","If the resulting state is safe (verified using the Safety Algorithm), the request is granted. Otherwise, thread T_i waits, and the system's state is restored to its original configuration."
"Define ""Banker's algorithm"".","A deadlock avoidance algorithm applicable to systems with multiple instances of each resource type, which is less efficient than the resource-allocation graph scheme."
When is deadlock detection used in a system?,It is used if the system does not employ deadlock-prevention or deadlock-avoidance schemes.
What two algorithms are needed if a system uses deadlock detection?,"An algorithm to determine if a deadlock has occurred, and an algorithm to recover from the deadlock."
What constitutes the overhead of deadlock detection and recovery?,Run-time costs of the detection algorithm and potential losses resulting from the recovery process.
What is the primary tool for deadlock detection when there is a single instance of each resource type?,A wait-for graph.
How is a wait-for graph derived from a resource-allocation graph?,It is obtained by removing resource nodes and collapsing the corresponding edges.
"In a wait-for graph, what does an edge $T_i \to T_j$ imply?",It implies that thread $T_i$ is waiting for thread $T_j$ to release a resource $R_q$ that $T_i$ needs.
How is a deadlock detected using a wait-for graph?,A deadlock exists if the wait-for graph contains a cycle.
What is the computational complexity of cycle detection in a wait-for graph?,"$O(n^2)$ operations, where $n$ is the number of vertices (threads)."
What is the purpose of the `deadlock_detector` tool in the BCC toolkit?,It is used for detecting deadlocks in Pthreads mutex locks on Linux.
How does the `deadlock_detector` tool work?,"It inserts probes to trace `pthread_mutex_lock()` and `pthread_mutex_unlock()` calls, constructs a wait-for graph, and reports a deadlock if a cycle is detected."
Is the wait-for graph scheme applicable for deadlock detection when there are several instances of a resource type?,"No, the wait-for graph scheme is not applicable in this scenario."
What is the detection algorithm for several instances of a resource type similar to?,It is similar to the banker's algorithm.
Define 'Available' vector in the context of deadlock detection for multiple resource instances.,A vector of length $m$ (number of resource types) indicating the number of available instances of each resource type.
Define 'Allocation' matrix in the context of deadlock detection for multiple resource instances.,"An $n \times m$ matrix (n threads, m resource types) indicating the number of resources of each type currently allocated to each thread."
Define 'Request' matrix in the context of deadlock detection for multiple resource instances.,"An $n \times m$ matrix (n threads, m resource types) indicating the current request of each thread; `Request[i][j] = k` means thread $T_i$ requests $k$ more instances of resource type $R_j$."
Briefly describe the detection algorithm steps for several instances of a resource type.,"1. Initialize 'Work' = 'Available' and 'Finish' flags (false if Allocation_i \u2260 0, else true). 2. Find an index $i$ where Finish[$i$] == false and Request$_i$ \u2264 Work. If none, go to step 4. 3. Work = Work + Allocation$_i$; Finish[$i$] = true. Go to step 2. 4. If Finish[$i$] == false for any $i$, then thread $T_i$ is deadlocked."
What is the computational complexity of the detection algorithm for several instances of a resource type?,"$O(m \times n^2)$ operations, where $m$ is the number of resource types and $n$ is the number of threads."
What is the 'optimistic attitude' adopted by the detection algorithm for multiple resource instances?,"It assumes that if a thread's request can be satisfied by the currently available resources (Work), that thread will complete its execution and return its allocated resources."
"In the example provided, was the initial system state (before $T_2$ requested 1 C) deadlocked?","No, the initial system state was not deadlocked. A safe sequence such as $<T_0, T_2, T_3, T_1, T_4>$ could be found."
"In the example provided, if thread $T_2$ requests 1 additional instance of resource C, does a deadlock occur?","Yes, a deadlock occurs. The system becomes deadlocked, involving threads $T_1, T_2, T_3, T_4$."
What factors influence when to invoke a deadlock detection algorithm?,The likelihood of a deadlock occurring and the number of threads that would be affected if a deadlock happens.
What happens to resources allocated to deadlocked threads?,"They become idle, leading to resource waste."
What is the most extreme frequency for invoking a deadlock detection algorithm?,Invoking it every time a resource request cannot be granted immediately.
What is a benefit of invoking a deadlock detection algorithm every time a resource request cannot be granted?,It identifies the deadlocked threads and the specific thread that 'caused' the deadlock.
What is a drawback of invoking a deadlock detection algorithm every time a resource request cannot be granted?,It incurs high computational overhead.
What is a less expensive strategy for invoking a deadlock detection algorithm?,"Invoking it at defined intervals (e.g., hourly, or when CPU utilization drops below a certain threshold like 40%)."
What is a drawback of invoking a deadlock detection algorithm at defined intervals?,It may not identify the exact thread that 'caused' the deadlock.
How do database systems typically manage deadlocks?,They use deadlock detection and recovery mechanisms.
"In database systems, what are updates referred to as, and what are locks used for?","Updates are referred to as 'transactions', and locks are used to ensure data integrity during multiple concurrent transactions."
How does a database server detect deadlocks?,It periodically searches for cycles in the wait-for graph of transactions.
What steps are taken when a deadlock is detected in a database system?,"A victim transaction is selected, aborted, and rolled back (releasing its locks). The remaining transactions are freed from the deadlock, and the aborted transaction is reissued."
How might a victim transaction be chosen in a database deadlock scenario?,"The choice often minimizes the cost of recovery, for example, by selecting the transaction that has performed the fewest inserts, updates, or deletes (e.g., as in MySQL)."
Definition: wait-for graph,"In deadlock detection, a variant of the resource-allocation graph with resource nodes removed; it indicates a deadlock if the graph contains a cycle."
Definition: thread dump,"In Java, a snapshot of the state of all threads in an application; a useful debugging tool for deadlocks."
"When deadlock is detected, what are the two main options for recovery?","1. Inform operator (manual recovery).
2. System recovers automatically."
What are the two primary options for breaking a deadlock once detected?,"1. Abort one or more threads (to break circular wait).
2. Preempt resources from deadlocked threads."
How does 'process and thread termination' contribute to eliminating deadlocks?,"By aborting processes/threads, which allows the system to reclaim all resources held by them."
Describe the 'Abort all deadlocked processes' method for deadlock recovery.,"This method immediately breaks the deadlock cycle. However, it is expensive because it discards all computations by the involved processes, necessitating recomputation."
What is the 'Abort one process at a time until deadlock eliminated' method for deadlock recovery?,This method involves aborting processes sequentially until the deadlock is resolved. It incurs high overhead because deadlock detection must be performed after each individual abort.
What are some issues that can arise when aborting a process for deadlock recovery?,"1. Files might be left in an incorrect state if they were being updated.
2. Shared data integrity issues can occur if the process was updating data while holding a mutex lock (the lock status must be restored)."
"If partial termination is chosen for deadlock recovery, how is the 'victim' process typically determined?","It is a policy decision, often based on economic factors to minimize cost."
What factors are considered when choosing a 'victim' process to terminate (to minimize cost) during deadlock recovery?,"1. Process priority.
2. Computation time (how long computed, how much longer it needs).
3. Resources used (types, ease of preemption).
4. Resources needed to complete.
5. Number of processes to terminate."
What is 'resource preemption' in the context of deadlock recovery?,It involves successively preempting resources from processes and allocating them to other processes until the deadlock is broken.
What is the 'Selecting a victim' issue in resource preemption for deadlock recovery?,"It involves determining which specific resources or processes to preempt, with the goal of minimizing the cost (e.g., considering resources held or time consumed by the process)."
Explain the 'Rollback' issue concerning resource preemption for deadlock recovery.,"When a resource is preempted, the process that held it cannot continue normal execution. The 'Rollback' issue addresses what to do with this preempted process, typically requiring it to roll back to a safe state and restart."
What is the simplest method for 'rollback' in resource preemption?,"Total rollback, which means aborting the process and restarting it from the beginning."
What is a more effective (but complex) method for 'rollback' in resource preemption?,"Rolling back only as necessary, which requires more detailed state information about the process."
"What is the 'Starvation' issue in resource preemption, and why is it a concern?","Starvation occurs when resources are always preempted from the same process, preventing it from ever completing its execution. It's a concern because it means a process never makes progress."
How is the 'starvation' issue commonly addressed in resource preemption?,"A common solution is to include the number of times a process has been rolled back (or picked as a victim) in the cost factor used for victim selection, ensuring it is picked a finite number of times."
Define 'recovery mode'.,A system boot state that provides limited services and is designed to enable the system administrator to repair system problems and debug system startup.
What is a deadlock?,A set of processes where each process is waiting for an event caused by another process in the set.
What are the four necessary conditions for deadlock?,"Mutual exclusion, Hold and wait, No preemption, Circular wait."
Define Mutual Exclusion in the context of deadlocks.,Only one process at a time can use a resource.
Define Hold and Wait in the context of deadlocks.,A process holding at least one resource is waiting to acquire additional resources held by other processes.
Define No Preemption in the context of deadlocks.,"A resource can only be released voluntarily by the process holding it, after that process has completed its task."
Define Circular Wait in the context of deadlocks.,"A set of processes {P0, P1, ..., Pn} exists such that P0 is waiting for a resource held by P1, P1 is waiting for a resource held by P2, ..., Pn-1 is waiting for a resource held by Pn, and Pn is waiting for a resource held by P0."
When is deadlock only possible?,"Only if all four necessary conditions (mutual exclusion, hold and wait, no preemption, circular wait) are present."
How are deadlocks typically modeled?,Using resource-allocation graphs.
What indicates a deadlock in a resource-allocation graph?,A cycle in the graph.
What is the goal of deadlock prevention?,To ensure that at least one of the four necessary conditions for deadlock cannot occur.
What is a practical strategy for deadlock prevention?,Eliminating circular wait.
What is the strategy for deadlock avoidance?,Using the Banker's Algorithm to not grant resources if doing so would lead to an unsafe state.
What is the strategy for deadlock detection?,An algorithm that evaluates processes and resources on a running system to find a deadlocked state.
What are two methods for deadlock recovery?,"Aborting one process in a circular wait, or preempting resources assigned to a deadlocked process."
Describe one method for deadlock recovery related to processes.,Aborting one process that is part of the circular wait.
Describe one method for deadlock recovery related to resources.,Preempting resources that are assigned to a deadlocked process.
What is the main purpose of a computer system?,To execute programs.
Where must programs and data reside for execution?,Partially in main memory.
How do modern computer systems handle multiple processes regarding memory?,They maintain several processes in memory concurrently.
What affects the effectiveness of memory-management schemes?,The specific situation or scenario.
What kind of support do most memory management algorithms require?,Hardware support.
What technique allows the CPU to be shared by processes?,CPU Scheduling.
What benefits arise from sharing the CPU among processes and keeping many processes in memory?,Improved CPU utilization and response speed.
What factor influences the selection of a memory management approach?,Hardware design.
Why is integrated hardware/OS memory management common?,Many memory management algorithms require hardware support.
How is memory structured in a modern computer system?,"As a large array of bytes, each with its own address."
What is the role of the program counter in the CPU's interaction with memory?,The CPU fetches instructions from memory based on the program counter.
What are the stages of an instruction-execution cycle?,"Fetch instruction, decode, fetch operands, execute, store results."
What are key issues pertinent to managing memory discussed in this context?,"Basic hardware, binding symbolic/virtual addresses to physical addresses, and the distinction between logical and physical addresses."
What are the only general-purpose storage components the CPU can access directly?,Main memory and registers.
What is the access speed difference between registers and main memory?,"Registers are accessible within one CPU clock cycle, while main memory accessed via a memory bus may take many CPU cycles."
"What is a ""stall"" in the context of CPU operation?","A CPU state when the CPU is waiting for data from main memory, which delays execution."
How is a CPU stall remedied?,"By adding a fast memory, called a cache, between the CPU and main memory."
Who manages the cache for speeding up memory access?,"Hardware automatically, without OS control."
How can a multithreaded core handle memory stalls?,It can switch threads during a memory stall.
What are the main concerns regarding memory access and concurrent execution?,"Correct operation, protection of the OS from user processes, and protection of user processes from each other."
How is memory protection typically implemented to ensure performance?,"By hardware, so the OS does not have to intervene for every memory access."
Why does each process need a separate memory space?,For protection and concurrent execution.
What two hardware components are used to implement memory protection by defining a legal address range?,A base register and a limit register.
What is the purpose of the base register in memory protection?,It holds the smallest legal physical memory address for a process.
What is the purpose of the limit register in memory protection?,"It defines the size of the legal memory range, in conjunction with the base register."
How does CPU hardware enforce memory protection in user mode?,It compares every address generated in user mode with the base and limit registers.
"What happens if a user-mode process attempts to access memory outside its defined legal range (e.g., OS or another user's memory)?","A trap to the OS occurs, which is considered a fatal error."
What types of modifications does the base/limit register protection mechanism prevent?,Accidental or deliberate modification of the OS or other user's code/data.
"Who is allowed to load the base and limit registers, and under what conditions?","Only the OS, using a privileged instruction while in kernel mode."
Does the OS have restricted or unrestricted access to memory?,The OS has unrestricted access to both OS and user memory.
What are some tasks the OS performs that require its unrestricted memory access?,"Loading programs, dumping programs on errors, accessing system call parameters, performing I/O, and context switches."
What is the initial state of a program on disk before execution?,A binary executable.
What steps are required for a binary executable to run?,"It must be brought into memory, placed in a process context, and then becomes eligible for execution."
What happens to memory when a process terminates?,The memory used by the process is reclaimed.
"In most systems, where can a user process reside in physical memory?",Anywhere.
"What is the first stage of address binding, involving symbolic addresses?","The compiler binds symbolic addresses (e.g., `count`) to relocatable addresses (e.g., ""14 bytes from module start"")."
"What is the second stage of address binding, involving relocatable addresses?","The linker/loader binds relocatable addresses to absolute addresses (e.g., 74014)."
What happens at each stage of address binding?,A mapping from one address space to another occurs.
"When does compile-time address binding occur, and what type of code is generated?","If the process's final memory location is known at compile time, absolute code is generated."
"When does load-time address binding occur, and what type of code is generated?","If the process's final memory location is unknown at compile time, the compiler generates relocatable code, and binding occurs at load time."
When does execution-time address binding occur?,"If the process can be moved in memory during its execution, binding is delayed until run time."
What is a logical address?,An address generated by the CPU.
What is a physical address?,"An address seen by the memory unit (i.e., loaded into the memory-address register), representing the actual location in physical memory."
When are logical and physical addresses identical?,With compile-time or load-time address binding.
When do logical and physical addresses differ?,With execution-time address binding.
What is another term for a logical address?,Virtual address.
What is a logical address space?,The set of all logical addresses generated by a program.
What is a physical address space?,The set of all physical addresses corresponding to the logical addresses generated by a program.
What hardware component performs the run-time mapping of virtual addresses to physical addresses?,The Memory-Management Unit (MMU).
Describe a simple MMU scheme for address translation.,It uses a relocation register (a generalization of the base register) whose value is added to every logical address generated by a user process to create the physical address.
Do user programs directly access real physical addresses?,"No, user programs only deal with logical addresses."
When is the final location of a referenced memory address determined in an MMU system?,At the time of reference (run time).
Why is the concept of a logical address space bound to a separate physical address space central to memory management?,"It allows for proper memory management, including protection and flexibility in process placement."
What was the traditional requirement for program execution regarding memory?,The entire program and data had to be in physical memory.
What was a limitation of the traditional memory loading approach?,Process size was limited by the physical memory size.
What technique improves memory-space utilization by loading routines only when needed?,Dynamic loading.
How does dynamic loading work?,"All routines are kept on disk in relocatable load format. The main program is loaded and executed. When a routine calls another, it checks if the routine is loaded; if not, a relocatable linking loader loads it, updates address tables, and passes control."
What is the main advantage of dynamic loading?,A routine is loaded into memory only when it is needed.
For what types of programs is dynamic loading particularly useful?,"Programs with large amounts of code that handle infrequent cases, such as error routines."
What is the relationship between the total program size and the loaded portion in dynamic loading?,"The total program size can be very large, but the portion actually used (and thus loaded) is much smaller."
Does dynamic loading require special OS support?,"No, it is generally the user's responsibility, although the OS may provide library routines to help."
What is the definition of dynamically linked libraries (DLLs)?,System libraries that are linked to user programs at run time.
How does static linking differ from dynamic linking?,"In static linking, system libraries are treated like object modules and combined by the loader into the binary program image, whereas in dynamic linking, linking is postponed until execution time."
What is the primary use case for dynamic linking?,"It is usually used with system libraries, such as the standard C library."
"What is a disadvantage of not using DLLs (i.e., using static linking for libraries)?","Each program includes a copy of the language library in its executable image, which increases executable size and wastes main memory."
What is a key advantage of DLLs related to memory usage?,"They can be shared among multiple processes, with only one instance loaded into memory."
What is another term for dynamically linked libraries when they are shared among processes?,Shared libraries.
How do programs reference a dynamic library routine?,"The loader locates the DLL, loads it if needed, and adjusts addresses referencing DLL functions to the DLL's memory location."
What is an advantage of DLLs concerning library updates and bug fixes?,"When a library is replaced by a new version, all programs referencing it automatically use the new version without needing to be relinked."
How is backward compatibility managed with dynamic linking and library updates?,"Version information is included in the program and library. Multiple library versions can be loaded, and programs use their specific version info. Minor changes retain the same version number, while major changes increment it."
Do dynamic linking and shared libraries generally require OS help?,Yes.
How does the OS facilitate shared libraries when processes are protected?,The OS checks if a routine is already in another process's memory and allows multiple processes to access the same addresses for shared libraries.
"Define ""stall"" in the context of CPU operation.","A CPU state when the CPU is waiting for data from main memory, which delays execution."
"Define ""cache"".",A temporary copy of data in a reserved memory area used to improve performance.
"Define ""base register"".","A CPU register holding the starting address of an address space, which defines a logical address space when used with a limit register."
"Define ""limit register"".","A CPU register defining the size of a memory range, which defines a logical address space when used with a base register."
"Define ""bind"" in the context of addresses.","To tie together; for example, a compiler binds a symbolic address to a relocatable address."
"Define ""absolute code"".",Code with bindings to absolute memory addresses.
"Define ""relocatable code"".",Code with bindings to memory addresses that can be changed at loading time to reflect the actual location in main memory.
"Define ""logical address"".",An address generated by the CPU; it is translated to a physical address before use.
"Define ""physical address"".",The actual location in physical memory of code or data.
"Define ""virtual address"".",An address generated by the CPU; it is translated to a physical address before use (synonymous with logical address).
"Define ""logical address space"".",The set of all logical addresses generated by a program.
"Define ""physical address space"".",The set of all physical addresses generated by a program.
"Define ""memory-management unit (MMU)"".",A hardware component of the CPU or motherboard that allows memory access.
"What does ""MMU"" stand for?",Memory-Management Unit.
"Define ""relocation register"".",A CPU register whose value is added to every logical address to create the corresponding physical address.
"Define ""dynamic loading"".","The loading of a process routine only when it is called, rather than at the start of the process."
"Define ""dynamically linked libraries (DLLs)"".","System libraries that are linked to user programs at run time, with the linking postponed until execution time."
"Define ""static linking"".",Linking where system libraries are treated like object modules and combined by the loader into the binary program image.
"Define ""shared libraries"".","Libraries that are loaded once and then used by many processes, typically found in systems supporting dynamic linking."
What does main memory accommodate?,The Operating System (OS) and user processes.
How is memory usually divided for the OS and user processes?,"Into two partitions: one for the OS, and one for user processes."
Where can the OS reside in memory?,"In low or high memory. Many OS, including Linux/Windows, use high memory."
How many user processes typically reside in memory concurrently?,Several.
Define Contiguous Memory Allocation.,A memory allocation method where each process is in a single contiguous memory section.
What is the primary goal of memory protection?,To prevent a process from accessing memory it does not own.
Which two registers are combined to implement memory protection?,The relocation register and the limit register.
What is the purpose of the relocation register in memory protection?,It holds the smallest physical address (base address) that a process can access.
What is the purpose of the limit register in memory protection?,It specifies the range of logical addresses that a process can access.
How is a logical address validated in a relocation-register scheme?,Each logical address must fall within the range specified by the limit register.
How does the MMU (Memory Management Unit) map a logical address dynamically using the relocation-register scheme?,By adding the relocation register's value to the logical address.
What is the CPU scheduler's role regarding relocation and limit registers during a context switch?,It loads the relocation and limit registers.
When are CPU-generated addresses checked against the relocation and limit registers?,Every CPU-generated address is checked.
What does the relocation-register scheme allow regarding OS size?,Dynamic changes to the OS size.
Why is the relocation-register scheme desirable for device drivers?,It allows them to be loaded only when needed and removed when no longer needed.
What is the simplest method for memory allocation?,Assigning processes to variably sized partitions.
Define Variable-Partition scheme.,A memory-allocation scheme where each memory partition contains exactly one process.
What does the OS keep track of regarding memory parts in variable-partition allocation?,A table of available and occupied memory parts.
What is the initial state of memory for user processes in a variable-partition scheme?,"All memory is available for user processes as one large block, known as a 'hole'."
What does memory contain after some processes have been loaded and removed?,A set of 'holes' of various sizes scattered throughout memory.
What does the OS consider when a process arrives and requires memory?,Its memory requirements and the available space (holes).
What happens if a process terminates and releases its memory block?,"The OS provides the released memory to another process, and the block is returned to the set of available 'holes'."
What action is taken if a new hole is adjacent to existing holes upon process termination?,The new hole is merged with the adjacent ones to form a larger hole.
Define Dynamic Storage-Allocation Problem.,The problem of satisfying a memory request of size 'n' from a list of free 'holes'.
Name the three common strategies for selecting a free 'hole' in memory allocation.,"First-fit, Best-fit, and Worst-fit."
Define First-Fit in memory allocation.,"In memory allocation, selecting the first hole large enough for a request, searching from the beginning or the last search end. The search stops when a large enough hole is found."
Define Best-Fit in memory allocation.,"In memory allocation, selecting the smallest hole large enough for a request."
Define Worst-Fit in memory allocation.,"In memory allocation, selecting the largest hole available."
What is typically required when using the Best-Fit strategy?,The entire list of holes must be searched (unless ordered by size).
What kind of leftover hole does Best-fit allocation produce?,The smallest leftover hole.
What kind of leftover hole does Worst-fit allocation produce?,The largest leftover hole.
How do First-fit and Best-fit compare to Worst-fit in simulations?,"First-fit and Best-fit are generally better than Worst-fit, both in terms of decreasing time and improving storage utilization."
"Which memory allocation strategy, First-fit or Best-fit, is generally faster?",First-fit.
Define External Fragmentation.,"A type of fragmentation where available memory has holes that together are enough to satisfy a request, but no single hole is large enough. The storage is fragmented and non-contiguous."
How does external fragmentation occur?,"When processes are loaded and removed from memory, free memory is broken into many small, noncontiguous pieces."
What is the 50-percent rule in memory fragmentation?,"A statistical finding that for N allocated blocks, 0.5 N blocks are lost to fragmentation, meaning one-third of memory may become unusable."
What are the two types of memory fragmentation?,Internal fragmentation and external fragmentation.
Define Internal Fragmentation.,Unused memory that is internal to a partition.
When does internal fragmentation occur?,"When allocated memory is slightly larger than the requested memory (e.g., in fixed-sized blocks)."
What is a common solution to external fragmentation?,Compaction.
Define Compaction in memory management.,Shuffling storage contents to consolidate all used space and create one or more large contiguous blocks of free memory (holes).
Under what condition is compaction possible?,Only if relocation is dynamic (execution time). It is not possible if relocation is static (assembly/load time).
What actions are required during compaction if dynamic relocation is enabled?,Move the program/data and change the base register.
Is compaction generally expensive or inexpensive?,"It can be expensive (e.g., moving all processes to one end)."
What is another solution to external fragmentation besides compaction?,Permitting a noncontiguous logical address space.
Which common memory-management technique permits noncontiguous logical address space to solve external fragmentation?,Paging.
What is the most common memory-management technique?,Paging.
What memory management challenge did paging address?,The requirement for contiguous physical address space.
What is Paging?,A memory-management scheme allowing noncontiguous physical address space.
What issues does Paging avoid?,External fragmentation and compaction issues.
Why is Paging widely used?,"Due to its advantages, it is used in most operating systems, from servers to mobile devices."
How is Paging implemented?,Through OS and hardware cooperation.
What are fixed-sized blocks of physical memory called in paging?,Frames.
What are fixed-sized blocks of logical memory called in paging?,Pages.
How are pages loaded during process execution in a paging system?,"Pages are loaded into any available memory frames, either from the file system or backing store."
How is the backing store divided in a paging system?,"Into fixed-sized blocks, typically the same size as frames or clusters."
What is the relationship between logical address space and physical address space in paging?,They are totally separate.
What are the two parts of a CPU-generated address in a paging system?,Page number (p) and page offset (d).
What is the page number (p) in a CPU-generated address?,It is an index into the per-process page table.
What is the page offset (d) in a CPU-generated address?,It is the location within the referenced frame.
What does a page table contain?,The base address of each frame in physical memory.
How is the physical memory address calculated in a paging system?,Base address of frame + page offset.
List the MMU steps to translate a logical address to a physical address in a paging system.,"1. Extract page number 'p', use as index into page table. 2. Extract corresponding frame number 'f' from page table. 3. Replace page number 'p' with frame number 'f'."
Does the page offset 'd' change during logical to physical address translation?,"No, the offset 'd' does not change."
Who defines the page size (and thus frame size) in a paging system?,Hardware.
"What characteristic do page sizes typically have, and what is their typical range?","They are a power of 2, typically ranging from 4 KB to 1 GB."
Why is a power of 2 page size beneficial?,It allows for easy translation of a logical address into its page number and offset.
"Given a logical address space of 2^m bytes and a page size of 2^n bytes, how are the page number and page offset determined?","The high-order m-n bits represent the page number, and the low-order n bits represent the page offset."
What form of relocation is paging considered?,Dynamic relocation.
How are logical addresses bound to physical addresses in a paging system?,Every logical address is bound by paging hardware to a physical address.
Does paging suffer from external fragmentation?,"No, because any free frame can be allocated."
Does paging suffer from internal fragmentation?,"Yes, the last frame allocated for a process may not be completely full."
What is the average internal fragmentation per process in a paging system?,One-half page.
What is the effect of using smaller page sizes?,Less internal fragmentation.
What is the effect of using larger page sizes?,"The overhead per page-table entry is reduced, and disk I/O is more efficient with larger data transfers."
How have page sizes changed over time?,"Page sizes have generally grown over time as processes, data sets, and main memory have become larger."
What are typical modern page sizes?,4 KB or 8 KB.
Give examples of systems that support multiple page sizes.,"Windows 10 (4 KB, 2 MB) and Linux (default 4 KB, huge pages)."
What is the typical size of a 32-bit CPU's page-table entry?,4 bytes.
How much physical memory can a system address if it uses 4 KB frames and 4-byte page-table entries that can point to 2^32 physical page frames?,2^44 bytes (16 TB).
Is the physical memory size typically the same as the maximum logical size of a process?,"No, physical memory size is typically different from the maximum logical size of a process."
What information do page-table entries contain in addition to frame addresses?,Other information that reduces the bits available for frame addresses.
"When a process arrives and needs 'n' pages, how many frames must be available?",'n' frames must be available.
How does the programmer's view of memory differ from the actual physical memory in a paged system?,"The programmer views memory as a single contiguous space for one program, while the user program is actually scattered throughout physical memory, which also holds other programs."
What reconciles the programmer's view of memory with the actual physical memory layout?,Address-translation hardware.
Can a user process access memory it doesn't own in a paging system? Why or why not?,"No, because it has no way to address memory outside of its own page table."
What information does the OS manage regarding physical memory allocation?,Details about allocated/available frames and total frames.
What is the system-wide data structure that keeps information about physical memory frames?,The frame table.
What information does the frame table contain?,"One entry per physical page frame, indicating whether it's free or allocated, and to which process/page it belongs."
How does the OS handle address parameters in system calls in a paged environment?,"The OS is aware that user processes operate in user space and their logical addresses are mapped to physical addresses, so it maps the given address to the correct physical address."
What does the OS maintain for each process related to paging?,"A copy of the page table, similar to an instruction counter or registers."
When is the OS's copy of the page table used?,For manual logical-to-physical translation by the OS and by the CPU dispatcher to define the hardware page table when a process is allocated the CPU.
How does paging affect context-switch time?,Paging increases context-switch time.
Are page tables per-process or system-wide data structures?,Page tables are per-process data structures.
Where is the pointer to a process's page table stored?,"In the process control block (PCB), along with other registers."
"What happens when the CPU scheduler selects a process, regarding hardware page-table values?",It reloads user registers and hardware page-table values from the stored user page table.
What is the simplest hardware implementation for a page table?,Using dedicated high-speed hardware registers.
What are the pros and cons of using dedicated hardware registers for page tables?,"Pro: Efficient translation. Con: Increases context-switch time (due to register exchange). Feasible only for small page tables (e.g., 256 entries)."
Why are dedicated hardware registers not feasible for page tables in contemporary CPUs?,"Contemporary CPUs have much larger page tables (e.g., 2^20 entries), making registers impractical."
Where are large page tables typically kept?,In main memory.
What register points to the page table when it's kept in main memory?,The Page-Table Base Register (PTBR).
How does using a PTBR for in-memory page tables affect context-switch time?,It reduces context-switch time because only the PTBR needs to be changed to switch page tables.
What is the main problem with storing page tables in main memory?,"It results in slower memory access times, effectively doubling the number of memory accesses needed for data."
How many memory accesses are typically needed to access data if the page table is stored in main memory?,"Two memory accesses: one for the page-table entry, and one for the actual data."
What is the standard solution to mitigate the performance overhead of in-memory page tables?,"A special, small, fast-lookup hardware cache called a Translation Look-aside Buffer (TLB)."
What kind of memory is a TLB?,"Associative, high-speed memory."
What does each TLB entry consist of?,A key (tag) and a value.
How does associative memory perform a search?,"An item presented to associative memory is compared with all keys simultaneously, returning the corresponding value quickly if found."
Why does TLB lookup not typically incur a performance penalty?,It is part of the instruction pipeline.
What is the typical size range for a TLB?,"32 to 1,024 entries."
What is the purpose of separate instruction and data address TLBs in some CPUs?,"It effectively doubles the number of entries, providing more cache for translations."
Describe the process of using a TLB with page tables when the CPU generates a logical address.,"The MMU first checks if the page number is in the TLB (TLB hit). If found, the frame number is immediately available and used. If not found (TLB miss), a memory reference to the page table is made to get the frame number, which is then used to access memory. The page number and frame number are then added to the TLB."
What happens when a TLB is full and a new entry needs to be added?,"An existing entry must be replaced, using policies like LRU, round-robin, or random."
What does it mean for a TLB entry to be 'wired down'?,"It means the entry cannot be removed by the usual replacement algorithms, typically used for critical kernel code."
What are Address-Space Identifiers (ASIDs) in TLB entries?,"ASIDs uniquely identify a process and provide address-space protection. If the current process's ASID does not match the virtual page's ASID in a TLB entry, it's treated as a TLB miss."
What is an advantage of TLBs that store ASIDs?,They allow the TLB to contain entries for multiple processes simultaneously without needing to be flushed on every context switch.
"What is required for a TLB without ASIDs on each context switch, and why?",It must be 'flushed' (erased) on each context switch to prevent the next process from using incorrect translation information from old entries.
What is the hit ratio in the context of a TLB?,The percentage of times a page number is found in the TLB (a measure of TLB effectiveness).
Calculate the effective memory-access time given an 80% TLB hit ratio and a 10 ns memory access time.,Effective memory-access time = (0.80 * 10 ns) + (0.20 * 20 ns) = 8 ns + 4 ns = 12 ns.
Calculate the effective memory-access time given a 99% TLB hit ratio and a 10 ns memory access time.,Effective memory-access time = (0.99 * 10 ns) + (0.01 * 20 ns) = 9.9 ns + 0.2 ns = 10.1 ns.
Describe the multi-level TLB architecture in modern CPUs like Intel Core i7.,"Modern CPUs can have multiple TLB levels, such as L1 instruction TLB, L1 data TLB, and L2 TLB. A miss at L1 checks L2; a miss at L2 requires walking page-table entries in memory or an OS interrupt."
How is memory protection implemented in a paged environment?,Through protection bits associated with each frame in the page table.
What can a single protection bit in a page table indicate?,Whether a page is read-write or read-only.
What happens if a process attempts to write to a read-only page?,"The hardware generates a trap to the OS, indicating a memory-protection violation."
What types of finer protection can be implemented with page-table bits?,"Read-only, read-write, execute-only (separate bits for each access type)."
What is the purpose of the 'valid-invalid' bit in a page table entry?,It indicates whether the page is part of the process's logical address space (valid) or not (invalid).
Who sets the valid-invalid bit for each page?,The operating system.
How does the valid-invalid bit help in handling illegal addresses?,"If a process tries to access a page marked invalid, the system traps to the OS, indicating an illegal address."
What is a potential issue with relying solely on the valid-invalid bit for protection with programs that don't use their full logical address range?,"It may allow access to valid, allocated pages beyond the program's actual used range (e.g., if a program uses up to address 10468, but its page 5 extends to 12287, all addresses in page 5 are valid, reflecting internal fragmentation)."
What register is used in some systems to indicate the size of the page table?,The Page-Table Length Register (PTLR).
How does the PTLR provide protection?,Its value is checked against every logical address to verify it is within the valid range of the page table. A test failure results in an error trap to the OS.
"What is a significant advantage of paging, especially in a multi-process environment?","The possibility of sharing common code, such as the standard C library (libc)."
What is the problem if each process loads its own copy of a common library like libc?,"It leads to significant memory waste. For example, 40 processes each loading a 2 MB libc would consume 80 MB of memory."
What characteristic must code have to be shared among multiple processes?,It must be reentrant code.
What is reentrant code?,"Non-self-modifying code that never changes during execution, allowing two or more processes to execute the same code simultaneously."
How do processes use shared reentrant code in a paged system?,"Only one copy of the reentrant code (e.g., libc) exists in physical memory. Each user process's page table maps to this same physical copy, while each process has its own copy of registers and data storage."
What are examples of other programs besides libc that can be shared via paging?,"Compilers, window systems, and database systems."
How are shared libraries (from dynamic linking) typically implemented?,Using shared pages.
What must the OS enforce regarding shared code?,The read-only nature of the shared code.
How is sharing memory among processes similar to sharing address space by threads?,"Both involve multiple entities accessing the same underlying memory resources, facilitated by mechanisms like shared pages."
How is shared memory for interprocess communication (IPC) implemented using paging?,"Through shared pages, where different processes can map the same physical pages into their respective logical address spaces."
paging,Memory management scheme avoiding external fragmentation by splitting physical memory into fixed-sized frames and logical memory into pages.
frames,Fixed-sized blocks of physical memory.
page,Fixed-sized block of logical memory.
page number (p),Part of CPU-generated memory address in paged system; index into page table.
page offset (d),Part of CPU-generated memory address in paged system; offset of location within page.
page table,"Table in paged memory containing base address of each physical memory frame, indexed by logical page number."
huge pages,Feature designating a region of physical memory for especially large pages.
frame table,"Table in paged memory containing frame details (allocated/free, total frames)."
page-table base register (PTBR),CPU register pointing to the in-memory page table.
translation look-aside buffer (TLB),"Small, fast-lookup hardware cache in paged memory address translation for fast access to a subset of addresses."
TLB miss,TLB lookup failing to provide address translation because it's not in TLB.
wired down,"TLB entry locked into TLB, not replaceable by usual algorithm."
address-space identifier (ASIDs),Part of TLB entry identifying the associated process; causes a TLB miss if the requesting process ID doesn't match.
flush (TLB),Erasure of entries in TLB or other cache to remove invalid data.
hit ratio,"Percentage of times a cache provides a valid lookup (e.g., TLB effectiveness measure)."
effective memory-access time,Statistical or real measure of CPU time to read/write to memory.
valid-invalid bit,Page-table bit indicating if an entry points to a page within a process's logical address space.
page-table length register (PTLR),CPU register indicating the size of the page table.
reentrant code,Code supporting multiple concurrent threads (can be shared) because it is non-self-modifying.
What are common techniques for structuring page tables?,"Hierarchical paging, hashed page tables, and inverted page tables."
What is the typical size of logical address spaces supported by modern computer systems?,$2^{32}$ to $2^{64}$ bits.
What problem arises with page tables when supporting large logical address spaces?,The page table itself becomes excessively large.
"For a 32-bit logical address space with a 4 KB page size, how many entries would a traditional page table have?",Over 1 million entries ($2^{20}$).
"Given a 32-bit logical address space, 4 KB page size, and 4-byte entries, how much physical address space would the page table alone consume per process?",Up to 4 MB.
What is a common solution to the problem of excessively large page tables?,Divide the page table into smaller pieces.
What is the 'two-level paging algorithm'?,A method where the page table itself is paged.
"In a two-level paging algorithm for a 32-bit logical address space with a 4 KB page size, how is the logical address divided?",Into a 20-bit page number and a 12-bit page offset.
"In a two-level paging algorithm for a 32-bit logical address space, how is the 20-bit page number further divided?","$p_1$ (10-bit outer page number, index into outer page table) and $p_2$ (10-bit inner page offset, displacement within inner page table)."
How does address translation occur in a two-level paging scheme?,From the outer page table inward.
What is a forward-mapped page table?,A scheme for hierarchical page tables where address translation starts at the outer page table and moves inward.
Why is a two-level paging scheme inappropriate for a 64-bit logical address space?,"Even with a 4 KB page size, the page table could have up to $2^{52}$ entries, resulting in an outer page table of $2^{42}$ entries or 16 GB, which is still too large."
What is a three-level paging scheme?,A hierarchical paging method that further pages the outer page table of a two-level scheme.
Why are hierarchical page tables generally inappropriate for 64-bit architectures?,"Because they would require too many levels of paging (e.g., seven for 64-bit UltraSPARC), leading to prohibitive memory accesses."
What is the primary approach that hashed page tables handle?,Handling address spaces larger than 32 bits.
What is used as the hash value in a hashed page table?,The virtual page number.
How do hashed page tables handle collisions?,Each entry in the hash table points to a linked list of elements.
What three fields does each element in a hashed page table's linked list consist of?,"1. Virtual page number.
2. Value of the mapped page frame.
3. Pointer to the next element in the linked list."
Describe the algorithm for a hashed page table lookup.,"The virtual page number in the virtual address is hashed into the hash table. The virtual page number is compared with field 1 in the first element of the linked list. If they match, the corresponding page frame (field 2) is used to form the physical address. If no match, subsequent entries in the linked list are searched."
What is a clustered page table?,"Similar to a hashed page table, but an entry refers to a cluster of several pages (e.g., 16) instead of a single page."
"What is a benefit of clustered page tables, and for what type of address spaces are they useful?","A single page-table entry stores mappings for multiple physical-page frames, making them useful for sparse address spaces where memory references are noncontiguous or scattered."
"In memory management, what does 'sparse' describe?","A page table with noncontiguous, scattered entries; an address space with many holes."
What is a significant drawback of standard page tables in terms of memory consumption?,"Each page table may consist of millions of entries, consuming large amounts of physical memory."
What is an inverted page table?,"A page-table scheme with one entry for each real physical page frame in memory, mapping to a logical page (virtual address) value."
What information does each entry in an inverted page table contain?,"The virtual address of the page stored in that real memory location, plus process information (often an address-space identifier or process-id)."
How many page tables are typically present in a system utilizing inverted page tables?,"Only one page table, with one entry per physical memory page."
Which systems are examples of those that use inverted page tables?,64-bit UltraSPARC and PowerPC.
"In the IBM RT simplified version of an inverted page table, what is the structure of the virtual address and the inverted page-table entry?","Virtual address: <process-id, page-number, offset>. Inverted page-table entry: <process-id, page-number>."
Describe the memory reference process in the IBM RT simplified version of an inverted page table.,"The <process-id, page-number> from the virtual address is presented to the memory subsystem. The inverted page table is searched for a match. If a match is found at entry 'i', the physical address <i, offset> is generated. If no match, it's an illegal address access."
What is a main drawback of inverted page tables concerning search time?,"It increases the time to search the table because the table is sorted by physical address, but lookups are by virtual address."
How can the increased search time in inverted page tables be alleviated?,By using a hash table to limit the search.
What is the performance implication of using a hash table with inverted page tables for address translation?,"Each access to the hash table adds a memory reference, meaning one virtual memory reference requires at least two real memory reads (one for the hash-table entry, one for the page table entry)."
What is searched first to improve performance with inverted page tables?,The TLB (Translation Lookaside Buffer).
"What issue arises with shared memory when using inverted page tables, compared to standard paging?","Standard paging allows multiple virtual addresses to map to the same physical address. Inverted page tables only have one virtual page entry for every physical page, meaning one physical page cannot have two or more shared virtual addresses. If another process sharing memory references it, it might cause a page fault and replace the mapping."
What is characteristic of the Oracle SPARC Solaris approach to virtual memory?,Modern 64-bit CPU and OS are tightly integrated for low-overhead virtual memory.
What operating system and CPU are discussed in the Oracle SPARC Solaris section?,Solaris running on SPARC CPU.
How does Oracle SPARC Solaris efficiently solve the virtual memory problem?,By using hashed page tables.
"How many hash tables does Oracle SPARC Solaris use for virtual memory, and what do they map?",Two hash tables: one for the kernel and one for all user processes. Both map virtual to physical memory.
"What do the hash-table entries in Oracle SPARC Solaris represent, and why is this efficient?","Each entry represents a contiguous area of mapped virtual memory (a 'span'), which is more efficient than a per-page entry."
What two fields does an Oracle SPARC Solaris hash-table entry include?,Base address and span (number of pages represented).
What is the TLB (Translation Lookaside Buffer)?,A cache that holds translation table entries (TTEs) for fast hardware lookups.
What is the TSB (Translation Storage Buffer) in Oracle SPARC Solaris?,A cache of TTEs (translation table entries) that includes an entry per recently accessed page.
Describe the steps of a virtual address reference process in Oracle SPARC Solaris when a translation is not immediately found in the TLB.,"1. Hardware searches TLB for translation.
2. None found, hardware walks through in-memory TSB (TLB walk).
3. Match in TSB, CPU copies TSB entry into TLB, memory translation completes.
4. No match in TSB, kernel interrupted to search hash table.
5. Kernel creates TTE from hash table, stores in TSB for automatic loading into TLB by MMU.
6. Interrupt handler returns control to MMU, completes address translation, retrieves data."
What is a TLB walk?,The steps involved in traversing page-table structures to locate a needed translation and copying the result into the TLB.
What is Solaris?,"A UNIX derivative, main operating system of Sun Microsystems (now Oracle); active open source version called Illumos."
What is SPARC?,A proprietary RISC CPU created by Sun Microsystems (now Oracle); active open source version called OpenSPARC.
Why must process instructions and data reside in memory?,They must be in memory for execution.
"What is the primary purpose of ""swapping"" in memory management?","To temporarily move a process or a portion of a process out of memory to a backing store, and then bring it back for continued execution."
"Define the term ""backing store"".",Secondary storage area used for process swapping.
Describe the characteristics of a backing store used for standard swapping.,It is a fast secondary storage that is large enough for process parts and provides direct access to memory images.
"Define the term ""swapped"".","Moved between main memory and a backing store. Process swapped out to free main memory, then swapped back in to continue execution."
What is the key benefit of swapping regarding memory utilization?,It allows the total physical address space of all processes to exceed the real physical memory.
How does swapping impact the degree of multiprogramming?,It increases the degree of multiprogramming.
"What does ""standard swapping"" involve?",Moving entire processes between main memory and a backing store.
What data structures are written to the backing store when a process (or part) is swapped out?,"Associated data structures, including per-thread data for multithreaded processes."
What does the operating system (OS) maintain for swapped-out processes?,Metadata for their restoration.
What is an advantage of standard swapping for physical memory oversubscription?,"It allows physical memory to be oversubscribed, accommodating more processes than physical memory can hold."
Which types of processes are ideal candidates for swapping out?,Idle or mostly idle processes.
What happens to memory allocated to inactive processes when they are swapped out?,It can be dedicated to active processes.
"What is required if an inactive, swapped-out process becomes active?",It must be swapped back into memory.
"Is ""standard swapping"" commonly used in contemporary operating systems?","Generally no, except for Solaris under dire circumstances."
Why is standard swapping generally no longer used in modern OS?,The time required to move entire processes is prohibitive.
"How do most contemporary systems (e.g., Linux, Windows) perform swapping?","They use a variation where individual pages of a process are swapped, not the entire process."
What benefit does swapping with paging share with standard swapping?,It still allows physical memory oversubscription.
What is an advantage of swapping with paging over standard swapping in terms of cost?,"It does not incur the high cost of swapping entire processes, as only a small number of pages are involved."
"How is the term ""swapping"" generally interpreted in contemporary discussions?","It generally refers to ""standard swapping"" (moving entire processes)."
"What does the term ""paging"" refer to?","It refers to ""swapping with paging"" (moving individual pages)."
"Define ""page out"".",The process of moving a page from memory to a backing store.
"Define ""page in"".","The reverse process of ""page out,"" where a page is moved from a backing store into memory."
How does swapping with paging relate to virtual memory?,It works well with virtual memory.
Do mobile systems typically support swapping?,No.
List the reasons why mobile systems generally do not support swapping.,"1. They use flash memory for nonvolatile storage, which has space constraints. 2. Flash memory has a limited number of writes it tolerates before becoming unreliable. 3. There is poor throughput between main memory and flash memory."
"How does Apple's iOS manage memory when free memory is low, instead of swapping?",It asks applications to voluntarily relinquish allocated memory.
"In iOS, what type of data is removed from main memory (and reloaded from flash if needed) when memory is low?","Read-only data (e.g., code)."
"In iOS, what type of data is never removed from main memory when memory is low?","Modified data (e.g., stack)."
What is the consequence for iOS applications that fail to free memory when requested?,They may be terminated by the OS.
How does Android manage memory when free memory is low?,"It uses a strategy similar to iOS, and may terminate processes if there is insufficient free memory."
What does Android do before terminating a process due to insufficient memory?,"It writes the ""application state"" to flash memory for a quick restart."
"Define the term ""application state"".",A software construct for data storage.
What is a key responsibility for developers on mobile systems regarding memory management?,They must carefully allocate and release memory to avoid excessive use or leaks.
"What does excessive ""swapping"" (in any form) often indicate about system performance?",It is often a sign of more active processes than available physical memory.
What are two general approaches to improve system performance when excessive swapping occurs?,1. Terminate some processes. 2. Get more physical memory.
What were the early 16-bit Intel chips that dominated the PC landscape?,Intel 8086 (late 1970s) and 8088 (original IBM PC).
What is IA-32?,"Intel's 32-bit chip architecture, which included Pentium processors."
What architecture are current 64-bit Intel chips based on?,x86-64 architecture.
Where does Intel dominate the market versus where it does not?,"Intel dominates PC OS (Windows, Mac, Linux) but not mobile systems, where ARM architecture is successful."
What are the two major memory-management concepts in IA-32?,Segmentation and Paging.
Describe the address translation process in IA-32 architecture.,CPU generates logical addresses → segmentation unit produces linear address → paging unit generates physical address in main memory.
What is the Memory-Management Unit (MMU) in IA-32?,The segmentation and paging units combined.
What is the maximum segment size in IA-32 segmentation?,Up to 4 GB.
What is the maximum number of segments allowed per process in IA-32 segmentation?,16 K segments.
How is the logical address space divided in IA-32 segmentation?,"Into two partitions: one for up to 8 K segments private to the process, and one for up to 8 K segments shared among all processes."
What is the Local Descriptor Table (LDT) in IA-32 segmentation?,A table that holds information for segments private to a process (first partition).
What is the Global Descriptor Table (GDT) in IA-32 segmentation?,A table that holds information for segments shared among all processes (second partition).
What is a segment descriptor in IA-32 segmentation?,"An 8-byte entry in an LDT or GDT that contains detailed information about a segment (e.g., base location, limit)."
What is the format of a logical address in IA-32 segmentation?,"(selector, offset)."
What are the components of the 16-bit selector in an IA-32 logical address?,"Segment number (s), GDT or LDT indicator (g), and protection (p)."
What is the offset in an IA-32 logical address?,A 32-bit number representing the byte location within a segment.
How many segments can be addressed at once in an IA-32 machine?,"Six, using six segment registers."
How does IA-32 optimize segment descriptor access?,Six 8-byte microprogram registers (LDT/GDT cache) hold descriptors to avoid reading from memory for every reference.
What is the length of a linear address in IA-32?,32 bits long.
How is a linear address generated from a logical address in IA-32 segmentation?,A segment register points to an LDT/GDT entry; the base and limit from the segment descriptor are used. The offset is added to the base address.
What happens if an address is invalid during IA-32 segmentation's limit check?,"A memory fault occurs, trapping to the OS."
What are the possible page sizes in IA-32 paging?,4 KB or 4 MB.
What paging scheme does IA-32 use for 4-KB pages?,A two-level paging scheme.
How is a 32-bit linear address divided for 4-KB pages in IA-32 paging?,"Page number p1 (10 high-order bits), Page number p2 (10 inner bits), and Page offset d (12 low-order bits)."
What is the page directory in IA-32 paging?,"The outermost page table, referenced by the 10 high-order bits of the linear address."
What is the function of the CR3 register in IA-32 paging?,It points to the page directory for the current process.
How does the Page_Size flag in a page directory entry affect IA-32 paging?,"If set, the page frame is 4 MB, bypassing the inner page table."
How many low-order bits are used as the offset in a 4-MB page frame in IA-32 paging?,22 low-order bits.
How does IA-32 handle page tables that are not in memory?,"Page tables can be swapped to disk for efficiency; an invalid bit in the page directory entry indicates if a table is on disk, and the OS brings it into memory on demand."
What is the purpose of Page Address Extension (PAE) in IA-32?,"To allow 32-bit processors to access physical address space larger than 4 GB, addressing the 4-GB memory limitation of 32-bit architectures."
How does PAE change the paging scheme in IA-32?,It changes it from a two-level to a three-level scheme.
What are the top two bits of a linear address used for in IA-32 PAE?,They refer to the page directory pointer table.
What additional page size does PAE support?,2-MB pages.
How did PAE increase the address space capability?,"It increased page-directory and page-table entries from 32 to 64 bits, allowing the base address of page tables/frames to extend from 20 to 24 bits."
What is the total address space increased to with PAE and what is the maximum physical memory supported?,"36 bits (allowing up to 64 GB physical memory), by combining the 24-bit base address with a 12-bit offset."
What OS support is required for PAE?,"OS support is required (e.g., Linux, Mac support it; 32-bit Windows desktop is limited to 4 GB)."
What was Intel's initial 64-bit architecture and why was it not widely adopted?,"IA-64, later known as Itanium. It was not widely adopted."
Who developed the x86-64 architecture and what were its key features?,"AMD developed it, extending the existing IA-32 instruction set to support larger logical/physical address spaces and architectural advances."
"Why is ""x86-64"" the general term for current 64-bit Intel/AMD CPUs?","Because Intel adopted AMD's x86-64 architecture, and it describes a class of 64-bit CPUs running identical instruction sets."
What is the theoretical maximum address space for a 64-bit architecture?,2^64 bytes (16 quintillion / 16 exabytes).
What is the actual virtual address size used in the x86-64 architecture?,48-bit virtual address.
What page sizes does x86-64 support?,"4 KB, 2 MB, or 1 GB."
How many levels of paging hierarchy does x86-64 use?,Four levels.
What existing addressing scheme does x86-64 use for paging?,PAE (Page Address Extension).
What are the maximum virtual and physical address capabilities of x86-64?,"Supports 48-bit virtual addresses and 52-bit physical addresses (4,096 terabytes)."
"Define ""page directory"" (Intel IA-32).","In Intel IA-32 CPU architecture, the outermost page table."
"Define ""page address extension (PAE)"".",Intel IA-32 CPU hardware allowing 32-bit processors to access physical address space larger than 4GB.
"Define ""page directory pointer table"".",PAE pointer to page tables.
What is Itanium?,Intel IA-64 CPU.
What is AMD 64?,A 64-bit CPU designed by Advanced Micro Devices; part of the x86-64 class.
What is Intel 64?,"Intel 64-bit CPUs, part of the x86-64 class."
"Define ""x86-64"".",A class of 64-bit CPUs running identical instruction sets; common in desktop/server systems.
What type of devices commonly use ARM processors?,"Mobile devices (smartphones, tablets) and real-time embedded systems."
What is Intel's primary role in chip production?,Intel designs and manufactures chips.
What is ARM's primary role in chip production?,ARM only designs and licenses architectural designs to manufacturers.
Name some well-known devices that utilize ARM processors.,"Apple devices (like iPhone, iPad) and most Android devices."
"By quantity, what is the most widely used processor architecture?","ARM architecture, with over 100 billion processors produced."
What specific ARM architecture is the primary focus of study?,The 64-bit ARM v8 architecture.
What are the three translation granule sizes available in ARM v8?,"4 KB, 16 KB, and 64 KB."
What do translation granules in ARM v8 define or provide?,Different page sizes and larger contiguous memory sections called regions.
"For an ARM v8 4 KB translation granule, what are the associated page and region sizes?","Page size: 4 KB; Region sizes: 2 MB, 1 GB."
"For an ARM v8 16 KB translation granule, what are the associated page and region sizes?",Page size: 16 KB; Region size: 32 MB.
"For an ARM v8 64 KB translation granule, what are the associated page and region sizes?",Page size: 64 KB; Region size: 512 MB.
How many levels of paging do 4-KB and 16-KB translation granules support in ARM v8?,Up to four levels.
How many levels of paging do 64-KB translation granules support in ARM v8?,Up to three levels.
"What is the bit architecture of ARM v8, and how many bits are currently utilized?","ARM v8 is a 64-bit architecture, but only 48 bits are currently used."
"In a 4-KB granule paging structure where all four levels are used, which bits define the offset within a page?",Bits 0-11 (low-order 12 bits) refer to the offset within the 4-KB page.
"If a Level-1 table entry in ARM v8 refers to a 1-GB region, which bits are used as the offset?",Low-order 30 bits (0-29).
"If a Level-2 table entry in ARM v8 refers to a 2-MB region, which bits are used as the offset?",Low-order 21 bits (0-20).
How many levels of TLBs does the ARM architecture support?,Two levels.
What are the components of the inner level of ARM TLBs?,"Two micro TLBs (one for data, one for instructions); they support ASIDs."
What is the component of the outer level of ARM TLBs?,A single main TLB.
Where does the address translation process begin in ARM architecture?,At the micro-TLB level.
What happens during address translation if there is a micro-TLB miss?,The main TLB is checked.
What happens during address translation if both the micro-TLB and main TLB miss?,A page table walk is performed in hardware.
Define translation granules.,Features of ARM v8 CPUs defining page sizes and regions.
Define regions (in ARM v8 CPUs).,Contiguous memory areas with separate privilege and access rules.
Define translation table base register (TTBR).,ARM v8 CPU register pointing to the level 0 (outer) page table for the current thread.
Define micro TLB.,"ARM CPU inner-level TLBs, one for instructions and one for data."
Define main TLB.,ARM CPU outer-level TLB; checked after micro TLB lookup and before page table walk.
What is memory in modern computer systems?,"A large array of bytes, each with its own address."
How is address space allocation managed?,Using base and limit registers.
What is a Base register?,The smallest legal physical memory address.
What does 'Limit' specify in address space allocation?,The size of the address range.
What are the different times for binding symbolic address references to physical addresses?,"Compile time, Load time, Execution time."
What is a Logical address?,An address generated by the CPU.
What is an MMU (Memory Management Unit)?,A hardware component that translates a logical address to a physical address.
What is a Physical address?,"The actual address in physical memory, resulting from an MMU translation of a logical address."
Describe a common memory allocation approach.,Contiguous memory partitions of varying sizes.
What are the three common partition allocation strategies?,"First fit, Best fit, Worst fit."
Which memory management technique do modern OS typically use?,Paging.
How is Physical memory divided in paging?,Into fixed-sized blocks called frames.
What are Frames in the context of paging?,Fixed-sized blocks into which physical memory is divided.
How is Logical memory divided in paging?,Into blocks of the same size called pages.
What are Pages in the context of paging?,Blocks of the same size into which logical memory is divided.
How is a logical address divided in paging?,Into a page number and a page offset.
What is a Page number?,An index into a per-process page table.
What is a Page table?,A data structure that contains the frame in physical memory holding a specific page.
What is the Offset in paging?,The specific location within a frame.
What is a TLB (Translation Look-aside Buffer)?,A hardware cache of the page table.
What information does each TLB entry contain?,A page number and its corresponding frame.
Describe the steps of TLB in address translation.,"1. Get the page number from the logical address. 2. Check if the frame for the page is in the TLB. 3. If in TLB, the frame is obtained from TLB. 4. If not in TLB (TLB miss), retrieve the frame from the page table."
What is Hierarchical paging?,A method where the logical address is divided into multiple parts for different page table levels.
What problem arises with expanding addresses beyond 32 bits when using hierarchical paging?,A large number of hierarchical levels would be required.
What strategies are used to address the problem of a large number of hierarchical levels in paging?,Hashed page tables and Inverted page tables.
What is Swapping?,The process of moving pages to disk to increase the degree of multiprogramming.
"How many levels of page tables does the Intel 32-bit architecture typically use, and what page sizes does it support?","Two levels of page tables, supporting 4-KB or 4-MB page sizes."
What is Page-address extension (PAE)?,A feature that allows 32-bit processors to access a physical address space greater than 4 GB.
Which modern architectures use hierarchical paging for their 64-bit systems?,x86-64 and ARM v8 architectures.
