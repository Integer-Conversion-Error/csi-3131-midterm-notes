What is the primary characteristic of virtual memory as a technique?,It requires entire processes to be loaded into physical memory for execution.,It allows processes to execute even if they are not entirely in physical memory.,It limits program size to the available physical memory.,It combines physical and logical memory into a single contiguous block.,It is solely responsible for managing CPU scheduling.,B,"The text states, ""Virtual memory: technique allowing execution of processes not entirely in memory."""
Virtual memory is primarily characterized by its separation of which two memory views?,Cache memory from main memory.,ROM from RAM.,Logical memory (programmer's view) from physical memory.,Primary storage from secondary storage.,System memory from user memory.,C,"The text explicitly states virtual memory ""Separates logical memory (programmer's view) from physical memory."""
What is considered a major advantage of using virtual memory?,It reduces the overall CPU clock speed.,It ensures all programs run faster regardless of I/O.,It allows programs to be larger than the physical memory available.,It eliminates the need for any form of memory management unit.,It simplifies hardware design by removing memory hierarchies.,C,"The text lists ""Major advantage: programs larger than physical memory."""
How does virtual memory primarily benefit programmers regarding memory limitations?,It forces programmers to manually manage physical memory addresses.,It restricts programmers to writing smaller programs.,It frees programmers from physical memory-storage limitations.,It increases the complexity of memory allocation for programmers.,It requires programs to be written in assembly language.,C,"The text states, ""Frees programmers from memory-storage limitations"" and ""Simplifies programming: no worry about physical memory limits."""
How does virtual memory contribute to increased CPU utilization and throughput?,By requiring more physical memory per program.,By decreasing the number of programs run concurrently.,"By allowing less physical memory per program, thus running more programs concurrently.",By increasing response and turnaround time.,By eliminating the need for context switching.,C,"The text explains, ""Less physical memory per program \u2192 more programs run concurrently \u2192 increased CPU utilization and throughput (no increase in response/turnaround time)."""
"One benefit of partial program execution in memory, facilitated by virtual memory, is its effect on I/O and program speed. What is this effect?","More I/O for loading/swapping, leading to slower program execution.","No change in I/O operations, but faster execution due to dedicated memory.","Less I/O for loading/swapping, leading to faster program execution.",Increased I/O but only for specific error handling routines.,It eliminates all I/O operations during program runtime.,C,"The text says, ""Less I/O for loading/swapping \u2192 faster program execution."""
What was the primary limitation of traditional memory management techniques before virtual memory?,Inability to run multiple programs simultaneously.,Programs were limited in size by the available physical memory.,It required complex hardware for memory allocation.,It could only manage read-only memory.,It prevented any form of shared memory.,B,"The text states under ""Background"" that ""Limitation: program size limited by physical memory."""
Which of the following is cited as a reason why real programs often do not need their entire code in physical memory?,Core program logic is always kept in cache memory.,All program options are used in every execution.,Error handling code is seldom executed.,Arrays and lists are always allocated the exact memory they need.,Programmers intentionally write redundant code.,C,"The text lists ""Error handling code: seldom executed"" as one reason why entire program code is not always needed."
"According to the provided text, what is the definition of a ""virtual address space""?",The physical memory available on the system.,The cache memory used by the CPU.,A logical view of how a process is stored in memory.,The total amount of disk space allocated for paging.,The address space used by the operating system kernel only.,C,"The text and glossary define ""virtual address space"" as ""Logical view of how a process is stored in memory."""
What is the primary function of the Memory-Management Unit (MMU) in the context of virtual memory?,To manage the growth of the heap and stack.,To map logical pages to physical page frames.,To directly store program instructions.,To determine the size of the virtual address space.,To allocate physical memory contiguously for all processes.,B,"The text states, ""Memory-management unit (MMU): maps logical pages to physical page frames."""
"What best describes a ""sparse"" address space?",An address space where all memory is contiguous and fully utilized.,An address space that is smaller than the physical memory.,An address space with many holes or unused regions.,An address space reserved exclusively for system libraries.,An address space that only supports read-only access.,C,"The glossary defines ""sparse"" as ""an address space with many holes,"" and the text notes ""Sparse address spaces: virtual address spaces with holes."""
One benefit of sparse address spaces is the facilitation of what process?,Static linking of all libraries at compile time.,Reducing the total size of physical memory required for the system.,Dynamic linking of libraries/shared objects during execution.,Ensuring all parts of a program are loaded into memory simultaneously.,Preventing any form of memory sharing between processes.,C,"The text lists ""Dynamic linking of libraries/shared objects during execution"" as a benefit of sparse address spaces."
How does virtual memory enable processes to share system libraries like the standard C library?,By duplicating the library code for each process.,"By loading libraries into a dedicated, non-sharable memory region.","By mapping the libraries into the virtual address space of processes, often read-only, sharing physical pages.",By forcing processes to include the library code directly in their executable.,By making libraries accessible only to the operating system kernel.,C,"The text states, ""System libraries (e.g., standard C library) shared by mapping into virtual address space. Libraries mapped read-only, physical pages shared by processes."""
"How does virtual memory contribute to speeding up process creation, particularly with operations like `fork()`?",By requiring complete duplication of all memory pages.,By preventing memory sharing during creation.,By enabling pages to be shared between parent and child processes.,"By using a slower, more deliberate copying mechanism.",By eliminating the need for a memory management unit.,C,"The text states, ""Pages shared during process creation (fork()) \u2192 speeds up process creation."""
Which of the following is an accurate statement about the impact of virtual memory on system performance?,It always decreases CPU utilization because of increased overhead.,It can increase CPU utilization and throughput by allowing more programs to run concurrently.,It leads to an increase in response and turnaround time for programs.,It eliminates the need for context switching between processes.,It only benefits single-process systems.,B,"The text explicitly states, ""Less physical memory per program \u2192 more programs run concurrently \u2192 increased CPU utilization and throughput (no increase in response/turnaround time)."""
Virtual memory abstracts main memory into what kind of storage structure?,"A small, fixed-size cache.","A fragmented, discontinuous block.","A large, uniform storage array.","A temporary, volatile disk space.","A hardware-specific, non-abstracted region.",C,"The text says virtual memory ""Abstracts main memory into large, uniform storage array."""
What is a potential negative consequence if virtual memory is used carelessly?,It can lead to an increase in physical memory requirements.,It can simplify the overall system architecture.,It can decrease performance.,It can make process creation less efficient.,It will always lead to programs being smaller than physical memory.,C,"The text warns, ""Implementation complex, can decrease performance if used carelessly."""
Which virtual memory management technique does Linux primarily use?,Segmentation,Swapping,Demand paging,Static partitioning,Overlaying,C,"Linux manages virtual memory using demand paging, which brings pages into memory only when they are needed."
What kind of page-replacement policy does Linux implement?,Local LRU,FIFO,Optimal,Global LRU-approximation clock algorithm (second-chance),Most Recently Used (MRU),D,"Linux uses a global page-replacement policy similar to the LRU-approximation clock algorithm, also known as second-chance."
Linux maintains two page lists for memory management. What are they called?,Allocated and Deallocated,In-use and Free,Active and Inactive,Primary and Secondary,Resident and Swapped,C,Linux maintains an `active_list` for pages considered in use and an `inactive_list` for pages not recently referenced and eligible for reclamation.
"In Linux, what happens when a page in the `inactive_list` is referenced?",Its accessed bit is reset.,It is immediately reclaimed for the free list.,It moves back to the rear of the `active_list`.,It is written to secondary storage.,It remains in the `inactive_list` until reclaimed by kswapd.,C,"If a page in the `inactive_list` is referenced, it moves back to the rear of the `active_list`, indicating it is now in use."
Which daemon process in the Linux kernel is responsible for managing free memory and reclaiming pages?,pageservd,memmgrd,kswapd,pagedaemon,freememd,C,"The Linux kernel's page-out daemon process, `kswapd`, periodically awakens to check free memory and reclaims pages from the `inactive_list` if free memory falls below a threshold."
When does the Linux `kswapd` process typically scan the `inactive_list` to reclaim pages?,When the `active_list` becomes empty.,When a new process is created.,If free memory falls below a predetermined threshold.,"Every 5 seconds, regardless of memory status.",Only when the system is booting up.,C,`kswapd` scans the `inactive_list` and reclaims pages for the free list if free memory falls below a specific threshold.
What is the primary function of the `accessed` bit for pages in Linux's virtual memory management?,To indicate if a page is dirty and needs to be written to disk.,To track the total number of times a page has been referenced.,To determine if a page has been recently referenced for page replacement.,To signify that a page is currently being used by the kernel.,To mark a page as read-only.,C,The `accessed` bit is set when a page is referenced and is used by the LRU-approximation algorithm to determine which pages are least recently used and thus eligible for reclamation.
What is 'clustering' in the context of Windows' virtual memory management?,A technique to group processes for better CPU scheduling.,Bringing in the faulting page along with several immediately preceding/following pages during a page fault.,A method for encrypting memory pages.,Storing multiple small files on a single disk block.,A mechanism for remote memory access over a network.,B,Clustering in Windows handles page faults by bringing in the faulting page plus several immediately preceding and following pages to potentially reduce future page faults.
"According to the text, what is the default virtual address space for a 32-bit Windows system?",4 GB,8 TB,2 GB,128 TB,24 TB,C,"For 32-bit systems, Windows 10 has a default 2 GB virtual address space, extendable to 3 GB."
What is the typical cluster size for a data page fault in Windows?,1 page,2 pages,3 pages,5 pages,7 pages,C,"For data page faults, Windows clustering brings in 3 pages: the faulting page, one immediately before, and one immediately after."
"In Windows, what does the 'working-set minimum' define?",The maximum number of pages a process can ever use.,The minimum number of pages guaranteed to a process in memory.,The initial number of pages allocated to a process at creation.,The smallest possible size of a virtual page.,The threshold for initiating automatic working-set trimming.,B,The 'working-set minimum' is the minimum number of frames (pages) guaranteed to a process in memory.
What is the default 'working-set maximum' assigned to a process upon creation in Windows?,50 pages,100 pages,256 pages,345 pages,512 pages,D,"Upon process creation, Windows assigns a default 'working-set maximum' of 345 pages."
What is the purpose of 'automatic working-set trimming' in Windows?,To increase the working-set size of active processes.,To decrease working-set frames for processes if a minimum free memory threshold is reached.,To periodically write all process pages to disk.,To ensure all processes reach their working-set maximum.,To optimize the initial allocation of memory to new processes.,B,Automatic working-set trimming is a global replacement tactic in Windows that decreases working-set frames for processes if the free memory falls below a threshold.
When does Windows perform a local LRU page replacement policy?,Only when the system boots up.,When a process is at its working-set maximum and there is insufficient free memory.,When a process is below its working-set minimum.,When `automatic working-set trimming` is active.,When the `hard working-set limits` are ignored.,B,"If a process is at its working-set maximum and there is insufficient free memory, the kernel selects a page from that process's working set for replacement using a local LRU policy."
Which of the following virtual memory features is NOT explicitly mentioned as implemented by Windows?,Demand paging,Copy-on-write,Memory compression,"First-In, First-Out (FIFO) page replacement",Shared libraries,D,"Windows implements demand paging, copy-on-write, paging, memory compression, and shared libraries. Its page replacement is an LRU-approximation clock algorithm (second-chance), not FIFO."
"In Solaris, what is the role of the `lotsfree` parameter?",It defines the maximum amount of physical memory available.,It is the threshold for Solaris to begin paging activities.,It specifies the number of pages to scan per second.,It determines the desired free memory level.,It represents the number of pages reclaimed by `kswapd`.,B,"`lotsfree` is a parameter in Solaris that serves as a threshold; if free pages fall below this value, the pageout process starts."
How often does the Solaris kernel check free memory against the `lotsfree` parameter?,Once per second,Twice per second,Four times per second,Every 30 seconds,Only when a page fault occurs,C,The Solaris kernel checks free memory against `lotsfree` four times per second.
Describe the 'two hands' mechanism used by the Solaris pageout process.,"One hand for reading from disk, one for writing to disk.","One hand for user pages, one for kernel pages.",A front hand to reset reference bits and a back hand to check them and reclaim pages.,"One hand for active pages, one for inactive pages.",Two distinct processes that simultaneously scan memory.,C,"The Solaris pageout process uses two hands: a front hand scans pages setting reference bits to 0, and a back hand checks those bits, reclaiming pages if they are still 0."
"In Solaris, what is `scanrate`?",The frequency at which the pageout process awakens.,The speed at which data is transferred to secondary storage.,The number of pages scanned per second by the pageout process.,The rate at which processes are swapped out of memory.,The time taken to clear and check a page's reference bit.,C,"`scanrate` in Solaris refers to the number of pages per second scanned by the pageout process, ranging from `slowscan` to `fastscan`."
What action does the Solaris kernel take if it cannot maintain `desfree` memory for a 30-second average?,It increases the `scanrate` to its maximum.,It calls the pageout process for every new page request.,"It starts to swap out idle processes, freeing all their pages.",It stops all new process creation.,It reconfigures the `lotsfree` parameter.,C,"If Solaris is unable to maintain `desfree` memory for a 30-second average, the kernel swaps processes, specifically looking for idle ones, to free their pages."
What is 'priority paging' in Solaris?,Giving higher priority to kernel pages over user pages.,"Prioritizing selection of victim frames based on criteria, such as avoiding shared library pages.",A method to quickly page out low-priority processes.,Ensuring critical system processes always reside in physical memory.,Assigning different page replacement algorithms based on process priority.,B,"Priority paging is when Solaris prioritizes the selection of victim frames based on certain criteria, such as explicitly skipping shared library pages even if they are eligible for reclamation."
"In Solaris, what happens if free memory falls below `minfree`?",The system halts to prevent data corruption.,The pageout process is called for every new page request.,The `scanrate` is automatically set to `slowscan`.,Only shared library pages are reclaimed.,The `lotsfree` parameter is adjusted dynamically.,B,"If the system is unable to maintain `minfree`, the Solaris pageout process is called for every new page request, indicating an extreme memory shortage."
Which of the following is NOT a feature or mechanism mentioned as part of Solaris's memory management?,Two-hand pageout process,`lotsfree` parameter,`kswapd` daemon,Priority paging,`desfree` parameter,C,"`kswapd` is the page-out daemon process specific to Linux, not Solaris. Solaris uses its own 'pageout process'."
What is the definition of 'clustering' as provided in the text?,A method for grouping CPUs to handle memory tasks.,Paging in a group of contiguous pages when a single page is requested via a page fault.,The act of collecting fragmented memory pages.,A system's ability to run multiple virtual machines.,Organizing processes into distinct memory regions.,B,Clustering is defined as 'Paging in a group of contiguous pages when a single page is requested via a page fault'.
"According to the provided definitions, what does 'working-set maximum' refer to?",The minimum number of frames guaranteed to a process in Windows.,The total physical memory available on the system.,The maximum number of frames allowed to a process in Windows.,The number of frames that are actively being used by the kernel.,The upper limit of virtual address space.,C,The 'working-set maximum' is defined as the 'Maximum number of frames allowed to a process in Windows'.
What is the meaning of 'hard working-set limit' in Windows?,A soft suggestion for memory usage that can always be exceeded.,The minimum guaranteed memory for a process.,The maximum amount of physical memory a process is allowed to use.,The total size of the swap file.,"A limit that can only be set by the operating system, not by the user.",C,The 'hard working-set limit' is defined as the 'Maximum amount of physical memory a process is allowed to use in Windows'.
What is the definition of 'priority paging' from the provided text?,Assigning a priority level to each page in memory.,"Prioritizing selection of victim frames based on criteria, e.g., avoiding shared library pages.",A mechanism to give preference to paging in critical system pages.,The process of quickly freeing memory for high-priority applications.,The ability to dynamically adjust page sizes based on priority.,B,"Priority paging is defined as 'Prioritizing selection of victim frames based on criteria, e.g., avoiding shared library pages'."
"In Linux, if the `active_list` grows larger than the `inactive_list`, what action is taken regarding pages from the `active_list`?",They are immediately written to swap space.,They are moved from the front of the `active_list` to the rear of the `inactive_list`.,Their accessed bits are permanently set to 1.,They are marked as read-only.,They are deallocated and added to the free frames list.,B,"If the `active_list` grows larger than the `inactive_list`, pages from the front (least recently used) of the `active_list` move to the `inactive_list`, making them eligible for reclamation."
Which of the following best describes virtual memory?,A contiguous block of physical RAM reserved for the operating system.,A technique to increase CPU clock speed by reducing memory access times.,An abstraction of physical memory into an extremely large uniform array of storage.,A specialized cache for frequently accessed data that is faster than RAM.,A method to manage network bandwidth and optimize data transfer.,C,"Virtual memory abstracts physical memory into an extremely large uniform array of storage, making it appear larger and uniform to processes."
One significant benefit of virtual memory is that it allows a program to:,Execute faster than if it were entirely in physical memory due to caching.,Be larger than the available physical memory.,Directly access hardware registers without operating system intervention.,Avoid the need for any form of secondary storage like hard drives.,Communicate directly with other processes using shared CPU registers.,B,Virtual memory enables programs to be larger than physical memory by loading only parts of the program into RAM as needed.
Which of the following is NOT listed as a direct benefit of virtual memory?,Programs do not need to be entirely in memory to execute.,Processes can share memory regions more efficiently.,Processes can be created more efficiently.,It eliminates the need for any form of memory swapping or paging to disk.,It allows a program to be larger than the physical memory available.,D,"The text states that virtual memory enables programs to be larger than physical memory, not entirely in memory, allows memory sharing, and makes process creation more efficient. It does not eliminate swapping; in fact, it relies on it."
What is the primary characteristic of 'demand paging'?,All pages of a program are loaded into memory at program start.,Pages are preloaded into memory based on anticipated future access patterns.,Pages are loaded into memory only when they are referenced or 'demanded' during program execution.,Pages are compressed before being loaded into memory to save space.,Pages are always written back to the backing store immediately after modification.,C,Demand paging ensures that pages are loaded into physical memory only when they are actively needed ('demanded') during program execution.
"Based on the concept of demand paging, what happens to pages that are never 'demanded' during program execution?",They are loaded into a special cache for later use.,They are immediately written to the backing store to free up space.,They are never loaded into physical memory.,They are marked for prefetching in a subsequent execution.,They are compressed and stored in a kernel-only region of memory.,C,A direct consequence of demand paging is that pages that are never referenced are never loaded into physical memory.
A 'page fault' occurs when:,A page is successfully written from physical memory to the backing store.,An attempt is made to access a page that is not currently present in physical memory.,Two processes try to write to the same memory page simultaneously.,The operating system detects an unrecoverable error in a page's data.,A page is successfully moved from the CPU cache to main memory.,B,A page fault is triggered when a process attempts to access a virtual memory page that is not currently loaded into physical memory.
What action typically follows a page fault?,The operating system terminates the process immediately.,The process is paused indefinitely until more physical memory becomes available.,The requested page must be brought from the backing store into an available page frame in physical memory.,The system initiates a full memory scan to diagnose the cause of the fault.,The page is marked as 'read-only' to prevent further issues.,C,"Upon a page fault, the necessary page must be fetched from secondary storage (backing store) and loaded into an empty page frame in RAM."
What is the primary characteristic of 'copy-on-write' in the context of process creation?,All pages are copied from the parent to the child process at the moment of creation.,"The child process receives a read-only copy of the parent's memory, which is never modified.","The child process initially shares the same address space as the parent, with copies made only upon modification.","Both parent and child processes immediately get independent, fully duplicated memory spaces.",Memory is copied only when the parent process terminates or exits.,C,Copy-on-write means that a child process initially shares the parent's memory pages. A copy of a page is only created if either the parent or child attempts to modify it.
"Under a copy-on-write mechanism, when is a copy of a shared page made?",Only when the child process is initially created.,When either the child or parent process attempts to modify the shared page.,Only when the parent process modifies the page.,Only when the child process modifies the page.,When the system memory becomes critically low.,B,"A copy of a page is made if either the child process or the parent process modifies that page, ensuring isolation without immediate full duplication."
A page-replacement algorithm is typically invoked when:,A new process is created and needs its initial memory allocation.,Available physical memory is low and a new page needs to be loaded from the backing store.,A process voluntarily releases its allocated memory regions.,The CPU cache becomes full and needs to be flushed.,The system is booting up and initializing its memory structures.,B,"When available memory is low, a page-replacement algorithm is used to select an existing page to be evicted from memory to make room for a new one."
Which of the following is explicitly listed as a page-replacement algorithm in the provided text?,LFU (Least Frequently Used),MRU (Most Recently Used),Optimal,Random,Segmented FIFO,C,"The text lists FIFO, optimal, and LRU as page-replacement algorithms."
Why is a 'pure LRU' page-replacement algorithm often impractical to implement in real systems?,It requires too much CPU time for calculations on every memory access.,It necessitates complex hardware support to accurately track the exact usage time for every page.,It inherently leads to an excessive number of page faults compared to other algorithms.,It is difficult to define 'least recently used' precisely across all processes.,It cannot be combined with other memory management techniques like demand paging.,B,"The text states that pure LRU is impractical to implement, implying the difficulty in accurately tracking precise usage times, leading most systems to use LRU-approximation algorithms."
A 'global page-replacement algorithm' selects a page for replacement from:,Only the pages belonging to the faulting process.,"A fixed set of system-wide shared pages, ignoring process-specific ones.",Any process currently loaded in the system's memory.,Only pages that have not been modified since being loaded.,Pages belonging to processes with the lowest priority or least activity.,C,"Global page-replacement algorithms are defined as selecting a page from any process for replacement, not just the faulting one."
"In contrast to a global algorithm, a 'local page-replacement algorithm' selects a page for replacement from:",Only pages that are currently in the CPU cache.,Only pages belonging to the faulting process.,"Any page in memory, regardless of its owning process.",Pages that are least frequently accessed system-wide.,"Pages within a specific, pre-defined memory region reserved for the OS.",B,Local page-replacement algorithms are defined as selecting a page from the faulting process itself.
'Thrashing' refers to a state where a system:,"Spends an excessive amount of time executing user-mode processes, neglecting system tasks.","Is unable to allocate any more physical memory, leading to system crash.",Spends more time paging (swapping pages in and out of memory) than executing useful work.,"Experiences frequent CPU context switches, but without significant performance impact.","Has its entire working set loaded into memory, leading to optimal performance.",C,"Thrashing is a severe performance degradation where the system is spending a disproportionate amount of time moving pages between RAM and disk, rather than performing actual computation."
"In the context of virtual memory, what does 'locality' refer to?",The physical location of memory modules on the motherboard.,A set of pages that are actively used together by a process.,The geographical location of data centers for distributed memory systems.,The closest available page frame for allocation when a page fault occurs.,A measure of how frequently a specific memory address is accessed.,B,"Locality is defined as a set of pages actively used together, often referring to spatial or temporal locality of reference."
How does process execution typically relate to 'locality'?,Processes remain strictly within a single locality throughout their entire execution.,Processes continuously move randomly across all available memory pages without structure.,Process execution typically moves from one locality to another as different functions or data are accessed.,"Locality only applies to kernel processes, not user-mode processes.",Locality prevents processes from ever needing to page to disk.,C,"The text states that process execution moves from locality to locality, reflecting changing working sets over time."
The 'working set' of a process is defined as:,The total number of pages allocated to a process since its creation.,"The set of pages currently in active use by a process, based on the concept of locality.","The set of all executable code pages belonging to a process, excluding data pages.",The maximum number of pages a process is allowed to hold in memory at any given time.,The set of pages that have been recently written to disk as part of a checkpointing process.,B,The working set is based on locality and represents the set of pages currently in use by a process.
What is 'memory compression' in the context of virtual memory?,A technique that reduces the physical size of RAM modules for compact devices.,A method of compressing a number of individual pages into a single page frame in memory.,An algorithm designed to increase the speed of memory access by reducing latency.,A way to store data redundantly across multiple pages for fault tolerance.,A security measure to encrypt memory contents to prevent unauthorized access.,B,Memory compression involves compressing multiple logical pages of data into a single physical page frame.
"Memory compression is noted as an alternative to paging, primarily used on which type of systems?",High-performance computing clusters that require extreme speed.,Enterprise server systems managing large databases.,Mobile systems that often lack full paging support.,Desktop workstations with large amounts of RAM.,Virtual machines running in cloud environments.,C,"Memory compression is used as an alternative to paging, particularly on mobile systems without full paging support."
How is kernel memory allocation described as being different from user-mode process allocation?,"Kernel memory is allocated in non-contiguous chunks, unlike user memory.","Kernel memory is allocated dynamically based on demand, while user memory is always pre-allocated.",Kernel memory is allocated in contiguous chunks of varying sizes.,"Kernel memory uses a FIFO allocation scheme, while user memory uses LRU.",Kernel memory is always much larger in total size than user memory.,C,"Kernel memory is allocated differently than user-mode processes, specifically in contiguous chunks of varying sizes."
Which of the following are mentioned as common techniques for kernel memory allocation?,LIFO and LRU.,Paging and Swapping.,Buddy system and Slab allocation.,First-fit and Best-fit.,Segmented memory and Pure demand paging.,C,The text explicitly lists the Buddy system and Slab allocation as common techniques for kernel memory allocation.
What does 'TLB reach' refer to in the context of memory management?,The physical distance between the CPU and main memory.,The speed at which the Translation Lookaside Buffer (TLB) can translate virtual addresses.,The total amount of memory that can be directly accessed and mapped by the TLB at any given time.,The maximum number of entries the TLB can hold.,The number of active processes that can simultaneously utilize the TLB.,C,TLB reach is defined as the amount of memory accessible from the Translation Lookaside Buffer (TLB).
How is TLB reach calculated?,Number of TLB entries divided by the system's smallest page size.,Total physical memory size multiplied by the system's largest page size.,Number of entries in the TLB multiplied by the page size.,CPU clock speed multiplied by the TLB access time.,Number of active processes multiplied by the number of TLB entries.,C,TLB reach is calculated as the number of entries in the TLB multiplied by the page size.
A common technique to increase TLB reach is to:,Decrease the number of entries in the TLB.,Use smaller page sizes for memory allocation.,Increase the page size used by the system.,Allocate significantly more physical memory (RAM).,Increase the frequency of TLB flushes.,C,The text states that a technique to increase TLB reach is to increase the page size.
"Which of the following virtual memory management techniques are commonly used by Linux, Windows, and Solaris?","Pure LRU, LIFO, and segmented memory allocation.","Demand paging, copy-on-write, and variations of LRU approximation (like the clock algorithm).","Buddy system, Slab allocation, and optimal page replacement.","Memory compression, pre-paging, and exact LRU implementations.","Strictly global page replacement, local page replacement, and pure FIFO.",B,"Linux, Windows, and Solaris are noted to manage virtual memory similarly, using demand paging, copy-on-write, and variations of LRU approximation (e.g., clock algorithm)."
What is the primary characteristic of demand paging?,Loading the entire program into physical memory before execution.,Loading program pages into memory only when they are needed during execution.,Storing all program pages permanently in physical memory.,Executing programs directly from secondary storage without loading them into RAM.,Prioritizing the loading of kernel pages over user program pages.,B,"Demand paging specifically loads pages into memory only when they are accessed or 'demanded' by the running program, rather than loading the entire program at once."
Which problem is primarily addressed by the use of demand paging?,Insufficient CPU processing speed for large programs.,"The need to load an entire program into physical memory, even if not all parts are immediately required.",Difficulty in managing multiple processes simultaneously.,The high cost of secondary storage devices.,Lack of proper security mechanisms for memory access.,B,Demand paging solves the problem of needing to load an entire program (which may include unneeded parts like unselected options) by only loading portions as they are accessed.
"In a system utilizing demand paging, what happens to pages that are never accessed during a program's execution?",They are loaded into a special cache for future use.,They remain in secondary storage and are never loaded into physical memory.,They are loaded into physical memory at program startup but later swapped out.,They cause a page fault upon program termination.,They are marked as invalid and immediately deleted from secondary storage.,B,"A key benefit of demand paging is that unaccessed pages are never loaded into physical memory, saving memory resources."
What hardware support is essential for distinguishing between pages that are in memory and those that are in secondary storage in a demand paging system?,A dedicated memory management unit (MMU) with a translation lookaside buffer (TLB).,A valid-invalid bit scheme in the page table.,High-speed solid-state drives for secondary storage.,Multiple CPU cores for parallel processing.,A large amount of RAM.,B,The valid-invalid bit in each page-table entry is used by hardware to indicate whether a page is legal and in memory (valid) or not (invalid).
"In the context of demand paging's valid-invalid bit scheme, what does a ""valid bit"" typically signify?",The page is corrupted and needs to be reloaded.,The page is legal and currently resides in physical memory.,The page is protected and cannot be modified.,The page is in secondary storage and needs to be swapped in.,The page is part of the operating system kernel.,B,A valid bit indicates that the corresponding page is both part of the process's logical address space and is currently loaded into a physical memory frame.
"What does an ""invalid bit"" in a page-table entry signify in a demand paging system?",The page contains invalid data and should be discarded.,"The page is legal and in memory, but read-only.","The page is either not a valid part of the logical address space, or it is valid but currently in secondary storage.",The page has been recently accessed and is likely to be accessed again.,The page is exclusively reserved for operating system use.,C,"An invalid bit indicates one of two possibilities: either the page address is illegal (not in the process's logical address space), or it is legal but currently resides in secondary storage."
"What immediately triggers a ""page fault"" in a demand paging system?",A process attempting to write to a read-only page.,The operating system deciding to swap out a less-used page.,An access attempt to a page marked as invalid in the page table.,The CPU requesting an instruction that is already in memory.,A system administrator manually initiating a page swap.,C,A page fault is explicitly defined as occurring when an access attempt is made to a page whose corresponding page-table entry is marked invalid.
What is the first action taken by the system immediately after a page fault occurs?,The process is immediately terminated.,The requested page is read from secondary storage.,A trap to the operating system is generated.,The page table is updated to mark the page as valid.,A new free frame is allocated.,C,"The text states, ""Access to invalid page → page fault. Page fault causes trap to OS."" This is the immediate consequence before any handling procedure begins."
"According to the provided text, what is the first step the operating system performs when handling a page fault?",Find a free frame in physical memory.,Schedule a secondary storage operation to read the page.,Check an internal table (like the process control block) to validate the memory access.,Modify the page table to mark the page as valid.,Restart the interrupted instruction.,C,"The first step in the page fault handling procedure is ""Check internal table (process control block) for valid/invalid memory access."""
"During the page fault handling procedure, if the OS determines that the memory access causing the fault is invalid (e.g., beyond the process's logical address space), what action is taken?",The page is loaded into memory regardless.,The process is terminated.,The page is moved to swap space.,The page table is updated with a valid bit.,"An error log is created, and the instruction is retried.",B,"Step 2 of the page fault handling procedure states: ""If invalid, terminate process."""
What is the final step in the page fault handling procedure as described in the text?,The process control block is updated.,The operating system finds a free frame.,The page is read from secondary storage.,The interrupted instruction is restarted.,The page is marked as invalid in the page table.,D,"The last step listed in the procedure is ""Restart interrupted instruction; process accesses page as if always in memory."""
"What defines ""pure demand paging""?",All pages are pre-loaded into memory before execution begins.,No pages are ever swapped out once loaded into memory.,"The process starts with no pages in memory, and pages are faulted in as needed.",Only read-only pages are loaded into memory on demand.,"Memory pages are only loaded from the file system, not swap space.",C,"Pure demand paging is defined as starting a process with no pages in memory, and then faulting for pages only as they are referenced."
"How does the ""locality of reference"" principle relate to the performance of demand paging?",It guarantees that all pages will be loaded into memory at startup.,"It increases the probability of page faults, thus degrading performance.","It suggests that processes access memory in patterns, which allows demand paging to perform reasonably well.","It indicates that memory access times are constant, regardless of paging.",It is irrelevant to the efficiency of demand paging.,C,"The text states, ""Programs tend to have locality of reference → reasonable demand paging performance,"" meaning the non-random access patterns make demand paging viable."
"What is the primary role of ""secondary memory"" (or swap device/swap space) in a demand paging system?",To store the entire operating system kernel.,To serve as a temporary cache for frequently accessed pages.,To hold pages that are not currently resident in main memory.,To execute programs directly without loading them into RAM.,To maintain a log of all page faults.,C,"Secondary memory, specifically swap space, is explicitly described as holding non-main-memory pages."
A crucial requirement for the successful implementation of demand paging is the ability to:,Pre-allocate all required memory frames at compile time.,Completely ignore the state of a process after a page fault.,Restart any instruction after a page fault has been handled.,Always keep the entire program in main memory.,Prevent any process from accessing pages in secondary storage.,C,"The text highlights this as a ""Crucial requirement: ability to restart any instruction after page fault,"" which involves saving and restoring process state."
"For instructions that modify multiple memory locations (like IBM System 360/370 MVC), how can a page fault be handled to ensure correct process state restoration?","The entire instruction is always aborted, and the process is restarted from scratch.",The instruction is immediately retried without any state restoration.,"The system uses microcode to check both ends of blocks before modification, or uses temporary registers to restore overwritten values.",Such instructions are prohibited in demand-paged systems.,The CPU automatically re-executes only the failed part of the instruction.,C,The text provides two solutions for this difficulty: microcode checks before modification or using temporary registers to restore old values on fault.
"From the perspective of the running process, how should paging be perceived?",It should be explicitly managed by the process's code.,It should be visible and require process intervention for page swaps.,"It should be transparent, meaning the process is unaware of it.",It should cause noticeable delays and require user interaction.,It should only occur during process startup.,C,"The text states, ""Paging should be transparent to process."""
"What is the primary purpose of the ""free-frame list"" maintained by the operating system?",To track all pages currently loaded into memory.,To store information about processes terminated due to page faults.,To provide a pool of available physical memory frames for allocation to processes.,To list all pages currently residing in secondary storage.,To manage the cache memory hierarchy.,C,"The free-frame list is described as a ""pool of free frames for page faults,"" and also for stack/heap segment expansion."
"What is the purpose of ""zero-fill-on-demand"" in the context of allocating free frames?",To fill unused frames with random data for testing purposes.,To prevent memory fragmentation.,To ensure security by clearing previous data from a frame before allocating it to a new process.,To optimize memory access by pre-fetching data.,To count the number of available frames.,C,"Zero-fill-on-demand is described as ""frames 'zeroed-out' before allocation (security)."""
"What does ""effective access time"" refer to in the context of demand-paged memory performance?",The time it takes for the CPU to access its internal registers.,"The average time required to access a memory location, considering both hits and page faults.",The maximum possible time for a memory access in the worst-case scenario.,The time it takes to transfer data between the CPU and cache.,The time it takes for a page to be written back to secondary storage.,B,"Effective access time is defined as the measured/calculated time to access something, and the formula explicitly combines memory access time with page fault time based on the probability of a fault."
Which of the following is NOT listed as a component of page fault service time?,Servicing the page-fault interrupt.,Reading the page into memory from secondary storage.,Restarting the interrupted process.,Compressing the page data before writing to swap space.,All listed are components.,D,"The three listed components are servicing the interrupt, reading the page, and restarting the process. Compressing data is not mentioned as a direct component of service time."
"Approximately how long does a typical HDD page-switch operation (including latency and seek time) take, as stated in the text?",10 nanoseconds,100 microseconds,8 milliseconds,3 seconds,200 nanoseconds,C,"The text explicitly states, ""HDD page-switch time: ~8 milliseconds (3ms latency, 5ms seek, 0.05ms transfer)."""
How does the page-fault rate (p) affect the effective access time in a demand paging system?,It is inversely proportional to the effective access time.,It has no significant impact on effective access time.,It is directly proportional to the effective access time.,It only affects the seek time of the hard disk.,It reduces the memory access time (ma).,C,"The formula and subsequent example clearly show that as 'p' increases, the effective access time also increases, making it directly proportional."
"To maintain acceptable performance in a demand paging system (e.g., less than 10% slowdown with an 'ma' of 200ns and page fault time of 8ms), what must be true about the page-fault rate (p)?",p must be exactly 1.,p must be greater than 0.1.,"p must be very low, specifically less than 0.0000025.",p should be close to 1/1000.,p should be around 8 milliseconds.,C,"The example calculation explicitly states that for a slowdown less than 10% (220ns effective access time), ""p < 0.0000025 (fewer than 1 fault per 399,990 accesses)."""
"Generally, how does I/O to swap space compare to I/O to the file system?",Swap space I/O is slower due to complex file system structures.,Swap space I/O is faster because it often involves larger blocks and no file lookups.,Both have identical performance characteristics.,File system I/O is always preferred for paging operations.,"Swap space is only used for caching, not for actual paging.",B,"The text states, ""Swap space I/O generally faster than file system I/O (larger blocks, no file lookups)."""
"In some operating systems like Linux and BSD UNIX, where are binary executables demand-paged from, and what acts as their backing store?","They are copied entirely to swap space at startup, which acts as the backing store.","They are demand-paged directly from the file system, which acts as their backing store.",They are kept entirely in RAM and never paged out.,They are always generated on-the-fly from source code.,They use anonymous memory backed by swap space.,B,"The text states: ""Demand-page binary executables directly from file system; overwrite frames when replaced (never modified); file system acts as backing store (Linux, BSD UNIX)."""
For what type of memory is swap space typically always used as a backing store when pages are modified and paged out?,Read-only code segments of binary executables.,Shared library pages.,"Anonymous memory, such as stack and heap.",Pages that have been compressed.,Files mapped into memory by multiple processes.,C,"The text states, ""Anonymous memory (stack, heap) still uses swap space."" The glossary also defines anonymous memory as ""Memory not associated with a file; stored in swap space if dirty and paged out."""
How do mobile operating systems like iOS typically handle memory management without traditional swapping?,They pre-load all necessary pages into memory.,They demand-page from the file system and reclaim read-only pages if memory is constrained.,They primarily use compressed memory as their only form of virtual memory.,They store all anonymous memory in non-volatile storage.,They rely solely on larger physical memory to avoid paging altogether.,B,"The text states: ""Mobile OS (e.g., iOS) typically no swapping: demand-page from file system, reclaim read-only pages if memory constrained."""
What alternative memory management technique is mentioned as being used in mobile systems in place of traditional swapping?,Memory pooling.,Compressed memory.,Static memory allocation.,Direct memory access (DMA).,Disk caching.,B,"The text explicitly mentions, ""Compressed memory is an alternative to swapping in mobile systems."""
"According to the glossary, what is ""swap space""?",The portion of RAM used for caching frequently accessed data.,Secondary storage backing-store space for paged-out memory.,A temporary directory for downloaded files.,The area in physical memory where the operating system kernel resides.,A list of available free memory frames.,B,"The glossary defines swap space as ""Secondary storage backing-store space for paged-out memory."""
"What does ""page-fault rate"" measure?",The speed at which pages can be loaded into memory.,The total number of page faults that have occurred.,How often a page fault occurs per memory access attempt.,The amount of memory occupied by invalid pages.,The average time taken to service a page fault.,C,"The glossary defines page-fault rate as ""Measure of how often a page fault occurs per memory access attempt."""
"In the context of memory management, what is ""anonymous memory""?",Memory that is shared between multiple processes.,"Memory not associated with a file, typically stored in swap space if paged out and dirty.",Memory segments containing only operating system code.,Memory used for storing file system metadata.,Memory that has been encrypted for security purposes.,B,"The glossary defines anonymous memory as ""Memory not associated with a file; stored in swap space if dirty and paged out."""
"At system startup, what is the initial state of the free-frame list?","It is empty, as all memory is initially reserved by the kernel.",It contains all available physical memory on the list.,It contains only frames allocated for the stack and heap.,It is populated only after the first page fault occurs.,It stores only frames that have been zero-filled.,B,"The text states: ""System startup: all available memory on free-frame list."""
"Beyond simply loading pages as needed, what general benefit does demand paging provide?",It eliminates the need for any secondary storage.,It ensures faster CPU clock speeds.,It enables more efficient memory use by loading only needed portions.,It prevents all forms of memory fragmentation.,It makes processes completely immune to performance slowdowns.,C,"One of the initial benefits listed is ""More efficient memory use by loading only needed portions."""
Demand paging is noted to be similar to which other memory management technique?,Segmentation.,Contiguous memory allocation.,Paging with swapping.,Static memory allocation.,Memory compression.,C,"The text mentions, ""Similar to paging with swapping."""
"In demand paging, what exactly constitutes a ""page fault""?",An error caused by trying to access a non-existent memory address.,A successful retrieval of data from the CPU cache.,A reference to a page that is currently not resident in physical memory.,An attempt to modify a read-only page.,The completion of a page write operation to secondary storage.,C,"The glossary defines page fault as ""Fault from reference to a non-memory-resident page."""
What is a primary objective of using copy-on-write (COW) during process creation with `fork()`?,"To ensure the child process always receives a complete, independent copy of the parent's address space.",To maximize the allocation of new pages to the child process immediately after creation.,To minimize the number of new pages allocated to the child process initially.,To prevent the child process from ever modifying any data inherited from the parent.,To bypass demand paging for all pages in the child process.,C,Copy-on-write (COW) is a technique that minimizes new pages allocated to the child process by initially sharing pages with the parent.
How does the copy-on-write (COW) mechanism fundamentally alter the traditional `fork()` behavior?,"Traditionally, `fork()` used a page-sharing mechanism similar to COW, whereas COW copies all pages.","Traditional `fork()` immediately copied the parent's entire address space, while COW initially shares pages.","COW ensures all parent pages are duplicated, while traditional `fork()` only duplicates modified pages.","Traditional `fork()` suspended the parent, which COW does not do.",COW is less efficient than traditional `fork()` for process creation.,B,"Traditionally, `fork()` copied the parent's entire address space for the child. With copy-on-write, the parent and child processes initially share the same pages."
"According to the copy-on-write (COW) technique, what action triggers the copying of a shared page?",When the child process terminates.,When the parent process resumes from suspension.,When either the parent or child process attempts to write to a shared page.,When the operating system runs out of free memory frames.,When an `exec()` call is made by the child process.,C,"If either the parent or child process writes to a shared page that is marked as copy-on-write, a copy of that shared page is created."
Which statement accurately describes how pages are handled under the copy-on-write (COW) mechanism?,"All pages, including executable code, are always copied to the child process.",Only stack pages are initially shared; all other pages are immediately copied.,"Unmodified pages, such as executable code, continue to be shared between parent and child.",Both modified and unmodified pages are always copied to ensure process isolation.,Pages are only copied if the child process modifies them and never if the parent modifies them.,C,"Only modified pages are copied under COW; unmodified pages (e.g., executable code) can continue to be shared between the parent and child processes."
"What is the primary benefit of using copy-on-write (COW) in the context of process creation, especially when the child process might immediately call `exec()`?",It ensures the child process has a completely independent address space from the start.,It increases the complexity of memory management for the operating system.,"It avoids unnecessary copying of memory pages, which might be discarded by `exec()`.",It allows the parent process to continue execution without suspension.,It prioritizes disk I/O over memory operations.,C,"Copying pages may be unnecessary if the child immediately calls `exec()`, as those copied pages would simply be overwritten. COW avoids this wasteful copying."
In which of the following operating systems is the copy-on-write (COW) technique commonly employed for process creation?,MS-DOS,"Windows, Linux, and macOS",Early versions of UNIX only,Only specialized real-time operating systems,Only systems that do not support virtual memory.,B,"Copy-on-write is a common technique used in modern operating systems such as Windows, Linux, and macOS."
What is a defining characteristic of the `vfork()` system call concerning the parent and child processes' address spaces?,The child process receives a completely independent copy of the parent's address space.,The child process uses its own unique address space while the parent's is suspended.,The child process shares the parent's address space directly for read and write operations.,"The child process initially shares pages, but a copy-on-write mechanism is used for modifications.","The parent and child processes share only read-only pages, with separate writable pages.",C,"With `vfork()`, the child process uses the parent's address space directly for read/write operations."
What is a fundamental difference in memory management between `vfork()` and `fork()` using copy-on-write?,"`vfork()` explicitly uses copy-on-write, whereas `fork()` does not.","`vfork()` does not use copy-on-write, and child modifications are visible to the parent, unlike `fork()` with COW.","`fork()` always copies all pages, while `vfork()` only copies modified pages.","`vfork()` allocates new pages for the child process, while `fork()` shares them.","`vfork()` allows concurrent execution of parent and child, while `fork()` suspends the parent.",B,"`vfork()` does not use copy-on-write, which means any changes made by the child to the shared address space are visible to the parent upon resumption. `fork()` with COW, conversely, ensures changes are made to a private copy."
For what specific scenario is the `vfork()` system call primarily intended?,When the child process is expected to perform extensive data processing within the inherited address space.,When the child process needs to create many new threads.,When the child process is intended to immediately call `exec()`.,When the parent process requires concurrent execution with the child without suspension.,When robust memory isolation between parent and child is paramount.,C,"`vfork()` is intended for use when the child process calls `exec()` immediately after its creation, as this avoids unnecessary copying."
What is a critical caution associated with using `vfork()`?,It is significantly slower than `fork()` for process creation.,"The child process might inadvertently modify the parent's address space, with visible effects.",It automatically applies copy-on-write to all shared pages.,It prevents the child process from calling `exec()`.,It is not supported on modern UNIX-like operating systems.,B,"A critical caution when using `vfork()` is that the child must not modify the parent's address space, as any changes made by the child will be visible to the parent upon resumption."
How does `vfork()`'s efficiency compare to other process creation methods?,It is less efficient due to the overhead of parent suspension.,It is extremely efficient due to the absence of page copying.,Its efficiency is comparable to traditional `fork()` with full page copying.,It is only efficient for very small child processes.,Its efficiency is entirely dependent on the amount of data modified by the child.,B,"`vfork()` is described as extremely efficient for process creation because it involves no page copying, unlike traditional `fork()` or even `fork()` with COW when pages are written to."
What happens to the parent process when `vfork()` is called?,The parent process continues execution concurrently with the child.,The parent process is immediately terminated.,The parent process is suspended until the child process completes its execution or calls `exec()`.,"The parent process enters a waiting state, but its address space is duplicated.",The parent process relinquishes its address space to the child.,C,"When `vfork()` is called, the parent process is suspended, and the child process uses the parent's address space directly. The parent typically resumes after the child calls `exec()` or exits."
"According to the provided glossary, which statement best defines 'copy-on-write'?",A mechanism where data is copied then modified upon a write attempt to a shared page.,"A technique where all shared pages are copied immediately upon process creation, then modified.",A system call that allows a child process to share the parent's address space for read/write operations without copying.,An optimization that prevents any page from being copied under any circumstances.,A process creation method that always bypasses demand paging entirely.,A,"The glossary defines 'copy-on-write' as: 'Write causes data to be copied then modified; on shared page write, page copied, write to copy.'"
"Based on the section glossary, what is the correct definition of 'virtual memory fork'?",A system call that duplicates the parent's entire virtual memory space into the child's.,A technique that uses copy-on-write to share virtual memory pages between parent and child processes.,"The `vfork()` system call; child shares parent's address space for read/write, parent suspended.",A method for creating virtual machines that share physical memory.,A process creation method that ensures absolute memory isolation between parent and child.,C,"The glossary defines 'virtual memory fork' as: '`vfork()` system call; child shares parent's address space for read/write, parent suspended.'"
Which of the following is a primary benefit of demand paging in memory management?,It ensures all pages are loaded into memory simultaneously for faster access.,It reduces the degree of multiprogramming to prevent memory contention.,It saves I/O by loading only the pages that are actively used.,It eliminates the need for swap space on secondary storage.,It guarantees that no page faults will occur during process execution.,C,"Demand paging saves I/O by loading only used pages, rather than loading an entire process into memory at once."
What is 'over-allocating memory' in the context of demand paging?,Providing a process with more physical memory frames than it requests.,"Allocating more virtual memory than physical memory is available, increasing the degree of multiprogramming.",Reserving all available memory frames for a single process to ensure optimal performance.,"Loading all pages of a process into memory at startup, regardless of immediate need.",The process of deallocating memory frames that are no longer in use.,B,"Over-allocating memory means providing access to more virtual memory than physically available, which can increase the degree of multiprogramming by allowing more processes to run concurrently."
A situation where a page fault occurs but no free memory frames are available is a manifestation of which memory management issue?,Optimal page replacement,Belady's anomaly,Over-allocation of memory,Insufficient I/O buffer space,The use of a dirty bit,C,"Over-allocation manifests as a page fault with no free frames, indicating that the system has committed more virtual memory than it can currently support with physical frames."
"When a free frame is needed but none are available, what action is taken to free up a frame?",The system immediately terminates the process causing the page fault.,"The contents of a selected frame are written to swap space, and its page table entry is updated.",The operating system waits for the user to manually free up memory.,All other processes in memory are swapped out to create space.,The CPU utilization is drastically reduced to free memory.,B,"If no free frame is available, a page-replacement algorithm selects a victim frame. Its contents are written to swap space (if modified), and its page table entry is updated to indicate it's no longer in memory."
What is the purpose of a 'modify bit' (or 'dirty bit') in the context of page replacement?,To indicate that a page has been recently referenced by the CPU.,"To mark a page as read-only, preventing any writes.","To signify that a page's contents have been altered, requiring it to be written back to secondary storage before replacement.",To prioritize pages that are frequently used for replacement.,To identify pages that are part of the operating system kernel.,C,"The modify bit, set by hardware, indicates if a page has been written to. If it's set, the page must be written back to secondary storage before being replaced; if not, it can simply be discarded, significantly reducing page-fault service time."
Which of the following is NOT one of the two major problems for demand paging that algorithms aim to solve?,Which frames to replace (page-replacement algorithm).,How many frames to allocate to each process (frame-allocation algorithm).,How to convert logical addresses to physical addresses.,Minimizing the page-fault rate.,Selecting a victim frame.,C,"The two major problems for demand paging are the frame-allocation algorithm (how many frames) and the page-replacement algorithm (which frames). Converting logical to physical addresses is handled by the MMU, not a problem demand paging algorithms solve directly."
What is a 'reference string' primarily used for in the context of page replacement?,To identify the physical location of a page in memory.,To determine the optimal size of the swap space.,To trace the sequence of memory accesses for evaluating page-replacement algorithms.,To allocate frames to different processes dynamically.,To encrypt data before it is written to secondary storage.,C,"A reference string is a trace of memory accesses (specifically, page numbers) used to evaluate the performance of different page-replacement algorithms by simulating their behavior."
Which page replacement algorithm is known for its simplicity and for replacing the page that has been in memory the longest?,Least Recently Used (LRU),Optimal (OPT),"First-In, First-Out (FIFO)",Least Frequently Used (LFU),Second-Chance,C,"FIFO (First-In, First-Out) replaces the oldest page in memory, which is the first one that was brought in. It is easy to understand and program."
What is 'Belady's anomaly'?,The phenomenon where the page-fault rate decreases as the number of allocated frames increases.,A situation where a page replacement algorithm cannot find a victim frame.,The counter-intuitive result where the page-fault rate increases even when more memory frames are allocated to a process.,The inability of an algorithm to use a modify bit effectively.,The problem of having too many free frames in memory.,C,"Belady's anomaly is a specific issue where, for some page-replacement algorithms (like FIFO), increasing the number of available memory frames can paradoxically lead to an increase in the page-fault rate."
Which page replacement algorithm guarantees the lowest possible page-fault rate but is impossible to implement in practice for general-purpose computing?,Least Recently Used (LRU),"First-In, First-Out (FIFO)",Least Frequently Used (LFU),Optimal (OPT),Second-Chance,D,"The Optimal (OPT) page-replacement algorithm replaces the page that will not be used for the longest period of time, thus guaranteeing the lowest page-fault rate. However, it requires future knowledge of the reference string, making it impractical for real-world implementation."
Which page replacement algorithm approximates the Optimal algorithm by replacing the page that has not been used for the longest period of time?,"First-In, First-Out (FIFO)",Most Frequently Used (MFU),Least Recently Used (LRU),Second-Chance,Additional-reference-bits,C,"The Least Recently Used (LRU) algorithm approximates the Optimal algorithm by choosing the page that has not been accessed for the longest duration, based on past usage."
Why is true LRU implementation often expensive?,It requires frequent disk I/O operations for every page replacement.,It needs complex software to track page usage patterns.,It requires substantial hardware assistance and per-memory-reference updates.,"It suffers from Belady's anomaly, leading to high page fault rates.",It necessitates a very large amount of swap space.,C,"True LRU implementation, whether using counters or a stack, requires updating usage information (e.g., timestamps or stack reordering) on every memory reference, which is computationally and hardware-intensively expensive."
What is a 'stack algorithm' in the context of page replacement?,An algorithm that uses a stack data structure to store pages in memory.,A class of algorithms that suffer from Belady's anomaly.,An algorithm that prioritizes pages based on their stack depth in the program's call stack.,A class of page-replacement algorithms that do not suffer from Belady's anomaly.,An algorithm that must be implemented using a hardware-supported stack.,D,"A stack algorithm is a class of page-replacement algorithms that have the property that the set of pages in memory with N frames is always a subset of the pages in memory with N+1 frames. This property ensures they do not suffer from Belady's anomaly, and LRU is an example."
Which hardware component is crucial for LRU-approximation algorithms like the 'additional-reference-bits' algorithm?,A dedicated high-speed cache memory.,A direct memory access (DMA) controller.,A reference bit set by hardware on page access.,A sophisticated I/O buffer management unit.,A specialized cryptographic co-processor.,C,"LRU-approximation algorithms like the additional-reference-bits algorithm rely on a 'reference bit' (or 'use bit') that is automatically set by hardware whenever a page is referenced, providing a basic indication of recent usage."
"In the 'additional-reference-bits' algorithm, how is the history of page use primarily tracked?",By maintaining a precise timestamp of the last access for each page.,By storing page references in a FIFO queue.,By periodically shifting the reference bit into an 8-bit byte for each page.,By counting the total number of accesses for each page since system startup.,By using a doubly linked list to order pages by recency.,C,"The additional-reference-bits algorithm uses an 8-bit byte for each page. On a timer interrupt, the current reference bit is shifted into the most significant bit of this byte, and the other bits are shifted right, creating a history (shift register) of recent usage."
The 'second-chance page-replacement algorithm' is also known by what other name?,Optimal algorithm,Clock algorithm,Stack algorithm,LFU algorithm,MFU algorithm,B,The second-chance page-replacement algorithm is also commonly referred to as the 'clock' algorithm because of its implementation using a circular queue (like a clock face) with a pointer.
"In the 'second-chance algorithm', what happens if a selected page's reference bit is 1?",The page is immediately replaced.,The page is written to disk and then replaced.,"The reference bit is cleared, the arrival time is reset, and the page is given a 'second chance'.",The page is moved to the head of the queue for immediate replacement.,The system determines it's an error and halts.,C,"If a selected page's reference bit is 1, the second-chance algorithm clears the bit, resets the page's 'arrival time' to the current time, and gives it a 'second chance' by moving the pointer to the next page without replacing it immediately."
The 'enhanced second-chance algorithm' primarily uses which two bits to classify pages for replacement?,Valid bit and Present bit,Read bit and Write bit,Protection bit and Share bit,Reference bit and Modify bit,Access bit and Execute bit,D,"The enhanced second-chance algorithm considers the (reference bit, modify bit) pair to classify pages into four categories, allowing for a more informed replacement decision that prioritizes clean, unused pages."
"According to the 'enhanced second-chance algorithm', which class of pages is considered the 'best to replace' due to minimal overhead?","(0, 1) - not recently used but modified","(1, 0) - recently used but clean","(0, 0) - neither recently used nor modified","(1, 1) - recently used and modified",All classes are equally good for replacement.,C,"The (0, 0) class (neither recently used nor modified) is the best to replace because the page hasn't been used lately and doesn't need to be written out to secondary storage, thus minimizing overhead."
Which counting-based page replacement algorithm selects the page with the smallest access count?,Most Frequently Used (MFU),Optimal (OPT),Least Recently Used (LRU),Least Frequently Used (LFU),"First-In, First-Out (FIFO)",D,"The Least Frequently Used (LFU) algorithm replaces the page that has the smallest count of references, assuming it's the least useful."
A common problem with the LFU algorithm is that a page heavily used initially might remain in memory even if unused later. What is a suggested solution for this?,Periodically resetting all counts to zero.,Converting it to an LRU algorithm.,Shifting counts right periodically to create an exponentially decaying average.,Increasing the count of frequently used pages to infinity.,Only allowing pages with zero counts to be replaced.,C,"To address the issue where initially heavily used pages retain high counts and aren't replaced, LFU can be modified by periodically shifting counts right, effectively creating an exponentially decaying average that favors more recent usage patterns."
Page-buffering algorithms are typically used in addition to main page-replacement algorithms. What is one of their benefits?,They eliminate the need for swap space entirely.,They ensure that all pages are modified before being written to disk.,They allow a process to restart faster after a page fault by reading the desired page into a free frame before the victim is written out.,They reduce the total number of page faults that occur.,They are a direct replacement for the LRU algorithm.,C,"Page-buffering algorithms often maintain a pool of free frames. On a page fault, the desired page can be read into a free frame from this pool immediately, allowing the process to restart faster, even if the victim frame has not yet been written out to swap space."
What is 'raw disk' or 'raw I/O' in the context of applications and page replacement?,A highly compressed format for storing data on disk to save space.,"A method for accessing secondary storage directly as an array of logical blocks, bypassing file-system services.",A technique that uses solid-state drives exclusively for all I/O operations.,A system where all disk operations are buffered in physical memory before being written.,A protocol for secure data transmission over a network.,B,"Raw disk (or raw I/O) refers to allowing special programs to use secondary storage directly as a large sequential array of logical blocks, completely bypassing standard file-system services like demand paging, locking, prefetching, and directory management."
"Why might some applications, like databases, prefer to use raw disk access over standard OS virtual memory buffering?",Raw disk access allows them to utilize unlimited virtual memory.,Applications can have a better understanding of their specific memory and storage usage patterns than general-purpose OS algorithms.,Raw disk access enables the application to run directly on the hardware without an OS.,It provides a mechanism for automatic data encryption and compression.,Raw disk access eliminates the need for any form of page replacement.,B,"Applications such as databases often perform worse with OS virtual memory buffering because they are designed with their own sophisticated memory and storage management, which can be more efficient for their specific workloads than the general-purpose algorithms provided by the OS. Raw disk access allows them to implement their own optimized I/O."
"In a pure demand paging system, what happens when a process terminates?",Its frames are immediately swapped to backing store.,Its frames are reallocated to other running processes without being freed.,Its frames are returned to the free-frame list.,The OS reclaims them for exclusive kernel use.,They remain allocated but marked as inactive until system shutdown.,C,"When a process terminates in a pure demand paging system, its allocated frames are returned to the free-frame list for future use."
Which of the following describes a common variation in frame allocation where the OS might utilize frames from the free-frame list?,The OS permanently reserves all free frames for future kernel upgrades.,"The OS allocates buffer/table space from the free-frame list, which can be temporarily used for user paging.","The OS strictly partitions free frames, dedicating half to kernel and half to user processes.","The OS uses free frames exclusively for disk caching, never for user processes.",The OS converts all free frames into swap space on secondary storage.,B,"A variation allows the OS to allocate buffer/table space from the free-frame list, with these frames being available for user paging when not otherwise in use."
What is the primary constraint regarding the minimum number of frames that must be allocated to a process?,It cannot exceed half of the total available frames.,It must be enough to hold the entire program code.,It must be at least the number required for all pages an instruction can reference.,"It is always a fixed value, such as three frames per process.",It depends solely on the process's priority.,C,"A process must be allocated at least a minimum number of frames, which is defined by the computer architecture and must be enough for all pages an instruction can reference, preventing an instruction from faulting before completion."
Why does a lower number of allocated frames generally lead to a higher page-fault rate and slower execution?,Fewer frames mean more I/O operations are needed to load the entire program at once.,"With fewer frames, the system has less buffer space for disk operations.","A smaller working set of pages can reside in memory, leading to more frequent page-ins.",The CPU cache becomes less effective with fewer main memory frames.,The operating system takes more time to manage a smaller number of frames.,C,"Fewer frames allocated to a process means that its working set of pages cannot be fully resident in memory, increasing the likelihood of page faults as needed pages are swapped out and then back in."
"According to the text, what is the minimum number of frames a process requires for a one-level indirect addressing instruction?",1 frame,2 frames,3 frames,4 frames,6 frames,C,"For a one-level indirect addressing instruction, a process needs at least three frames: one for the instruction, one for the address, and one for the operand."
Which allocation algorithm assigns an equal number of frames to each process?,Proportional allocation,Priority-based allocation,Dynamic allocation,Equal allocation,Global allocation,D,Equal allocation is the strategy where available frames are split equally among all active processes.
What is the primary drawback of 'equal allocation' when processes have significantly different memory needs?,"It can lead to over-allocation for larger processes, causing system instability.",It requires complex algorithms to manage frame distribution.,"It allocates frames inefficiently, potentially wasting frames for smaller processes.",It makes it difficult to implement page replacement algorithms.,It prioritizes system processes over user processes.,C,"Equal allocation can be inefficient because it gives the same number of frames to all processes, regardless of their actual memory needs, potentially wasting frames for processes that require less memory."
How does 'proportional allocation' determine the number of frames assigned to each process?,Based on the CPU priority of the process.,In inverse proportion to the process's virtual memory size.,In proportion to the process's virtual memory size.,By allocating frames from a reserved pool for each process.,Randomly assigns frames from the free-frame list.,C,"Proportional allocation assigns memory frames to a process in proportion to its virtual memory size, aiming for a more efficient distribution based on need."
Under what circumstance might processes lose frames in an equal or proportional allocation scheme?,When the system's overall memory utilization decreases.,When the multiprogramming level decreases.,When a process's priority is lowered.,When the multiprogramming level increases.,When the system switches from demand paging to pre-paging.,D,"If the multiprogramming level increases (more processes share the same total frames), each process's share of frames will necessarily decrease."
"Which page-replacement strategy allows a process to select a replacement frame from any frame in the system, even if it is currently allocated to another process?",Local replacement,Segmented replacement,Proportional replacement,Global replacement,Priority-based replacement,D,"Global replacement allows a process to choose a replacement frame from the set of all frames in the system, potentially taking frames from other processes."
What is a disadvantage of a global replacement strategy compared to a local replacement strategy?,It typically results in lower system throughput.,A process's performance becomes dependent on the paging behavior of other processes.,It is more complex to implement in the operating system kernel.,It prevents high-priority processes from acquiring more frames.,It requires a larger minimum number of frames for each process.,B,"In global replacement, a process's performance can be adversely affected by the paging behavior of other processes (external circumstances), as they might take its frames."
A page fault that requires reading the page from the backing store is known as a:,Minor page fault,Soft page fault,Cache miss,Major page fault,TLB miss,D,A major page fault (or hard fault) occurs when the referenced page is not in memory and must be read from secondary storage (backing store).
Which of the following scenarios would typically result in a minor page fault?,Accessing a page that was swapped out to disk due to memory pressure.,Referencing a page that has never been loaded into physical memory.,A process referencing a shared library page that is already in memory but lacks a logical mapping for the process.,"Attempting to write to a read-only page, triggering a protection fault.",An invalid memory address being accessed by a process.,C,"A minor page fault occurs when the page is already in physical memory (e.g., a shared library page used by another process), but the current process does not have a valid logical mapping to it, only requiring a page table update."
"In the context of page reclaiming, what are 'reapers'?",Processes terminated by the OOM killer.,Background routines that scan memory to free frames and maintain minimum free memory.,Algorithms used to select a victim page during a major page fault.,User-space applications that monitor memory usage.,Hardware components responsible for memory garbage collection.,B,"Reapers are kernel routines that are triggered when free memory falls below a minimum threshold, and they reclaim pages from processes to maintain sufficient free memory."
What is the function of the Linux 'out-of-memory (OOM) killer'?,It alerts the system administrator when memory is full.,It terminates processes to free memory when free memory is critically low.,It defragments memory pages to improve performance.,It prevents processes from allocating too much memory initially.,It migrates processes to different NUMA nodes to balance memory usage.,B,"The Linux OOM killer is a mechanism that terminates processes, typically those with the highest 'OOM score' (indicating high memory usage), to free up memory when the system runs out."
What defines a Non-Uniform Memory Access (NUMA) system?,All CPU cores have equal access time to all memory locations.,Memory access time varies depending on the CPU core accessing it.,It is a system that uses only solid-state drives for storage.,Memory is exclusively accessed via a shared bus.,It uses a single large physical memory module for all CPUs.,B,"NUMA systems are characterized by varying memory access times depending on which CPU core is accessing a particular memory location, typically faster for local memory and slower for remote memory."
How do NUMA-aware allocation strategies typically handle page faults?,They always allocate the new frame from a centralized free-frame pool.,They allocate the new frame as close as possible to the CPU that caused the fault.,They swap out a page from a remote memory node to make space for the new page.,They randomly select a free frame from anywhere in the system.,They request user intervention to specify the allocation location.,B,"NUMA-aware systems prioritize allocating frames 'as close as possible' to the CPU that caused the page fault, typically from memory on the same system board, to minimize latency."
What is the purpose of 'lgroups' in Solaris for NUMA systems?,To group similar applications for better resource management.,To define security boundaries for memory access.,To gather CPUs and memory into locality groups for optimized access and reduced latency.,To manage inter-process communication between different CPU cores.,To dynamically adjust CPU clock speeds based on memory load.,C,Solaris uses 'lgroups' (locality groups) to group CPUs and their associated memory into hierarchical structures. The system then schedules threads and allocates memory within these lgroups to minimize memory latency and maximize cache hit rates.
A page fault that is resolved by updating the page table without needing to read data from secondary storage is referred to as a:,Hard page fault,Major page fault,Minor page fault,Swap-in fault,Protection fault,C,"A minor page fault (or soft fault) is resolved without performing I/O from the backing store, usually because the page is already in memory but needs a page table entry update."
"If a system has 93 frames available for user processes and 5 processes are running, how many frames would each process receive under 'equal allocation' and how many would be leftover for buffer space?","15 frames each, 18 leftover","18 frames each, 3 leftover","20 frames each, 3 leftover","17 frames each, 8 leftover","18 frames each, 0 leftover",B,"Under equal allocation, 93 frames / 5 processes = 18 frames per process with a remainder of 3 frames (93 - 5*18 = 3) which can be used for buffer space or other system needs."
Which of the following best defines 'thrashing' in an operating system context?,A process spending more time executing than paging.,"A low rate of page faults, indicating efficient memory utilization.",High paging activity where a process spends more time paging than executing.,The process of swapping entire processes between main memory and disk.,The mechanism by which the CPU scheduler increases multiprogramming.,C,"Thrashing is defined as high paging activity where a process spends more time paging than executing, leading to severe performance problems."
What is a primary symptom of a process that is thrashing?,It rarely experiences page faults.,It replaces pages that are not needed immediately.,"It quickly page-faults and replaces pages needed immediately, leading to repeated faults.",It consistently uses its minimum required frames efficiently.,Its CPU utilization remains consistently high.,C,"A process without 'enough' frames (minimum needed for working set) will quickly page-fault and replace pages needed immediately, causing it to fault again and again."
Which of the following operating system behaviors can exacerbate or cause thrashing?,Maintaining a high CPU utilization by reducing multiprogramming.,Increasing the degree of multiprogramming when CPU utilization is low.,Implementing a local page-replacement algorithm for all processes.,Allocating enough frames to each process to cover its working set.,Prioritizing processes with high page-fault rates for CPU time.,B,"The text describes a scenario where low CPU utilization prompts the OS to increase multiprogramming, which can lead to processes taking frames from each other, increasing page faults, and eventually resulting in thrashing as CPU utilization drops further and the OS continues to increase multiprogramming."
How does the use of a global page-replacement algorithm contribute to thrashing?,It ensures each process maintains its minimum frame requirement.,"It allows a thrashing process to steal frames from other processes, causing them to fault.","It prevents new processes from being initiated, thus reducing multiprogramming.",It allocates frames based on a process's working-set size.,It limits paging device queues by prioritizing non-faulting processes.,B,"A global page-replacement algorithm replaces pages without regard to the process they belong to. This allows a process needing more frames (e.g., one that's starting to thrash) to take frames from other processes, which then causes those processes to fault, cascading the problem across the system."
"During thrashing, what typically happens to system throughput and CPU utilization?",Both system throughput and CPU utilization increase.,"System throughput increases, but CPU utilization decreases.",Both system throughput and CPU utilization plunge/drop sharply.,"System throughput plunges, while CPU utilization remains stable.","CPU utilization increases rapidly, while throughput remains low.",C,"The text states that when thrashing occurs, 'system throughput plunges' and 'CPU utilization drops sharply' (as illustrated in Figure 10.6.1). Processes spend all their time paging, and no work gets done."
"According to the text, what is the most direct action to stop thrashing once it has begun?",Increase the degree of multiprogramming.,Implement a global page-replacement algorithm.,Decrease the degree of multiprogramming.,Increase the system's effective memory-access time.,Allow processes to frequently change their working sets.,C,The text explicitly states: 'To stop thrashing: decrease degree of multiprogramming.'
What is the primary advantage of a 'local replacement algorithm' or 'priority replacement algorithm' in mitigating thrashing?,It allows a thrashing process to steal frames from other processes more efficiently.,It ensures that all processes have an equal number of frames.,It prevents a thrashing process from stealing frames from other processes.,It completely eliminates the need for a paging device.,It automatically adjusts the working-set window size for each process.,C,Local (or priority) replacement algorithms are defined as avoiding thrashing by not allowing a process to steal frames from other processes. A process selects pages only from its own frames.
"While local replacement algorithms help limit thrashing effects, why do they not entirely solve the problem?","They increase the CPU utilization, leading to more processes being admitted.",They don't prevent processes from needing more frames than available in their own set.,"Thrashing processes still queue for the paging device, increasing average service time for page faults for all processes.",They lead to an increase in the number of frames required for each process.,They only work for processes with very small working sets.,C,"The text explains that while local replacement prevents a thrashing process from stealing frames, the thrashing processes still queue for the paging device, leading to increased average service time for page faults and, consequently, increased effective access time for all processes."
"What is the fundamental approach to preventing thrashing, as stated in the text?",To always use a global page-replacement algorithm.,To continuously increase the degree of multiprogramming.,To provide a process with enough frames.,To ignore the page-fault rate of processes.,To exclusively use the Page-Fault Frequency strategy.,C,The text explicitly states: 'To prevent thrashing: provide process with enough frames.'
"What concept is introduced to determine how many frames a process needs, based on the pages it actually uses?",The Page-Fault Frequency strategy.,The Global Replacement Algorithm.,The Locality Model.,The Least Recently Used (LRU) algorithm.,"The First-In, First-Out (FIFO) algorithm.",C,The text states: 'How many frames needed? Look at frames actually used -> locality model.'
"According to the locality model, what is a 'locality'?",The total number of frames available in physical memory.,The set of all pages ever referenced by a process.,The set of pages actively used together by a process during a phase of its execution.,A fixed memory region allocated to a process at startup.,The physical address space where a process's code resides.,C,A 'locality' is defined as the set of pages actively used together. The locality model states that a process moves from locality to locality during execution.
The working-set model is based on which fundamental assumption?,Processes typically require an infinite number of frames.,Memory accesses are completely random.,The principle of locality.,All processes should have the same page-fault frequency.,CPU utilization is inversely proportional to multiprogramming.,C,"The working-set model is based on the locality assumption, meaning that programs exhibit patterned memory accesses, not random ones."
"In the working-set model, what does the parameter Δ (Delta) define?",The maximum allowed page-fault frequency.,The total number of available frames in memory.,"The working-set window, representing the most recent page references.",The number of processes that can be simultaneously active.,The minimum number of frames required for any process.,C,"The working-set model uses parameter Δ to define the 'working-set window', which is the time interval or number of page references examined to determine the working set."
How is a 'working set' defined in the working-set model?,All pages currently present in physical memory.,The set of all pages ever referenced by a process since its inception.,The set of pages referenced in the most recent Δ (Delta) page references.,The minimum number of frames required for a process to execute without faulting.,The sum of all working-set sizes for all active processes.,C,"The 'working set' is defined as the set of pages in the most recent Δ references, approximating a program's current locality."
"In the working-set model, if the total demand for frames (D) (sum of all $WSS_i$) exceeds the total available frames (m), what is the expected outcome?",The operating system will increase the degree of multiprogramming.,All processes will run with optimal performance.,Thrashing will occur because some processes will lack enough frames.,The working-set window (Δ) will automatically decrease.,The system will automatically switch to a global replacement algorithm.,C,The text states: 'If D > m (total available frames) -> thrashing (some processes lack frames).'
How does an operating system typically manage processes using the working-set model to prevent thrashing?,It randomly allocates frames to processes without monitoring their working sets.,It decreases the degree of multiprogramming whenever CPU utilization is high.,"It allocates enough frames for each process's working-set size; if total demand exceeds available frames, it suspends a process.",It constantly swaps all processes in and out of memory to balance load.,It maintains a fixed number of frames for all processes regardless of their demand.,C,"The OS monitors the working set of each process, allocates enough frames for its size. If the sum of working-set sizes exceeds available frames, the OS suspends a process, swaps out its pages, reallocates frames, and restarts it later."
What is a major difficulty in implementing the working-set model precisely?,Determining the total number of available physical frames.,Calculating the page-fault frequency accurately.,Tracking the moving working-set window in real-time.,Deciding which processes to suspend when memory is insufficient.,Defining what constitutes a 'locality' for a given program.,C,The text points out the difficulty: 'tracking moving working-set window.'
How does an operating system approximate the working-set window in practice?,By continuously monitoring all page references in real-time with no approximation.,By using a fixed-interval timer interrupt and reference bits.,By always assuming an infinite working-set window.,By setting the Delta (Δ) parameter to zero for all processes.,By only considering pages that have been modified (dirty bits).,B,"The working-set window is approximated using a fixed-interval timer interrupt and reference bits. At each interrupt, reference bits are copied and cleared, helping to track recent page usage."
Why was the Page-Fault Frequency (PFF) strategy developed as an alternative to the working-set model for thrashing control?,The working-set model was too simple and not comprehensive enough.,The working-set model was entirely unsuccessful and never used.,"The working-set model was successful and useful for prepaging, but clumsy for thrashing control.","PFF allows for a completely random allocation of frames, simplifying management.",PFF does not require monitoring of page references.,C,"The text states: 'Working-set model successful, useful for prepaging, but clumsy for thrashing control. Page-fault frequency (PFF) strategy: more direct.'"
How does the Page-Fault Frequency (PFF) strategy adjust the number of frames allocated to a process?,"If PFF is too high, it removes frames; if too low, it allocates more frames.",It always allocates a fixed number of frames regardless of PFF.,It ignores the PFF and only considers the process's priority.,"If actual PFF exceeds an upper limit, it allocates another frame; if it falls below a lower limit, it removes a frame.",It only removes frames when the system is about to crash.,D,"The PFF strategy establishes upper and lower bounds on the desired page-fault rate. If the actual PFF exceeds the upper limit, the process needs more frames, so another is allocated. If it falls below the lower limit, the process may have too many frames, so one is removed."
"According to current practice, what is considered the best way to avoid thrashing and swapping?",Continuously monitor and adjust the degree of multiprogramming.,Implement complex working-set and page-fault frequency algorithms.,Include enough physical memory to meet virtual memory demand.,Force all applications to use a local replacement algorithm.,Increase the frequency of timer interrupts for better working-set approximation.,C,The 'Current practice' section states: 'Best practice: include enough physical memory to avoid thrashing/swapping.' This provides the best user experience.
Memory compression is presented as an alternative to which common memory management technique?,Virtualization,Disk caching,Paging,Segmentation,Multithreading,C,The text states: 'Alternative to paging: memory compression.'
What is the primary method memory compression uses to reduce memory usage?,Swapping frames to disk more frequently,Consolidating multiple frames into a single frame,Increasing the size of individual memory frames,Eliminating the need for a free-frame list,Reducing the number of processes in memory,B,The text explains: 'Compress several frames into a single frame' and 'Reduces memory usage without swapping pages.'
"According to the example, what condition typically triggers memory compression as part of page replacement?",A page fault occurs for a non-resident page,The CPU utilization exceeds a certain threshold,The free-frame list drops below a specific threshold,A new process requests a large block of memory,An application explicitly requests memory compression,C,The example states: 'Free-frame list below threshold → triggers page replacement.'
"When frames are selected for compression, what happens instead of writing them to swap space?",They are immediately discarded to free memory,They are marked as read-only and kept in memory,They are compressed and stored within another single page frame,They are moved to a special 'quarantine' area of memory,They are replicated across multiple physical frames for redundancy,C,"The text describes: 'Instead of writing to swap space, compress frames (e.g., three) into single page frame.'"
"After frames like 15, 3, and 35 are compressed and stored into a new frame (e.g., frame 7), what happens to the original frames (15, 3, 35)?",They remain on the modified-frame list,They are permanently deleted from memory,They are moved to the free-frame list,They are immediately swapped out to disk,They are marked as reserved for future use by the compressed data,C,"The example states: 'Frames 15, 3, 35 moved to free-frame list.'"
What action occurs if a compressed frame is referenced by the system?,The system issues a warning to the user,The compressed frame is immediately swapped out,"A page fault occurs, leading to decompression and restoration of original pages",The frame is simply re-compressed into an even smaller size,"The reference is ignored, as the data is not directly accessible",C,"The text explains: 'If compressed frame referenced → page fault, decompressed, restoring original pages.'"
Which mobile operating systems are noted for using memory compression as an integral part of their memory-management strategy due to generally not supporting standard swapping/paging?,Windows Mobile and BlackBerry OS,Android and iOS,Symbian and WebOS,Tizen and Sailfish OS,Ubuntu Touch and Firefox OS,B,"The text states: 'Mobile systems (Android, iOS) generally don't support standard swapping/paging. Memory compression integral to their memory-management strategy.'"
Which of the following desktop operating systems are mentioned as supporting memory compression?,Linux and Chrome OS,FreeBSD and OpenBSD,Windows 7 and macOS (Pre-10.9),Windows 10 and macOS (Version 10.9+),MS-DOS and Windows XP,D,The text explicitly states: 'Windows 10 and macOS support memory compression.'
"On Windows 10, which type of applications are specifically mentioned as candidates for memory compression on mobile devices?",Classic Win32 applications,Universal Windows Platform (UWP) apps,Java-based applications,Linux subsystem applications,Legacy .NET Framework applications,B,The text specifies: 'Windows 10: Universal Windows Platform (UWP) apps on mobile devices are candidates.'
How does macOS (Version 10.9+) prioritize memory compression relative to paging?,It always pages to SSD before attempting any compression.,It only compresses pages that are actively being used.,"It compresses LRU pages when free memory is short, then pages if needed.",It compresses pages only after all swap space has been exhausted.,It uses memory compression and paging simultaneously with equal priority.,C,"The text states: 'macOS (Version 10.9+): compresses LRU pages when free memory is short, then pages if needed.'"
"According to performance tests on macOS, how does memory compression compare to paging to an SSD?",Memory compression is significantly slower than paging to SSD.,Memory compression and paging to SSD have comparable performance.,Memory compression is faster than paging to SSD.,Memory compression is only faster if the SSD is nearly full.,There is no significant performance difference between the two.,C,The text notes: 'Performance tests: memory compression faster than paging to SSD on macOS.'
What is a requirement for memory compression to store compressed pages?,A dedicated hardware compression unit,Allocating free frames for the compressed data,Pre-existing swap space on a hard drive,Disabling the CPU's caching mechanism,Increasing the total physical RAM in the system,B,The text states: 'Memory compression requires allocating free frames for compressed pages.'
What is the main contention or trade-off in designing memory compression algorithms?,Between data integrity and system stability,Between network bandwidth and CPU cycles,Between compression speed and compression ratio,Between power consumption and memory capacity,Between security vulnerabilities and ease of implementation,C,The text mentions: 'Contention between compression speed and compression ratio (amount of reduction).'
What is generally true about memory compression algorithms that aim for higher compression ratios?,They are typically faster and less computationally expensive.,They are only effective on very small amounts of data.,They tend to be slower and more computationally expensive.,They require specialized hardware found only in servers.,They have no impact on CPU usage or speed.,C,"The text states: 'Higher compression ratios → slower, more computationally expensive algorithms.'"
How can the performance of memory compression be improved using hardware resources?,By increasing the clock speed of the single core,By reducing the total amount of RAM available,By using parallel compression with multiple cores,By offloading compression tasks to the GPU,"By using slower, but more energy-efficient, storage",C,The text indicates: 'Improved by parallel compression using multiple cores.'
Microsoft's Xpress and Apple's WKdm are cited as examples of compression algorithms that balance which two factors?,Security and accessibility,Storage capacity and network latency,High ratios with fast algorithms,Power efficiency and manufacturing cost,Compatibility and proprietary features,C,"The text states these examples 'balance factors: high ratios with fast algorithms' and are 'fast, compress to 30-50% original size.'"
What is the definition of 'compression ratio' as provided in the glossary?,The speed at which data can be compressed.,The amount of memory required to perform compression.,A measurement of compression effectiveness (ratio of compressed to uncompressed space).,The number of frames that can be compressed into one.,The percentage of CPU time spent on compression tasks.,C,The glossary defines 'compression ratio' as 'Measurement of compression effectiveness (ratio of compressed to uncompressed space).'
"What does the acronym 'UWP' stand for, and what is its purpose according to the glossary?",Unified Windows Protocol; a new network communication standard.,Universal Wireless Power; a technology for wireless charging.,Unique Workstation Platform; a system for specialized workstations.,Universal Windows Platform; Windows 10 architecture providing common app platform for all devices running it.,Underlying Web Portal; a framework for web-based applications.,D,The glossary defines 'Universal Windows Platform (UWP)' as 'Windows 10 architecture providing common app platform for all devices running it.'
What is the estimated typical compression ratio achieved by fast algorithms like Microsoft's Xpress and Apple's WKdm?,5-10% of original size,15-25% of original size,30-50% of original size,60-75% of original size,80-95% of original size,C,"The text mentions: 'Examples: Microsoft's Xpress, Apple's WKdm → fast, compress to 30-50% original size.'"
What is the typical behavior when a user-mode process requests memory from the kernel?,Pages are allocated from the kernel's free page frame list.,Memory is directly allocated from the physically contiguous kernel pool.,A fixed-size 256 KB segment is always granted.,The request is immediately denied due to security policies.,The memory is allocated from a dedicated user-mode heap.,A,User-mode process requests memory typically result in pages being allocated from the kernel's free page frame list.
"When a user-mode process requests a single byte of memory, what type of fragmentation is commonly observed due to the allocation of an entire page frame?",External fragmentation,Internal fragmentation,Contiguous fragmentation,Virtual fragmentation,Paging fragmentation,B,"A single byte request resulting in an entire page frame being granted leads to internal fragmentation, as the full page is allocated but only a small portion is used."
Which of the following is a primary reason why kernel memory is often allocated from a different free-memory pool compared to user-mode memory?,Kernel requests always require larger memory blocks than user-mode processes.,Kernel code and data are always subject to paging.,"Kernel requests vary in data structure sizes, some being less than a page, requiring conservative memory use.",User-mode pages must always be physically contiguous.,Kernel memory is never deallocated once allocated.,C,"Kernel memory allocation differs because kernel requests often involve varying data structure sizes, some less than a page, necessitating conservative memory use to minimize fragmentation."
"Why might hardware devices interacting with physical memory require physically contiguous pages, unlike typical user-mode pages?",Hardware devices use virtual memory interfaces.,User-mode pages are always physically contiguous.,Hardware devices directly interact with physical memory and may not have a virtual memory interface.,Kernel code is always mapped to non-contiguous physical memory.,It's a security measure to prevent unauthorized access.,C,"Hardware devices interact directly with physical memory and often lack a virtual memory interface, thus requiring physically contiguous pages for their operations."
What are the two main strategies discussed for managing kernel free memory?,First-fit and Best-fit,Paging and Swapping,Buddy system and Slab allocation,Segmentation and Partitioning,LIFO and FIFO,C,The text explicitly states 'buddy system' and 'slab allocation' as the strategies for managing kernel free memory.
The 'buddy system' is characterized as a 'power-of-2 allocator'. What does this mean for how it satisfies memory requests?,"It allocates memory in arbitrary sizes, then rounds down to the nearest power of 2.","It satisfies requests in units sized as a power of 2 (e.g., 4 KB, 8 KB, 16 KB).",It always allocates exactly 2 KB or 4 KB segments.,It divides memory into two equal parts repeatedly until the exact size is found.,It only allocates memory if the request is an exact power of 2.,B,A 'power-of-2 allocator' means the buddy system satisfies memory requests in units that are sized as a power of 2.
"If a kernel process requests 21 KB of memory using the buddy system, what is the size of the segment that would typically be allocated?",16 KB,21 KB,24 KB,32 KB,64 KB,D,"Requests not appropriately sized are rounded up to the next highest power of 2. For 21 KB, the next highest power of 2 is 32 KB."
What is the primary advantage of the buddy system for kernel memory management?,It eliminates all forms of internal fragmentation.,It ensures that all memory requests are fulfilled with exact sizes.,It allows for quick combining of adjacent freed segments into larger ones through coalescing.,It guarantees that less than 10% of allocated memory is wasted.,It exclusively allocates non-contiguous memory blocks.,C,A key advantage of the buddy system is its ability to quickly combine adjacent buddies to form larger segments using coalescing.
What is a significant drawback of the buddy system regarding memory utilization?,It requires manual intervention to allocate memory.,It always results in external fragmentation.,It suffers from internal fragmentation due to requests being rounded up to the next power of 2.,It cannot allocate memory segments larger than 4 KB.,It has very slow allocation and deallocation times.,C,A major drawback is the internal fragmentation caused by rounding up memory requests to the next highest power of 2.
"In the context of slab allocation, what is a 'slab' defined as?",A collection of unrelated kernel objects.,One or more physically contiguous pages.,A single byte of allocated memory.,A fixed-size 256 KB segment for kernel data.,A temporary copy of user data for performance.,B,A slab is defined as one or more physically contiguous pages used in slab allocation.
What is a 'cache' in the context of slab allocation?,A single kernel object.,A temporary data storage for CPU registers.,A collection of one or more slabs.,A list of free page frames managed by the kernel.,A mechanism for virtual to physical address translation.,C,"In slab allocation, a cache consists of one or more slabs and is used to store objects of a specific kernel data structure."
"In slab allocation, a single cache is typically maintained for what purpose?",To handle all general-purpose memory requests.,To store executable kernel code.,For each unique kernel data structure.,To manage the free page frame list.,To buffer I/O operations.,C,"Slab allocation uses a single cache for each unique kernel data structure, such as process descriptors or file objects."
"When the slab allocator needs to fulfill a new object request, what is its preferred order of satisfaction?","First from a newly allocated slab, then an empty slab, then a partial slab.","First from an empty slab, then a partial slab, then a newly allocated slab.","First from a partial slab, then an empty slab, then a newly allocated slab.",It randomly picks any available slab.,Only from a full slab after objects are released.,C,"The slab allocator first attempts to find a free object in a partial slab, then an empty slab, and finally allocates a new slab if necessary."
Which of the following is a primary benefit of slab allocation regarding memory fragmentation?,It introduces external fragmentation but reduces internal fragmentation.,It ensures memory is always allocated in fixed 4 KB pages.,It eliminates memory wasted due to fragmentation by returning the exact amount requested for kernel objects.,It relies on periodic defragmentation to consolidate free space.,It rounds up memory requests to the next power of 2.,C,A main benefit of slab allocation is that it avoids memory wasted due to fragmentation by dividing slabs into object-sized chunks and returning the exact amount requested.
How does slab allocation contribute to satisfying memory requests quickly?,By deallocating all objects immediately after use.,By creating objects in advance and making released objects immediately available from the cache.,By always allocating a brand new slab for every request.,By using a best-fit algorithm for memory placement.,By outsourcing memory management to user-mode processes.,B,"Slab allocation speeds up memory requests because objects are created in advance, and released objects are marked free and returned to the cache, making them immediately available for reuse."
"In the Linux kernel's slab allocator, what is the state of a slab if all of its objects are marked as 'used'?",Empty,Partial,Available,Full,Idle,D,A slab is considered 'Full' when all objects within it are marked as 'used'.
"Which Linux kernel memory allocator is specifically designed for systems with limited memory, such as embedded systems?",SLAB,SLUB,SLOB,Buddy system,Page frame allocator,C,"The SLOB allocator is explicitly mentioned as being for systems with limited memory, such as embedded systems."
"What is a significant improvement of the SLUB allocator over the original SLAB allocator in Linux, as mentioned in the text?",It introduced the concept of physically contiguous pages for slabs.,It added per-CPU queues for objects to enhance performance.,"It stores metadata in the 'page' structure instead of with each slab, reducing overhead.",It ensures that all allocated memory is an exact power of 2.,It is primarily used for user-mode memory requests.,C,"The SLUB allocator reduced overhead by storing metadata in the 'page' structure instead of with each slab, and it also removed per-CPU queues for objects."
"The SLOB allocator manages memory through three lists: small, medium, and large. What policy does it use to allocate from the appropriate list?",Least Recently Used (LRU),Best-fit,Worst-fit,First-fit,Round Robin,D,The SLOB allocator allocates from its lists using a first-fit policy.
What is the definition of 'coalescing' in the context of the buddy system?,Dividing a large memory segment into smaller 'buddies'.,Rounding up a memory request to the next highest power of 2.,Combining freed memory in adjacent buddies into larger segments.,Allocating memory from a fixed-size segment of physically contiguous pages.,Marking objects as 'used' in a slab.,C,Coalescing refers to the process of combining freed memory in adjacent buddies to form larger segments.
What defines an 'object' in the context of slab allocation?,A generic block of memory.,A single physically contiguous page.,An instantiation of a kernel data structure.,A cache of memory.,A list of free memory regions.,C,"In slab allocation, an 'object' is an instantiation of a kernel data structure (e.g., process descriptors)."
"What is the primary purpose of ""prepaging"" in memory management?",To reduce the overall number of physical memory pages used by a process.,To prevent a high number of initial page faults when a process starts.,To improve the hit ratio of the Translation Look-aside Buffer (TLB).,To allow pages to be locked in memory during I/O operations.,To ensure that all pages of a process are always present in memory.,B,Prepaging is an attempt to prevent the high number of initial page faults that occur due to initial locality when a process starts.
Which strategy describes how prepaging attempts to achieve its goal?,By swapping out less frequently used pages to disk.,By bringing some or all potentially needed pages into memory at once.,By increasing the size of the TLB to reduce lookup times.,By reordering process instructions to improve data locality.,By dynamically adjusting the page replacement algorithm.,B,"The strategy for prepaging is to bring some or all needed pages into memory at once, anticipating their use."
How does the working-set model relate to prepaging when a suspended process resumes?,"The working set is ignored, and all pages are brought in on demand.",Only the modified pages from the working set are brought back.,The entire working set is automatically brought back into memory before restarting the process.,The working set is copied to disk to free up memory for other processes.,"Prepaging is only applied to the code segment, not the working set.",C,An example of prepaging is remembering the working set for a suspended process and automatically bringing the entire working set back before restarting it.
"In the cost analysis of prepaging, if 's' pages are prepaged and 'alpha' is the fraction of those pages actually used, what scenario would cause prepaging to be disadvantageous?",Alpha is approximately 1.,The cost of 's * (1 - alpha)' unnecessary pages is much higher than 's * alpha' saved page faults.,The cost of 's * alpha' saved page faults is much higher than 's * (1 - alpha)' unnecessary pages.,The total number of prepaged pages 's' is very small.,The system has an infinite supply of free memory frames.,B,"Prepaging loses if alpha is approximately 0, meaning many prepaged pages are not used. This makes the cost of unnecessary pages outweigh the benefit of saved page faults."
For which type of data is prepaging generally more predictable and effective?,Dynamically linked libraries.,Executable programs.,Data structures with poor locality.,Files accessed sequentially.,Encrypted data streams.,D,"Prepaging files is more predictable because they are often accessed sequentially, making it easier to anticipate which pages will be needed."
Which Linux system call is explicitly mentioned as a mechanism for prefetching file contents into memory?,mmap(),sync(),readahead(),mlock(),fork(),C,The text states that the Linux `readahead()` system call prefetches file contents into memory.
Page sizes in new machine designs are invariably chosen to be what type of number?,Prime numbers.,Multiples of 100.,Powers of 2.,Numbers divisible by 3.,Arbitrary integers.,C,The text states that page sizes are 'invariably powers of 2'.
How does decreasing the page size affect the size of the page table?,"It decreases the number of pages, thus decreasing the page table size.","It increases the number of pages, thus increasing the page table size.",It has no effect on the page table size.,"It reduces internal fragmentation, making the page table smaller.","It only affects the TLB reach, not the page table size.",B,"Decreasing page size increases the number of pages required for a given virtual memory space, which in turn increases the size of the page table."
Which page size generally leads to better memory utilization and minimizes internal fragmentation?,Larger page sizes.,Smaller page sizes.,Page sizes that are multiples of 1024 bytes.,Page sizes equal to the working set size.,Page sizes that are prime numbers.,B,Memory utilization is better with smaller pages because the average waste due to internal fragmentation (part of the final page allocated but unused) is minimized.
"Considering the components of I/O time (seek, latency, transfer), which page size is generally argued to minimize the *total* I/O time for a given amount of data?","Extremely small page sizes (e.g., 1 byte) to minimize transfer time.","Moderate page sizes (e.g., 4KB) for a balance.","Larger page sizes, because seek and latency times often dwarf transfer time, so fewer I/O operations are better.",Page sizes that are not powers of 2.,Variable page sizes managed by the application.,C,"Although transfer time is proportional to page size, seek and latency times often dwarf it. Therefore, reading a larger page size in a single I/O operation (rather than multiple smaller ones) generally leads to less total I/O time."
"From the perspective of locality and resolution, why are smaller page sizes considered advantageous?","They lead to higher internal fragmentation, which improves resolution.","They allow for a larger TLB reach, improving locality.","They reduce total I/O and improve locality by matching program locality more accurately, isolating only memory actually needed.",They ensure that all pages of a process fit into physical memory.,"They increase the number of page faults, thereby forcing better locality.",C,"Smaller page sizes improve locality and resolution because each page matches program locality more accurately, isolating only the memory actually needed, and reducing total I/O."
Which page size generally helps in minimizing the number of page faults for a process?,"Smaller page sizes, as they allow for finer-grained memory management.","Larger page sizes, as more data can be brought in with each single page fault.",Page sizes that are a multiple of the cache line size.,Page sizes that are dynamically adjusted based on process behavior.,Page sizes determined by the operating system at runtime.,B,"Each page fault incurs significant overhead. Larger page sizes mean that a single page fault brings in more data, thereby reducing the total number of page faults for a given working set."
What has been the historical trend regarding page sizes in computing systems?,A trend towards smaller page sizes to reduce internal fragmentation.,"A trend towards variable page sizes, with no single optimal size.","A trend towards larger page sizes, even for mobile systems.",A trend towards eliminating paging in favor of larger physical memory.,A trend towards fixed 4KB page sizes across all systems.,C,"The text states, 'Historical trend: toward larger page sizes, even for mobile systems.'"
What is 'TLB reach'?,The maximum number of entries a TLB can hold.,The amount of memory accessible by the Translation Look-aside Buffer (TLB).,The percentage of virtual address translations that miss the TLB.,The speed at which the TLB can perform lookups.,The total physical memory available on a system.,B,TLB reach is defined as 'the amount of memory accessible from the TLB.'
How is TLB reach calculated?,Number of TLB entries divided by page size.,Total virtual memory size minus physical memory size.,Number of TLB entries multiplied by page size.,Hit ratio multiplied by the number of TLB entries.,The sum of all active process working sets.,C,TLB reach is calculated as 'number of entries × page size.'
"Besides increasing the number of TLB entries, what is another primary approach mentioned to increase TLB reach?",Decreasing the overall page table size.,Increasing the amount of physical RAM.,Reducing the CPU clock speed.,Increasing the page size or providing multiple page sizes.,Implementing a software-managed TLB.,D,"The text states, 'Another approach: increase page size or provide multiple page sizes.'"
"In ARM v8 architecture, what is the purpose of the ""contiguous bit"" in a TLB entry?",It indicates if the TLB entry has been modified recently.,It marks the TLB entry as invalid and ready for replacement.,It signifies that the entry maps a block of memory that is physically contiguous.,It determines if the page is read-only or writable.,It ensures that the TLB entry is always in the cache.,C,"The text states, 'Contiguous bit set: entry maps contiguous (adjacent) blocks of memory.'"
"What is a potential downside of increasing page size to improve TLB reach, especially for some applications?",Decreased number of page faults.,Reduced I/O overhead.,Increased internal fragmentation.,Slower TLB lookup times.,Higher cache hit ratio.,C,"The text mentions, 'Downside of larger page size: increased fragmentation for some applications.'"
What is the main purpose of using inverted page tables?,To improve the TLB hit ratio.,To reduce the amount of physical memory needed for virtual-to-physical address translations.,To accelerate page fault handling.,To allow for dynamic adjustment of page sizes.,To prevent internal fragmentation.,B,The primary purpose of inverted page tables is 'to reduce physical memory needed for virtual-to-physical address translations.'
How is an inverted page table structured?,"One entry per virtual page, indexed by process ID.","One entry per physical memory frame, indexed by <process-id, page-number>.","One entry per process, listing all its virtual pages.",A single global table indexed by virtual address.,A table stored entirely in the TLB.,B,"The method is 'one entry per page of physical memory, indexed by <process-id, page-number>.'"
"What is a significant downside of inverted page tables, and what is the typical solution for it?",They increase TLB lookup time; solved by larger TLB.,They increase internal fragmentation; solved by smaller page sizes.,They no longer contain complete info about a process's logical address space; solved by keeping external page tables.,They are prone to deadlock; solved by a dedicated lockout mechanism.,They require more physical memory; solved by using smaller entries.,C,"A downside is that they 'no longer contains complete info about logical address space of a process,' which is problematic for demand paging. The solution is to keep 'external page table (one per process).'"
"What special consideration is required for kernel handling when using inverted page tables, especially during a page fault?",Page faults can never occur with inverted page tables.,The kernel must immediately restart the faulting process without delay.,A page fault may cause another page fault when paging in the external page table.,The external page table must always be locked in memory.,Inverted page tables eliminate the need for page fault handling.,C,"A special case is mentioned: 'page fault may cause another page fault (paging in external page table),' requiring careful kernel handling."
"While demand paging is designed to be transparent to the user program, when is system performance improved regarding program structure?",When the user program intentionally introduces more page faults.,When the user and compiler are aware of demand paging and optimize for it.,When all data is stored contiguously in memory.,When the operating system uses a purely random page replacement policy.,When the TLB is completely disabled.,B,System performance is 'improved if user/compiler aware of demand paging.'
"In the example of initializing a 128x128 array with 128-word pages, which access order significantly reduces the number of page faults?","Row major order (data[i][j] with outer loop 'j', inner loop 'i').","Column major order (data[i][j] with outer loop 'i', inner loop 'j').",Random access pattern.,Diagonal access pattern.,Concurrent access by multiple threads.,B,"Column major order 'reduces page faults to 128' compared to 16,384 for row major order, by improving locality."
Which of the following data structures is cited as an example of having good locality of reference?,Hash table.,Linked list.,Stack.,Binary tree.,Heap.,C,The text states: 'Good locality: stack (access always to top).'
How can compilers and loaders contribute to better paging performance?,By placing frequently calling routines across page boundaries.,By marking all code pages as writable.,By separating code and data and packing frequently calling routines into the same page.,By increasing the overall number of pages used by a program.,By always using the smallest possible page size.,C,"Compilers and loaders can improve performance by 'separating code and data, reentrant code' and 'Pack frequently calling routines into same page' to enhance locality."
What is the main problem that I/O interlock and page locking aim to solve in demand paging?,Preventing a process from exceeding its allocated physical memory.,Ensuring that the TLB always has the correct translation for I/O buffers.,"Preventing pages containing I/O buffers from being paged out while I/O is in progress, leading to incorrect I/O.",Reducing the overhead of page table lookups during I/O operations.,Accelerating the transfer of data between the CPU and I/O devices.,C,"The problem scenario describes how an I/O buffer's page can be paged out by other processes (due to global replacement), causing I/O to occur to a frame now used for a different page. Locking prevents this."
What is the most common solution mentioned to prevent I/O buffers from being paged out during an I/O operation?,Copying data between system memory and user memory for every I/O operation.,Disabling page faults globally during I/O.,"Associating a 'lock bit' with every frame, preventing a locked frame from being selected for replacement.",Increasing the priority of the I/O-performing process to prevent preemption.,Using a dedicated I/O TLB that is never flushed.,C,"The text presents 'Allow pages to be locked into memory: lock bit associated with every frame' as a solution, explicitly stating that a 'Locked frame: cannot be selected for replacement.'"
"What is the term used when user processes, like database applications, request to lock pages into memory?",Swapping.,Flushing.,Pinning.,Caching.,Throttling.,C,The text states: 'User processes: may need to lock pages (pinning).'
How can a lock bit be used in the context of normal page replacement to improve performance or fairness for newly brought-in pages?,It prevents a low-priority process from ever experiencing a page fault.,It ensures that newly brought-in pages are immediately written to disk.,It allows a newly brought-in page to be protected from replacement until the faulting process has been dispatched again.,It forces a high-priority process to wait until all newly brought-in pages are used.,It automatically unlocks all pages after a fixed time interval.,C,The text describes this policy decision: 'Preventing replacement of newly brought-in page until used once: use lock bit. Page selected for replacement: lock bit on. Remains on until faulting process dispatched again.'
What is a potential danger or risk associated with the use of lock bits for pages?,It always leads to higher internal fragmentation.,"The lock bit may get turned on but never off, making the frame unusable.",It significantly increases the page table size.,It reduces the overall system throughput.,It makes it impossible to distinguish between clean and dirty pages.,B,"The text warns, 'Danger of lock bit: may get turned on but never off (bug). Locked frame becomes unusable.'"
"In the context of a cache, such as a TLB, what does ""hit ratio"" refer to?",The total number of successful memory accesses.,The percentage of virtual address translations resolved in the cache.,The speed at which data can be retrieved from the cache.,The amount of memory that can be stored in the cache.,The frequency of cache updates.,B,"'Hit ratio' is defined as the 'Percentage of times a cache provides a valid lookup (e.g., TLB effectiveness).'"
"What is the primary purpose of ""huge pages"" in modern operating systems like Linux?",To provide smaller page sizes for applications with high internal fragmentation.,"To designate a region of physical memory for especially large pages, often to improve TLB reach.",To reduce the total physical memory required for page tables.,To enable more efficient I/O operations by always locking pages.,To allow for pages to be encrypted transparently.,B,'Huge pages' are defined as a 'Feature designating a region of physical memory for especially large pages' and are mentioned in the context of increasing TLB reach.
