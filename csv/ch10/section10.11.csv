Question,Option A,Option B,Option C,Option D,Option E,Answer,Explanation
Which of the following best describes virtual memory?,A contiguous block of physical RAM reserved for the operating system.,A technique to increase CPU clock speed by reducing memory access times.,An abstraction of physical memory into an extremely large uniform array of storage.,A specialized cache for frequently accessed data that is faster than RAM.,A method to manage network bandwidth and optimize data transfer.,C,"Virtual memory abstracts physical memory into an extremely large uniform array of storage, making it appear larger and uniform to processes."
One significant benefit of virtual memory is that it allows a program to:,Execute faster than if it were entirely in physical memory due to caching.,Be larger than the available physical memory.,Directly access hardware registers without operating system intervention.,Avoid the need for any form of secondary storage like hard drives.,Communicate directly with other processes using shared CPU registers.,B,Virtual memory enables programs to be larger than physical memory by loading only parts of the program into RAM as needed.
Which of the following is NOT listed as a direct benefit of virtual memory?,Programs do not need to be entirely in memory to execute.,Processes can share memory regions more efficiently.,Processes can be created more efficiently.,It eliminates the need for any form of memory swapping or paging to disk.,It allows a program to be larger than the physical memory available.,D,"The text states that virtual memory enables programs to be larger than physical memory, not entirely in memory, allows memory sharing, and makes process creation more efficient. It does not eliminate swapping; in fact, it relies on it."
What is the primary characteristic of 'demand paging'?,All pages of a program are loaded into memory at program start.,Pages are preloaded into memory based on anticipated future access patterns.,Pages are loaded into memory only when they are referenced or 'demanded' during program execution.,Pages are compressed before being loaded into memory to save space.,Pages are always written back to the backing store immediately after modification.,C,Demand paging ensures that pages are loaded into physical memory only when they are actively needed ('demanded') during program execution.
"Based on the concept of demand paging, what happens to pages that are never 'demanded' during program execution?",They are loaded into a special cache for later use.,They are immediately written to the backing store to free up space.,They are never loaded into physical memory.,They are marked for prefetching in a subsequent execution.,They are compressed and stored in a kernel-only region of memory.,C,A direct consequence of demand paging is that pages that are never referenced are never loaded into physical memory.
A 'page fault' occurs when:,A page is successfully written from physical memory to the backing store.,An attempt is made to access a page that is not currently present in physical memory.,Two processes try to write to the same memory page simultaneously.,The operating system detects an unrecoverable error in a page's data.,A page is successfully moved from the CPU cache to main memory.,B,A page fault is triggered when a process attempts to access a virtual memory page that is not currently loaded into physical memory.
What action typically follows a page fault?,The operating system terminates the process immediately.,The process is paused indefinitely until more physical memory becomes available.,The requested page must be brought from the backing store into an available page frame in physical memory.,The system initiates a full memory scan to diagnose the cause of the fault.,The page is marked as 'read-only' to prevent further issues.,C,"Upon a page fault, the necessary page must be fetched from secondary storage (backing store) and loaded into an empty page frame in RAM."
What is the primary characteristic of 'copy-on-write' in the context of process creation?,All pages are copied from the parent to the child process at the moment of creation.,"The child process receives a read-only copy of the parent's memory, which is never modified.","The child process initially shares the same address space as the parent, with copies made only upon modification.","Both parent and child processes immediately get independent, fully duplicated memory spaces.",Memory is copied only when the parent process terminates or exits.,C,Copy-on-write means that a child process initially shares the parent's memory pages. A copy of a page is only created if either the parent or child attempts to modify it.
"Under a copy-on-write mechanism, when is a copy of a shared page made?",Only when the child process is initially created.,When either the child or parent process attempts to modify the shared page.,Only when the parent process modifies the page.,Only when the child process modifies the page.,When the system memory becomes critically low.,B,"A copy of a page is made if either the child process or the parent process modifies that page, ensuring isolation without immediate full duplication."
A page-replacement algorithm is typically invoked when:,A new process is created and needs its initial memory allocation.,Available physical memory is low and a new page needs to be loaded from the backing store.,A process voluntarily releases its allocated memory regions.,The CPU cache becomes full and needs to be flushed.,The system is booting up and initializing its memory structures.,B,"When available memory is low, a page-replacement algorithm is used to select an existing page to be evicted from memory to make room for a new one."
Which of the following is explicitly listed as a page-replacement algorithm in the provided text?,LFU (Least Frequently Used),MRU (Most Recently Used),Optimal,Random,Segmented FIFO,C,"The text lists FIFO, optimal, and LRU as page-replacement algorithms."
Why is a 'pure LRU' page-replacement algorithm often impractical to implement in real systems?,It requires too much CPU time for calculations on every memory access.,It necessitates complex hardware support to accurately track the exact usage time for every page.,It inherently leads to an excessive number of page faults compared to other algorithms.,It is difficult to define 'least recently used' precisely across all processes.,It cannot be combined with other memory management techniques like demand paging.,B,"The text states that pure LRU is impractical to implement, implying the difficulty in accurately tracking precise usage times, leading most systems to use LRU-approximation algorithms."
A 'global page-replacement algorithm' selects a page for replacement from:,Only the pages belonging to the faulting process.,"A fixed set of system-wide shared pages, ignoring process-specific ones.",Any process currently loaded in the system's memory.,Only pages that have not been modified since being loaded.,Pages belonging to processes with the lowest priority or least activity.,C,"Global page-replacement algorithms are defined as selecting a page from any process for replacement, not just the faulting one."
"In contrast to a global algorithm, a 'local page-replacement algorithm' selects a page for replacement from:",Only pages that are currently in the CPU cache.,Only pages belonging to the faulting process.,"Any page in memory, regardless of its owning process.",Pages that are least frequently accessed system-wide.,"Pages within a specific, pre-defined memory region reserved for the OS.",B,Local page-replacement algorithms are defined as selecting a page from the faulting process itself.
'Thrashing' refers to a state where a system:,"Spends an excessive amount of time executing user-mode processes, neglecting system tasks.","Is unable to allocate any more physical memory, leading to system crash.",Spends more time paging (swapping pages in and out of memory) than executing useful work.,"Experiences frequent CPU context switches, but without significant performance impact.","Has its entire working set loaded into memory, leading to optimal performance.",C,"Thrashing is a severe performance degradation where the system is spending a disproportionate amount of time moving pages between RAM and disk, rather than performing actual computation."
"In the context of virtual memory, what does 'locality' refer to?",The physical location of memory modules on the motherboard.,A set of pages that are actively used together by a process.,The geographical location of data centers for distributed memory systems.,The closest available page frame for allocation when a page fault occurs.,A measure of how frequently a specific memory address is accessed.,B,"Locality is defined as a set of pages actively used together, often referring to spatial or temporal locality of reference."
How does process execution typically relate to 'locality'?,Processes remain strictly within a single locality throughout their entire execution.,Processes continuously move randomly across all available memory pages without structure.,Process execution typically moves from one locality to another as different functions or data are accessed.,"Locality only applies to kernel processes, not user-mode processes.",Locality prevents processes from ever needing to page to disk.,C,"The text states that process execution moves from locality to locality, reflecting changing working sets over time."
The 'working set' of a process is defined as:,The total number of pages allocated to a process since its creation.,"The set of pages currently in active use by a process, based on the concept of locality.","The set of all executable code pages belonging to a process, excluding data pages.",The maximum number of pages a process is allowed to hold in memory at any given time.,The set of pages that have been recently written to disk as part of a checkpointing process.,B,The working set is based on locality and represents the set of pages currently in use by a process.
What is 'memory compression' in the context of virtual memory?,A technique that reduces the physical size of RAM modules for compact devices.,A method of compressing a number of individual pages into a single page frame in memory.,An algorithm designed to increase the speed of memory access by reducing latency.,A way to store data redundantly across multiple pages for fault tolerance.,A security measure to encrypt memory contents to prevent unauthorized access.,B,Memory compression involves compressing multiple logical pages of data into a single physical page frame.
"Memory compression is noted as an alternative to paging, primarily used on which type of systems?",High-performance computing clusters that require extreme speed.,Enterprise server systems managing large databases.,Mobile systems that often lack full paging support.,Desktop workstations with large amounts of RAM.,Virtual machines running in cloud environments.,C,"Memory compression is used as an alternative to paging, particularly on mobile systems without full paging support."
How is kernel memory allocation described as being different from user-mode process allocation?,"Kernel memory is allocated in non-contiguous chunks, unlike user memory.","Kernel memory is allocated dynamically based on demand, while user memory is always pre-allocated.",Kernel memory is allocated in contiguous chunks of varying sizes.,"Kernel memory uses a FIFO allocation scheme, while user memory uses LRU.",Kernel memory is always much larger in total size than user memory.,C,"Kernel memory is allocated differently than user-mode processes, specifically in contiguous chunks of varying sizes."
Which of the following are mentioned as common techniques for kernel memory allocation?,LIFO and LRU.,Paging and Swapping.,Buddy system and Slab allocation.,First-fit and Best-fit.,Segmented memory and Pure demand paging.,C,The text explicitly lists the Buddy system and Slab allocation as common techniques for kernel memory allocation.
What does 'TLB reach' refer to in the context of memory management?,The physical distance between the CPU and main memory.,The speed at which the Translation Lookaside Buffer (TLB) can translate virtual addresses.,The total amount of memory that can be directly accessed and mapped by the TLB at any given time.,The maximum number of entries the TLB can hold.,The number of active processes that can simultaneously utilize the TLB.,C,TLB reach is defined as the amount of memory accessible from the Translation Lookaside Buffer (TLB).
How is TLB reach calculated?,Number of TLB entries divided by the system's smallest page size.,Total physical memory size multiplied by the system's largest page size.,Number of entries in the TLB multiplied by the page size.,CPU clock speed multiplied by the TLB access time.,Number of active processes multiplied by the number of TLB entries.,C,TLB reach is calculated as the number of entries in the TLB multiplied by the page size.
A common technique to increase TLB reach is to:,Decrease the number of entries in the TLB.,Use smaller page sizes for memory allocation.,Increase the page size used by the system.,Allocate significantly more physical memory (RAM).,Increase the frequency of TLB flushes.,C,The text states that a technique to increase TLB reach is to increase the page size.
"Which of the following virtual memory management techniques are commonly used by Linux, Windows, and Solaris?","Pure LRU, LIFO, and segmented memory allocation.","Demand paging, copy-on-write, and variations of LRU approximation (like the clock algorithm).","Buddy system, Slab allocation, and optimal page replacement.","Memory compression, pre-paging, and exact LRU implementations.","Strictly global page replacement, local page replacement, and pure FIFO.",B,"Linux, Windows, and Solaris are noted to manage virtual memory similarly, using demand paging, copy-on-write, and variations of LRU approximation (e.g., clock algorithm)."
