Question,Option A,Option B,Option C,Option D,Option E,Answer,Explanation
"What is the primary purpose of ""prepaging"" in memory management?",To reduce the overall number of physical memory pages used by a process.,To prevent a high number of initial page faults when a process starts.,To improve the hit ratio of the Translation Look-aside Buffer (TLB).,To allow pages to be locked in memory during I/O operations.,To ensure that all pages of a process are always present in memory.,B,Prepaging is an attempt to prevent the high number of initial page faults that occur due to initial locality when a process starts.
Which strategy describes how prepaging attempts to achieve its goal?,By swapping out less frequently used pages to disk.,By bringing some or all potentially needed pages into memory at once.,By increasing the size of the TLB to reduce lookup times.,By reordering process instructions to improve data locality.,By dynamically adjusting the page replacement algorithm.,B,"The strategy for prepaging is to bring some or all needed pages into memory at once, anticipating their use."
How does the working-set model relate to prepaging when a suspended process resumes?,"The working set is ignored, and all pages are brought in on demand.",Only the modified pages from the working set are brought back.,The entire working set is automatically brought back into memory before restarting the process.,The working set is copied to disk to free up memory for other processes.,"Prepaging is only applied to the code segment, not the working set.",C,An example of prepaging is remembering the working set for a suspended process and automatically bringing the entire working set back before restarting it.
"In the cost analysis of prepaging, if 's' pages are prepaged and 'alpha' is the fraction of those pages actually used, what scenario would cause prepaging to be disadvantageous?",Alpha is approximately 1.,The cost of 's * (1 - alpha)' unnecessary pages is much higher than 's * alpha' saved page faults.,The cost of 's * alpha' saved page faults is much higher than 's * (1 - alpha)' unnecessary pages.,The total number of prepaged pages 's' is very small.,The system has an infinite supply of free memory frames.,B,"Prepaging loses if alpha is approximately 0, meaning many prepaged pages are not used. This makes the cost of unnecessary pages outweigh the benefit of saved page faults."
For which type of data is prepaging generally more predictable and effective?,Dynamically linked libraries.,Executable programs.,Data structures with poor locality.,Files accessed sequentially.,Encrypted data streams.,D,"Prepaging files is more predictable because they are often accessed sequentially, making it easier to anticipate which pages will be needed."
Which Linux system call is explicitly mentioned as a mechanism for prefetching file contents into memory?,mmap(),sync(),readahead(),mlock(),fork(),C,The text states that the Linux `readahead()` system call prefetches file contents into memory.
Page sizes in new machine designs are invariably chosen to be what type of number?,Prime numbers.,Multiples of 100.,Powers of 2.,Numbers divisible by 3.,Arbitrary integers.,C,The text states that page sizes are 'invariably powers of 2'.
How does decreasing the page size affect the size of the page table?,"It decreases the number of pages, thus decreasing the page table size.","It increases the number of pages, thus increasing the page table size.",It has no effect on the page table size.,"It reduces internal fragmentation, making the page table smaller.","It only affects the TLB reach, not the page table size.",B,"Decreasing page size increases the number of pages required for a given virtual memory space, which in turn increases the size of the page table."
Which page size generally leads to better memory utilization and minimizes internal fragmentation?,Larger page sizes.,Smaller page sizes.,Page sizes that are multiples of 1024 bytes.,Page sizes equal to the working set size.,Page sizes that are prime numbers.,B,Memory utilization is better with smaller pages because the average waste due to internal fragmentation (part of the final page allocated but unused) is minimized.
"Considering the components of I/O time (seek, latency, transfer), which page size is generally argued to minimize the *total* I/O time for a given amount of data?","Extremely small page sizes (e.g., 1 byte) to minimize transfer time.","Moderate page sizes (e.g., 4KB) for a balance.","Larger page sizes, because seek and latency times often dwarf transfer time, so fewer I/O operations are better.",Page sizes that are not powers of 2.,Variable page sizes managed by the application.,C,"Although transfer time is proportional to page size, seek and latency times often dwarf it. Therefore, reading a larger page size in a single I/O operation (rather than multiple smaller ones) generally leads to less total I/O time."
"From the perspective of locality and resolution, why are smaller page sizes considered advantageous?","They lead to higher internal fragmentation, which improves resolution.","They allow for a larger TLB reach, improving locality.","They reduce total I/O and improve locality by matching program locality more accurately, isolating only memory actually needed.",They ensure that all pages of a process fit into physical memory.,"They increase the number of page faults, thereby forcing better locality.",C,"Smaller page sizes improve locality and resolution because each page matches program locality more accurately, isolating only the memory actually needed, and reducing total I/O."
Which page size generally helps in minimizing the number of page faults for a process?,"Smaller page sizes, as they allow for finer-grained memory management.","Larger page sizes, as more data can be brought in with each single page fault.",Page sizes that are a multiple of the cache line size.,Page sizes that are dynamically adjusted based on process behavior.,Page sizes determined by the operating system at runtime.,B,"Each page fault incurs significant overhead. Larger page sizes mean that a single page fault brings in more data, thereby reducing the total number of page faults for a given working set."
What has been the historical trend regarding page sizes in computing systems?,A trend towards smaller page sizes to reduce internal fragmentation.,"A trend towards variable page sizes, with no single optimal size.","A trend towards larger page sizes, even for mobile systems.",A trend towards eliminating paging in favor of larger physical memory.,A trend towards fixed 4KB page sizes across all systems.,C,"The text states, 'Historical trend: toward larger page sizes, even for mobile systems.'"
What is 'TLB reach'?,The maximum number of entries a TLB can hold.,The amount of memory accessible by the Translation Look-aside Buffer (TLB).,The percentage of virtual address translations that miss the TLB.,The speed at which the TLB can perform lookups.,The total physical memory available on a system.,B,TLB reach is defined as 'the amount of memory accessible from the TLB.'
How is TLB reach calculated?,Number of TLB entries divided by page size.,Total virtual memory size minus physical memory size.,Number of TLB entries multiplied by page size.,Hit ratio multiplied by the number of TLB entries.,The sum of all active process working sets.,C,TLB reach is calculated as 'number of entries Ã— page size.'
"Besides increasing the number of TLB entries, what is another primary approach mentioned to increase TLB reach?",Decreasing the overall page table size.,Increasing the amount of physical RAM.,Reducing the CPU clock speed.,Increasing the page size or providing multiple page sizes.,Implementing a software-managed TLB.,D,"The text states, 'Another approach: increase page size or provide multiple page sizes.'"
"In ARM v8 architecture, what is the purpose of the ""contiguous bit"" in a TLB entry?",It indicates if the TLB entry has been modified recently.,It marks the TLB entry as invalid and ready for replacement.,It signifies that the entry maps a block of memory that is physically contiguous.,It determines if the page is read-only or writable.,It ensures that the TLB entry is always in the cache.,C,"The text states, 'Contiguous bit set: entry maps contiguous (adjacent) blocks of memory.'"
"What is a potential downside of increasing page size to improve TLB reach, especially for some applications?",Decreased number of page faults.,Reduced I/O overhead.,Increased internal fragmentation.,Slower TLB lookup times.,Higher cache hit ratio.,C,"The text mentions, 'Downside of larger page size: increased fragmentation for some applications.'"
What is the main purpose of using inverted page tables?,To improve the TLB hit ratio.,To reduce the amount of physical memory needed for virtual-to-physical address translations.,To accelerate page fault handling.,To allow for dynamic adjustment of page sizes.,To prevent internal fragmentation.,B,The primary purpose of inverted page tables is 'to reduce physical memory needed for virtual-to-physical address translations.'
How is an inverted page table structured?,"One entry per virtual page, indexed by process ID.","One entry per physical memory frame, indexed by <process-id, page-number>.","One entry per process, listing all its virtual pages.",A single global table indexed by virtual address.,A table stored entirely in the TLB.,B,"The method is 'one entry per page of physical memory, indexed by <process-id, page-number>.'"
"What is a significant downside of inverted page tables, and what is the typical solution for it?",They increase TLB lookup time; solved by larger TLB.,They increase internal fragmentation; solved by smaller page sizes.,They no longer contain complete info about a process's logical address space; solved by keeping external page tables.,They are prone to deadlock; solved by a dedicated lockout mechanism.,They require more physical memory; solved by using smaller entries.,C,"A downside is that they 'no longer contains complete info about logical address space of a process,' which is problematic for demand paging. The solution is to keep 'external page table (one per process).'"
"What special consideration is required for kernel handling when using inverted page tables, especially during a page fault?",Page faults can never occur with inverted page tables.,The kernel must immediately restart the faulting process without delay.,A page fault may cause another page fault when paging in the external page table.,The external page table must always be locked in memory.,Inverted page tables eliminate the need for page fault handling.,C,"A special case is mentioned: 'page fault may cause another page fault (paging in external page table),' requiring careful kernel handling."
"While demand paging is designed to be transparent to the user program, when is system performance improved regarding program structure?",When the user program intentionally introduces more page faults.,When the user and compiler are aware of demand paging and optimize for it.,When all data is stored contiguously in memory.,When the operating system uses a purely random page replacement policy.,When the TLB is completely disabled.,B,System performance is 'improved if user/compiler aware of demand paging.'
"In the example of initializing a 128x128 array with 128-word pages, which access order significantly reduces the number of page faults?","Row major order (data[i][j] with outer loop 'j', inner loop 'i').","Column major order (data[i][j] with outer loop 'i', inner loop 'j').",Random access pattern.,Diagonal access pattern.,Concurrent access by multiple threads.,B,"Column major order 'reduces page faults to 128' compared to 16,384 for row major order, by improving locality."
Which of the following data structures is cited as an example of having good locality of reference?,Hash table.,Linked list.,Stack.,Binary tree.,Heap.,C,The text states: 'Good locality: stack (access always to top).'
How can compilers and loaders contribute to better paging performance?,By placing frequently calling routines across page boundaries.,By marking all code pages as writable.,By separating code and data and packing frequently calling routines into the same page.,By increasing the overall number of pages used by a program.,By always using the smallest possible page size.,C,"Compilers and loaders can improve performance by 'separating code and data, reentrant code' and 'Pack frequently calling routines into same page' to enhance locality."
What is the main problem that I/O interlock and page locking aim to solve in demand paging?,Preventing a process from exceeding its allocated physical memory.,Ensuring that the TLB always has the correct translation for I/O buffers.,"Preventing pages containing I/O buffers from being paged out while I/O is in progress, leading to incorrect I/O.",Reducing the overhead of page table lookups during I/O operations.,Accelerating the transfer of data between the CPU and I/O devices.,C,"The problem scenario describes how an I/O buffer's page can be paged out by other processes (due to global replacement), causing I/O to occur to a frame now used for a different page. Locking prevents this."
What is the most common solution mentioned to prevent I/O buffers from being paged out during an I/O operation?,Copying data between system memory and user memory for every I/O operation.,Disabling page faults globally during I/O.,"Associating a 'lock bit' with every frame, preventing a locked frame from being selected for replacement.",Increasing the priority of the I/O-performing process to prevent preemption.,Using a dedicated I/O TLB that is never flushed.,C,"The text presents 'Allow pages to be locked into memory: lock bit associated with every frame' as a solution, explicitly stating that a 'Locked frame: cannot be selected for replacement.'"
"What is the term used when user processes, like database applications, request to lock pages into memory?",Swapping.,Flushing.,Pinning.,Caching.,Throttling.,C,The text states: 'User processes: may need to lock pages (pinning).'
How can a lock bit be used in the context of normal page replacement to improve performance or fairness for newly brought-in pages?,It prevents a low-priority process from ever experiencing a page fault.,It ensures that newly brought-in pages are immediately written to disk.,It allows a newly brought-in page to be protected from replacement until the faulting process has been dispatched again.,It forces a high-priority process to wait until all newly brought-in pages are used.,It automatically unlocks all pages after a fixed time interval.,C,The text describes this policy decision: 'Preventing replacement of newly brought-in page until used once: use lock bit. Page selected for replacement: lock bit on. Remains on until faulting process dispatched again.'
What is a potential danger or risk associated with the use of lock bits for pages?,It always leads to higher internal fragmentation.,"The lock bit may get turned on but never off, making the frame unusable.",It significantly increases the page table size.,It reduces the overall system throughput.,It makes it impossible to distinguish between clean and dirty pages.,B,"The text warns, 'Danger of lock bit: may get turned on but never off (bug). Locked frame becomes unusable.'"
"In the context of a cache, such as a TLB, what does ""hit ratio"" refer to?",The total number of successful memory accesses.,The percentage of virtual address translations resolved in the cache.,The speed at which data can be retrieved from the cache.,The amount of memory that can be stored in the cache.,The frequency of cache updates.,B,"'Hit ratio' is defined as the 'Percentage of times a cache provides a valid lookup (e.g., TLB effectiveness).'"
"What is the primary purpose of ""huge pages"" in modern operating systems like Linux?",To provide smaller page sizes for applications with high internal fragmentation.,"To designate a region of physical memory for especially large pages, often to improve TLB reach.",To reduce the total physical memory required for page tables.,To enable more efficient I/O operations by always locking pages.,To allow for pages to be encrypted transparently.,B,'Huge pages' are defined as a 'Feature designating a region of physical memory for especially large pages' and are mentioned in the context of increasing TLB reach.
