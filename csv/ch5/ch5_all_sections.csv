What is the primary goal of CPU scheduling in multiprogrammed operating systems?,To minimize the number of processes in the ready queue.,To decrease the overhead of context switching.,To maximize CPU utilization.,To ensure processes complete their execution as quickly as possible.,To prioritize I/O operations over CPU execution.,C,"CPU scheduling is fundamental to multiprogrammed operating systems, with the primary goal of maximizing CPU utilization by switching the CPU among processes."
"On modern operating systems, which entities are typically scheduled by the CPU scheduler?",User-level processes.,Kernel-level threads.,Applications running in user space.,Memory pages for virtual memory.,I/O requests in the device queue.,B,"On modern operating systems, kernel-level threads are scheduled, although 'process scheduling' and 'thread scheduling' are often used interchangeably."
"In the context of process execution, what does the general terminology of scheduling a process to 'run on a CPU' imply?",The process is loaded into main memory.,The process is allocated a segment of virtual memory.,The process is running on a CPU's core.,The process is waiting for an I/O operation to complete.,The process has completed its execution.,C,A process executes on a CPU's core; general terminology of scheduling a process to 'run on a CPU' implies running on a core.
Which of the following best describes the CPU-I/O burst cycle?,A continuous period of CPU execution without any I/O operations.,"A loop of CPU execution followed by I/O wait, alternating between CPU burst and I/O burst.",A process state where only I/O operations are performed.,The time it takes for a process to switch from user mode to kernel mode.,The mechanism by which the operating system decides which process to run next.,B,"Process execution consists of a cycle of CPU execution and I/O wait, alternating between CPU burst and I/O burst."
What is characteristic of the duration of CPU bursts?,They are typically of fixed length for all processes.,"They tend to have a uniform distribution, with all lengths equally likely.","They vary and tend to have an exponential or hyperexponential frequency curve, meaning many short bursts and few long bursts.",They increase linearly with the complexity of the program.,They are always longer for I/O-bound programs than for CPU-bound programs.,C,"Durations of CPU bursts vary but tend to have an exponential or hyperexponential frequency curve (many short, few long bursts)."
How do I/O-bound programs and CPU-bound programs typically differ in their CPU burst characteristics?,"I/O-bound programs have a few long CPU bursts, while CPU-bound programs have many short CPU bursts.",Both I/O-bound and CPU-bound programs have an equal number of short and long CPU bursts.,"I/O-bound programs have many short CPU bursts, while CPU-bound programs have a few long CPU bursts.","I/O-bound programs only perform I/O bursts, and CPU-bound programs only perform CPU bursts.","CPU-bound programs primarily wait for I/O, whereas I/O-bound programs use the CPU extensively.",C,I/O-bound programs have many short CPU bursts; CPU-bound programs have a few long CPU bursts. This distribution is important for CPU-scheduling algorithms.
What is the primary responsibility of the CPU scheduler?,To manage the allocation of memory to processes.,To perform I/O operations for waiting processes.,To select a process from the ready queue to execute when the CPU becomes idle and allocate the CPU to it.,To handle system calls and interrupts from hardware devices.,To terminate processes that have completed their execution.,C,"When the CPU becomes idle, the operating system selects a process from the ready queue to execute. The CPU scheduler performs this selection and allocates the CPU to the chosen process."
Which of the following is true regarding the structure of the ready queue?,"It is always implemented as a strictly First-In, First-Out (FIFO) queue.",It must be an unordered linked list to ensure fairness.,"It can be a FIFO queue, priority queue, tree, or unordered linked list.",Its structure is fixed and cannot be changed by the operating system.,"It only stores process IDs, not full Process Control Blocks (PCBs).",C,"The ready queue is not necessarily FIFO; it can be a FIFO queue, priority queue, tree, or unordered linked list. Records in queues are generally process control blocks (PCBs)."
What information is typically stored as records in the ready queue?,Only the Process ID (PID) of the waiting processes.,The memory addresses of the process code segments.,The Process Control Blocks (PCBs) of the processes.,The historical CPU burst durations of the processes.,The I/O requests that processes are waiting for.,C,Records in queues are generally process control blocks (PCBs).
Which two circumstances allow for a scheduling choice to be made by the operating system?,"When a process switches from the running to the waiting state, or when it terminates.","When a process switches from the running to the ready state, or when a process switches from the waiting to the ready state.","When a process requests an I/O operation, or when it creates a child process.","When an interrupt occurs, or when the process requests a system call.","When a process is first admitted to the ready queue, or when it completes its first CPU burst.",B,"CPU-scheduling decisions occur under four circumstances. For circumstances where a process switches from running to ready (e.g., interrupt occurs) or from waiting to ready (e.g., I/O completion), choices exist for scheduling a new process. No choice is made when a process switches from running to waiting or terminates, as a new process *must* be selected."
"What happens when a process switches from the running to the waiting state (e.g., due to an I/O request) in terms of CPU scheduling decisions?",The OS defers the scheduling decision until the process completes its I/O.,A scheduling choice is made among multiple ready processes.,No scheduling choice is needed; a new process must be selected to run.,The process immediately switches back to the running state after a brief pause.,The CPU scheduler enters an idle state until the process returns from waiting.,C,"For circumstances where a process switches from running to waiting state (e.g., I/O request) or terminates, no scheduling choice is made; a new process must be selected, as the CPU has become idle due to the current process relinquishing it."
Under which type of scheduling does a CPU core keep a thread until it voluntarily releases the core by terminating or switching to the waiting state?,Preemptive scheduling.,Real-time scheduling.,Cooperative scheduling.,Round-robin scheduling.,Priority-based scheduling.,C,Nonpreemptive or cooperative scheduling means the CPU is allocated to a process until it releases it (terminates or switches to waiting state). The glossary defines 'cooperative' as 'A form of scheduling in which threads voluntarily move from the running state'.
What is a significant characteristic of preemptive CPU scheduling?,"It ensures that once a process starts executing, it completes its task without interruption.",The CPU can be taken away from a process even if it has not completed its current burst or voluntarily released the CPU.,It is primarily used in older operating systems due to its simplicity.,It avoids the need for context switching altogether.,It guarantees that no race conditions will occur with shared data.,B,Preemptive scheduling means the CPU can be taken away from a process. This is in contrast to nonpreemptive scheduling where the process holds the CPU until it releases it.
"Which type of CPU scheduling is predominantly used by most modern operating systems like Windows, macOS, and Linux?",Nonpreemptive scheduling.,Cooperative scheduling.,Batch scheduling.,Preemptive scheduling.,"First-Come, First-Served (FCFS) scheduling.",D,"Most modern OS (Windows, macOS, Linux, UNIX) use preemptive scheduling."
"What potential issue can arise when using preemptive scheduling, especially concerning shared data?",Increased I/O burst durations.,A reduction in overall CPU utilization.,"Race conditions, where shared data might become inconsistent due to preemption.",The inability to switch processes from running to the ready state.,Exclusive reliance on FIFO queues for process management.,C,"Preemptive scheduling can cause race conditions with shared data (e.g., one process updates, is preempted, second process reads inconsistent data)."
How do nonpreemptive kernels handle context switching compared to preemptive kernels?,"Nonpreemptive kernels allow context switches at any time, while preemptive kernels wait for system call completion.","Nonpreemptive kernels always use mutex locks, while preemptive kernels never do.","Nonpreemptive kernels wait for a system call completion or process block before context switching, ensuring simple kernel structure and consistent data, whereas preemptive kernels require mechanisms to prevent race conditions when accessing shared kernel data structures.","Nonpreemptive kernels are ideal for real-time computing, while preemptive kernels are not.","Both types of kernels handle context switching identically, but their scheduling algorithms differ.",C,"Nonpreemptive kernel: waits for system call completion or process block before context switch, ensuring simple kernel structure and consistent data. Preemptive kernel: requires mechanisms (e.g., mutex locks) to prevent race conditions when accessing shared kernel data structures."
What mechanism is typically used to prevent simultaneous use and data loss in sections of code affected by interrupts in an OS kernel?,Implementing a FIFO queue for all kernel operations.,Increasing dispatch latency to allow time for data synchronization.,Guarding the sections by disabling interrupts at entry and re-enabling them at exit.,Limiting the number of CPU-bound processes.,Switching to nonpreemptive scheduling for all kernel operations.,C,"Sections of code affected by interrupts must be guarded (e.g., disable interrupts at entry, reenable at exit) to prevent simultaneous use and data loss."
What is the role of the dispatcher in CPU scheduling?,It decides which process should run next from the ready queue.,It manages the memory allocation for processes.,It gives control of the CPU's core to the process selected by the CPU scheduler.,It handles I/O requests from running processes.,It tracks the duration of CPU and I/O bursts.,C,The dispatcher is a component of the CPU-scheduling function. It gives control of the CPU's core to the process selected by the CPU scheduler.
Which of the following is NOT a function of the dispatcher?,Switching context from one process to another.,Switching to user mode.,Jumping to the proper location in the user program to resume that program.,Selecting the next process to run from the ready queue.,Executing instructions of the chosen process immediately after context switch.,D,"The dispatcher's functions include switching context, switching to user mode, and jumping to the program's resume location. Selecting the next process is the role of the CPU scheduler, not the dispatcher."
Why should the dispatcher be designed to be very fast?,To minimize the time processes spend in the waiting state.,"Because it is invoked during every context switch, and its speed directly impacts system performance by affecting dispatch latency.",To ensure that I/O operations are completed quickly.,To prevent deadlocks in the system.,To reduce the number of CPU bursts for CPU-bound programs.,B,"The dispatcher should be fast, as it's invoked during every context switch. Its speed is critical as it contributes to dispatch latency."
What is 'dispatch latency'?,The time a process spends waiting in the ready queue.,The total time a process takes to complete its execution.,The time it takes for the dispatcher to stop one process and start another running.,The delay introduced by I/O operations.,The interval between two consecutive CPU bursts of the same process.,C,Dispatch latency is the time for the dispatcher to stop one process and start another.
What distinguishes a 'voluntary context switch' from a 'nonvoluntary context switch'?,"A voluntary switch occurs when a process terminates, while a nonvoluntary switch occurs when it requests I/O.","A voluntary switch is initiated by the operating system, while a nonvoluntary switch is initiated by the user.","A voluntary switch is when the process gives up the CPU (e.g., blocking for I/O), whereas a nonvoluntary switch is when the CPU is taken from the process (e.g., time slice expired or preemption).","A voluntary switch only happens in nonpreemptive systems, while a nonvoluntary switch only happens in preemptive systems.",There is no functional difference; they are just different terms for the same event.,C,"Voluntary context switch: process gives up CPU (e.g., blocking for I/O). Nonvoluntary context switch: CPU taken from process (e.g., time slice expired, preempted by higher-priority process)."
"According to the glossary, what is the definition of a 'CPU burst'?",A period when the CPU is idle.,A scheduling process state in which the CPU performs I/O.,A repeating loop in process execution.,A scheduling process state in which the process executes on CPU.,The time it takes for a dispatcher to switch contexts.,D,The glossary defines 'CPU burst' as 'Scheduling process state in which the process executes on CPU.'
Which term describes a form of scheduling where processes or threads are involuntarily moved from the running state?,Cooperative scheduling.,Nonpreemptive scheduling.,Batch scheduling.,Preemptive scheduling.,"First-Come, First-Served scheduling.",D,The glossary defines 'preemptive' as 'A form of scheduling in which processes or threads are involuntarily moved from the running state (by for example a timer signaling the kernel to allow the next thread to run).'
Which statement accurately describes a key characteristic of different CPU-scheduling algorithms?,They all aim to achieve the same optimal performance metrics for every system.,"They have varying properties, favoring certain process classes.",They are designed to equally prioritize both CPU-bound and I/O-bound processes.,"Their properties are identical, differing only in implementation details.",They always minimize CPU utilization to save power.,B,"The text states: 'Different CPU-scheduling algorithms have varying properties, favoring certain process classes.'"
What is the primary factor that determines the choice of a CPU-scheduling algorithm?,The number of concurrent users on the system.,The amount of physical memory available.,The desired characteristics for comparison.,The specific clock speed of the CPU.,The total number of processes in the system.,C,The text indicates: 'The choice of algorithm depends on the desired characteristics for comparison.'
"As a CPU-scheduling criterion, what does 'CPU utilization' specifically aim to achieve?",Minimizing the total power consumed by the CPU.,Keeping the CPU as busy as possible.,Maximizing the time the CPU spends in an idle state.,Ensuring an equal share of CPU time for all processes.,Measuring the number of context switches per second.,B,CPU utilization is defined as an effort to 'Keep the CPU as busy as possible'.
What is considered an ideal range for CPU utilization in real systems?,0-10%,10-20%,20-40%,40-90%,90-100%,D,The text states that CPU utilization is 'ideally 40-90% in real systems'.
"In the context of CPU scheduling, what does the criterion 'throughput' measure?",The total amount of data processed per unit of time.,The number of processes completed per unit time.,The rate at which processes arrive in the ready queue.,The average CPU time allocated to each process.,The number of I/O operations performed per second.,B,Throughput is defined as 'Number of processes completed per unit time'.
Which of the following accurately defines 'turnaround time' in CPU scheduling?,The time a process spends actively executing on the CPU.,The total time a process spends waiting in the ready queue.,The time from request submission until the first response is produced.,"The total time from process submission to completion, including waiting in ready queue, CPU execution, and I/O.",The time taken for a process to perform all its I/O operations.,D,"Turnaround time is 'Total time from process submission to completion (includes waiting in ready queue, CPU execution, and I/O)'."
What does 'waiting time' specifically refer to as a CPU-scheduling criterion?,The total time a process spends performing I/O operations.,The time a process waits for system resources other than the CPU.,The total time a process spends waiting in the ready queue.,The time from process submission until it begins execution.,The time elapsed between a request and its first response.,C,Waiting time is defined as 'Total time a process spends waiting in the ready queue'.
For which type of systems is 'response time' a particularly important CPU-scheduling criterion?,Batch processing systems,Embedded systems with fixed tasks,Real-time systems requiring predictable delays,Interactive systems,High-performance computing clusters,D,Response time is defined as 'Time from request submission until the first response is produced (for interactive systems)'.
"When optimizing CPU scheduling criteria, what is the desired goal for 'CPU utilization' and 'throughput'?",Minimize both.,Maximize both.,Minimize CPU utilization and maximize throughput.,Maximize CPU utilization and minimize throughput.,Maintain them at a constant average.,B,The optimization goals state: 'Maximize CPU utilization and throughput'.
"Which of the following is the optimization goal for 'turnaround time', 'waiting time', and 'response time'?",Maximize them.,Minimize them.,Keep them constant.,Ensure they are balanced with throughput.,Allow them to fluctuate for system flexibility.,B,"The optimization goals state: 'Minimize turnaround time, waiting time, and response time'."
"While optimizing CPU scheduling, when might optimizing minimum or maximum values be preferred over optimizing the average measure?",Only when the system is under low load.,"Never, average optimization is always superior.","For guaranteed service requirements, such as minimizing maximum response time.",When the CPU utilization is consistently below 50%.,Only for processes that are entirely CPU-bound.,C,"The text mentions: 'Often, the goal is to optimize the average measure, but sometimes minimum or maximum values are preferred (e.g., minimizing maximum response time for guaranteed service)'."
"For interactive systems, what specific aspect of response time is often considered more important to minimize than the average response time?",Its peak value.,Its standard deviation.,Its variance.,Its maximum value.,Its initial delay.,C,"The text states: 'For interactive systems, minimizing the variance in response time may be more important than minimizing the average'."
"According to the provided glossary, what is the specific definition of 'throughput' in the context of scheduling?",The general amount of work done over time.,The total number of processes submitted to the system.,The number of threads completed per unit time.,The average time spent by threads in the ready queue.,The rate at which the CPU can switch between different threads.,C,The glossary defines 'throughput' in scheduling as 'the number of threads completed per unit time'.
What is the primary function of CPU scheduling?,To manage memory allocation for processes.,To decide which process in the ready queue is allocated the CPU's core.,To handle I/O operations for processes.,To perform context switches between threads.,To determine the priority of system interrupts.,B,CPU scheduling involves deciding which process in the ready queue is allocated the CPU's core.
"In the context of the described CPU-scheduling algorithms, what assumption is made about the processing environment?",It involves a multiprocessor system with shared memory.,It assumes a distributed system across multiple nodes.,"It is described for a single processing core, capable of running one process at a time.",It focuses on real-time operating systems with strict deadlines.,It implies that CPU burst times are always known in advance.,C,"These algorithms are described in the context of a single processing core, capable of running one process at a time."
Which CPU-scheduling algorithm is known as the simplest and allocates the CPU to the process that requests it first?,Shortest-Job-First (SJF),Round-Robin (RR),Priority Scheduling,"First-Come, First-Served (FCFS)",Multilevel Feedback Queue,D,"First-Come, First-Served (FCFS) is the simplest CPU-scheduling algorithm, where the process that requests the CPU first is allocated the CPU first."
"How is the First-Come, First-Served (FCFS) scheduling algorithm typically implemented?",Using a priority queue where processes are sorted by their burst time.,"With a circular queue, preempting processes after a time quantum.","Using a FIFO queue, linking process PCBs to the tail and allocating CPU from the head.",By assigning processes to different queues based on their type.,"Through a stack, where the last process in is the first process out.",C,"FCFS is implemented with a FIFO queue: process PCBs are linked to the tail, and the CPU is allocated to the process at the head."
What is a common characteristic of the average waiting time under FCFS scheduling?,It is always minimal compared to other algorithms.,It is consistently short and predictable.,It is often long and can vary substantially with CPU burst times.,It improves significantly with a smaller time quantum.,It is inversely proportional to the number of processes.,C,Average waiting time under FCFS is often long and can vary substantially with CPU burst times.
"Which phenomenon describes a CPU-bound process holding the CPU, causing I/O-bound processes to wait in the ready queue, leading to lower CPU and device utilization?",Context switching overhead,Starvation,Convoy effect,Priority inversion,Deadlock,C,"The 'convoy effect' occurs when a CPU-bound process holds the CPU, causing I/O-bound processes to wait, leading to lower CPU and device utilization."
"Is First-Come, First-Served (FCFS) a preemptive or nonpreemptive scheduling algorithm?","Preemptive, meaning processes can be interrupted.","Nonpreemptive, meaning a process keeps the CPU until it terminates or requests I/O.","Both, depending on system configuration.",It is preemptive only for interactive systems.,It is nonpreemptive only for batch processes.,B,"FCFS is nonpreemptive; once allocated, a process keeps the CPU until it terminates or requests I/O."
What is a Gantt chart used for in the context of CPU scheduling?,To display the memory usage of processes over time.,"To illustrate a particular schedule, including the start and finish times of participating processes.",To show the hierarchical structure of process calls.,To graph the network latency between different system components.,To track the number of context switches per second.,B,"A Gantt chart is a bar chart that illustrates a particular schedule, including the start and finish times of each of the participating processes."
What characteristic does the Shortest-Job-First (SJF) scheduling algorithm associate with each process?,Its priority level.,The total memory required.,The length of its next CPU burst.,Its arrival time in the ready queue.,The number of I/O operations it performs.,C,The SJF scheduling algorithm associates each process with the length of its next CPU burst.
When does SJF scheduling break ties between processes with identical next CPU burst lengths?,Randomly.,Using a priority value.,By resorting to FCFS.,By choosing the process with the lowest process ID.,By choosing the process with the shortest remaining time.,C,"In SJF, the CPU is assigned to the process with the smallest next CPU burst, and FCFS breaks ties."
SJF is provably optimal for a given set of processes because it provides the minimum of what?,Number of context switches.,Total CPU utilization.,Average turnaround time.,Average waiting time.,Throughput.,D,"SJF is provably optimal, providing the minimum average waiting time for a given set of processes."
Why can't the Shortest-Job-First (SJF) algorithm be implemented directly at the CPU scheduling level?,It requires excessive computational resources.,The length of the next CPU burst is generally unknown.,It suffers from the convoy effect.,It is exclusively designed for batch systems.,It causes too many context switches.,B,"SJF cannot be implemented at the CPU scheduling level directly, as the length of the next CPU burst is unknown."
What method is commonly used to approximate the next CPU burst for SJF scheduling?,Linear regression analysis.,A simple arithmetic mean of all previous bursts.,An exponential average of previous CPU bursts.,By querying the user for an estimate.,By setting all future bursts to a fixed default value.,C,SJF can be approximated by predicting the next CPU burst using an exponential average of previous CPU bursts.
"In the exponential average formula ${	au _{n + 1}} = \alpha {t_n} + \left( {1 - \alpha } 
ight){	au _n}$, what does $t_n$ represent?",The predicted value for the next CPU burst.,The length of the $n$th CPU burst.,The average length of all CPU bursts so far.,The time quantum.,The context switch time.,B,"In the exponential average formula, $t_n$ represents the length of the $n$th CPU burst."
What is the effect of setting the parameter $\alpha$ to 0 in the exponential average formula for predicting CPU bursts?,Only the most recent CPU burst matters for prediction.,Recent history has no effect on the prediction.,It equally weights recent and past history.,The prediction will always be zero.,It causes the system to use actual burst times instead of predictions.,B,"If $\alpha = 0$, the formula becomes $	au_{n+1} = 	au_n$, meaning recent history ($t_n$) has no effect on the prediction."
What is the specific name for the preemptive version of Shortest-Job-First (SJF) scheduling?,"First-Come, First-Served (FCFS)",Round-Robin (RR),Shortest-Remaining-Time-First (SJRF),Priority with Preemption,Multilevel Feedback Queue,C,Preemptive SJF is called shortest-remaining-time-first (SJRF) scheduling. It preempts the current process if a new process has a shorter remaining CPU burst.
The Round-Robin (RR) scheduling algorithm is similar to FCFS but includes what key addition?,Priority assignment.,Memory management.,Preemption.,Multiprocessing support.,Optimized I/O handling.,C,The Round-Robin (RR) scheduling algorithm is similar to FCFS but includes preemption.
What is a 'time quantum' or 'time slice' in the context of Round-Robin scheduling?,The total time a process runs before termination.,A small unit of time for which the CPU scheduler allocates the CPU to each process.,The time it takes for a context switch to occur.,The period between a process's arrival and its completion.,The maximum allowed waiting time for a process.,B,"A small unit of time, called a time quantum or time slice (typically 10-100 milliseconds), is defined, and the CPU scheduler allocates the CPU to each process for up to 1 time quantum."
"In Round-Robin scheduling, what happens if a process's CPU burst is longer than 1 time quantum?",It continues to execute until it finishes its burst.,It is terminated immediately.,It is preempted by a timer interrupt and moved to the tail of the ready queue.,"Its priority is decreased, and it's re-evaluated later.",It voluntarily releases the CPU.,C,"If a process's CPU burst is longer than 1 time quantum, it is preempted by a timer interrupt and moved to the tail of the ready queue."
How does the performance of Round-Robin (RR) scheduling change if the time quantum is very large?,It becomes highly efficient for interactive systems.,RR degenerates to FCFS.,It leads to an increase in context switches.,Average turnaround time is guaranteed to improve.,It mimics Shortest-Job-First behavior.,B,"If the time quantum is large, RR degenerates to FCFS because processes are rarely preempted within their burst."
What is a negative consequence of setting a very small time quantum in Round-Robin scheduling?,It leads to the convoy effect.,It increases the average waiting time for processes.,"It results in many context switches, increasing overhead.",It makes the algorithm nonpreemptive.,It causes indefinite blocking or starvation.,C,"A small quantum results in many context switches, increasing overhead, as the system spends more time switching than executing processes."
What is the recommended relationship between the time quantum and context-switch time for efficient Round-Robin scheduling?,Context-switch time should be much larger than the time quantum.,They should be exactly equal for optimal performance.,"Context-switch time should be large relative to the time quantum (e.g., > 50%).","Time quantum should be large relative to context-switch time (e.g., context-switch time < 10% of time quantum).",There is no significant relationship between them.,D,"Time quantum should be large relative to context-switch time (e.g., context-switch time < 10% of time quantum) to minimize overhead."
What rule of thumb is suggested for the length of CPU bursts relative to the time quantum in Round-Robin scheduling?,All CPU bursts must be exactly equal to the time quantum.,No CPU burst should exceed the time quantum.,80% of CPU bursts should be shorter than the time quantum.,All CPU bursts should be longer than the time quantum.,The sum of all CPU bursts should equal the total time quantum.,C,"A rule of thumb is that 80% of CPU bursts should be shorter than the time quantum, which allows most processes to finish without being preempted unnecessarily."
The Shortest-Job-First (SJF) algorithm is considered a special case of which general scheduling algorithm?,"First-Come, First-Served (FCFS)",Round-Robin (RR),Priority-scheduling,Multilevel Queue Scheduling,Multilevel Feedback Queue Scheduling,C,The SJF algorithm is a special case of the general priority-scheduling algorithm.
"In priority scheduling, how are processes with equal priority typically handled?",They are scheduled randomly.,"They are scheduled in Last-In, First-Out (LIFO) order.",They are scheduled using a Round-Robin approach.,"They are scheduled in First-Come, First-Served (FCFS) order.",They are sent to a lower-priority queue.,D,Equal-priority processes are scheduled in FCFS order.
What defines the priority in SJF when viewed as a priority-scheduling algorithm?,"It's based on arrival time, with earlier arrival meaning higher priority.",Priority is the inverse of the predicted next CPU burst (shorter burst = higher priority).,It's determined by the memory requirements of the process (less memory = higher priority).,Priority is assigned externally by the system administrator.,It depends on the number of I/O operations a process performs.,B,SJF is a priority algorithm where priority is the inverse of the (predicted) next CPU burst (shorter burst = higher priority).
What is the major problem associated with priority scheduling?,Excessive context switching.,Difficulty in estimating CPU burst times.,Indefinite blocking or starvation of low-priority processes.,Degeneration to FCFS for large quantum sizes.,Inefficient use of I/O devices.,C,"The major problem with priority scheduling is indefinite blocking or starvation, where low-priority processes may wait indefinitely for the CPU."
Which technique is used to prevent starvation in priority-scheduling algorithms?,Time slicing.,Context switching.,Aging.,Dynamic priority reassignment.,Shortest-Job-First (SJF).,C,"A solution to starvation is aging, which gradually increases the priority of processes that wait for a long time."
What defines a 'multilevel queue' scheduling algorithm?,It allows processes to move between different queues based on their behavior.,"It partitions the ready queue into several separate queues, each potentially with its own scheduling algorithm.",It assigns a single queue for all processes but uses multiple priority levels.,It uses a single FIFO queue for all processes.,It schedules processes only based on their memory footprint.,B,"Multilevel queue scheduling partitions the ready queue into several separate queues, and each queue may have its own scheduling algorithm."
"In a multilevel queue scheduling system, how are processes typically assigned to a queue?",They move between queues based on their CPU burst behavior.,They are dynamically assigned based on system load.,They are typically permanently assigned to a queue.,They are randomly assigned to any available queue.,Their assignment changes based on aging.,C,"In multilevel queue scheduling, processes are typically permanently assigned to a queue."
How does the multilevel feedback queue scheduling algorithm differ fundamentally from the multilevel queue algorithm?,It uses only a single queue.,It does not use fixed priorities among queues.,It allows a process to move between queues.,It only supports nonpreemptive scheduling.,It primarily focuses on batch processes.,C,"The multilevel feedback queue scheduling algorithm allows a process to move between queues, unlike multilevel queue scheduling where processes are usually permanently assigned."
What is the purpose of allowing processes to move between queues in a multilevel feedback queue scheduling algorithm?,To ensure all processes receive equal CPU time regardless of burst characteristics.,To keep I/O-bound and interactive processes (short CPU bursts) in higher-priority queues.,To force all processes into the lowest-priority queue after some time.,To increase the average waiting time for CPU-bound processes.,To simplify the scheduling logic by reducing the number of queues.,B,"The multilevel feedback queue separates processes by CPU burst characteristics: processes using too much CPU time are moved to lower-priority queues, which keeps I/O-bound and interactive processes (short CPU bursts) in higher-priority queues."
How is aging typically implemented within a multilevel feedback queue scheduling algorithm?,By decreasing the time quantum for processes that wait too long.,By permanently assigning waiting processes to the lowest priority queue.,By moving processes that wait too long in lower-priority queues to higher-priority queues.,By terminating processes that have been waiting for an extended period.,By giving a bonus time quantum to processes that complete quickly.,C,"Aging is implemented by moving processes that wait too long in lower-priority queues to higher-priority queues, preventing starvation."
Which scheduling algorithm is described as the 'most general and configurable CPU-scheduling algorithm' but also the 'most complex to define optimally'?,"First-Come, First-Served (FCFS)",Shortest-Job-First (SJF),Round-Robin (RR),Multilevel Feedback Queue Scheduling,Priority Scheduling,D,"The multilevel feedback queue is described as the most general and configurable CPU-scheduling algorithm, but also the most complex to define optimally."
"According to the glossary, what is 'starvation' in the context of CPU scheduling?",A situation where a CPU-bound process monopolizes the CPU.,A scheduling risk in which a thread that is ready to run never gets put onto the CPU due to the scheduling algorithm.,The condition where a process requests I/O too frequently.,When a system runs out of available memory for new processes.,A temporary pause in process execution due to an interrupt.,B,Starvation (or infinite blocking) is a scheduling risk in which a thread that is ready to run never gets put onto the CPU due to the scheduling algorithm - it is starved for CPU time.
What is the main characteristic of a 'foreground' thread as defined in the glossary?,It is a batch job with no interactive input.,It runs with the lowest possible priority.,It is interactive and has input directed to it.,It primarily performs I/O operations.,It is permanently assigned to a background queue.,C,A foreground thread is interactive and has input directed to it (such as a window currently selected as active or a terminal window that is currently selected to receive input).
What entity do modern operating systems primarily schedule for CPU execution?,Processes,User-level threads,Kernel-level threads,Lightweight processes,Applications,C,"Modern operating systems schedule kernel-level threads, not processes."
How do user-level threads gain access to a CPU for execution?,They directly access the CPU.,They are managed and scheduled by the operating system kernel directly.,"They are mapped to kernel-level threads, possibly via a Lightweight Process (LWP).",They run independently of kernel-level threads.,They must first be converted into full processes.,C,"User-level threads are managed by a thread library and must be mapped to kernel-level threads (possibly via a lightweight process, LWP) to run on a CPU."
"In the context of thread scheduling, what defines Process-Contention Scope (PCS)?",The kernel schedules kernel-level threads among all threads in the system.,"The thread library schedules user-level threads onto available LWPs, with competition among threads within the same process.",Competition for the CPU occurs among all processes in the system.,Threads are scheduled directly onto the CPU without intermediate mapping.,Only a single thread from a process can run at a time.,B,"Process-contention scope (PCS) is where the thread library schedules user-level threads onto available LWPs, and competition for the CPU occurs among threads within the same process."
What characterizes System-Contention Scope (SCS) in thread scheduling?,User-level threads compete for CPU time within a single process.,The thread library manages scheduling exclusively.,"The kernel schedules kernel-level threads onto a CPU, with competition among all threads in the system.",Scheduling is limited to threads of the same priority level.,It applies only to many-to-one threading models.,C,"System-contention scope (SCS) is a method in which kernel schedules kernel-level threads onto a CPU, and competition for the CPU occurs among all threads in the system."
Which of the following is typically true regarding Process-Contention Scope (PCS) scheduling?,It guarantees time slicing among equal-priority threads.,It is solely managed by the operating system kernel.,It uses priority-based scheduling and preempts lower-priority threads.,Competition for the CPU occurs among all threads in the system.,It is the only scheduling scope used in one-to-one systems like Windows.,C,"PCS typically uses priority-based scheduling, preempting lower-priority threads."
"Under Process-Contention Scope (PCS), what is the guarantee regarding time slicing among equal-priority threads?",Time slicing is always guaranteed.,Time slicing is guaranteed only if there are enough LWPs.,There is no guarantee of time slicing.,Time slicing is managed by the kernel.,Time slicing only occurs for higher-priority threads.,C,"The text states that for PCS, there is 'No guarantee of time slicing among equal-priority threads'."
Which type of threading model system commonly schedules threads using *only* System-Contention Scope (SCS)?,Many-to-many models,Many-to-one models,One-to-one models,User-level only models,Hybrid models,C,"Systems using the one-to-one model (e.g., Windows, Linux) schedule threads using only SCS."
What Pthread constant is used to specify Process-Contention Scope (PCS) during thread creation?,PTHREAD_SCOPE_SYSTEM,PTHREAD_SCOPE_THREAD,PTHREAD_SCOPE_PROCESS,PTHREAD_SCOPE_KERNEL,PTHREAD_SCOPE_LWP,C,The POSIX Pthread API allows specifying PCS using PTHREAD_SCOPE_PROCESS.
Which Pthread constant is utilized to specify System-Contention Scope (SCS) when creating a thread?,PTHREAD_SCOPE_PROCESS,PTHREAD_SCOPE_USER,PTHREAD_SCOPE_LIBRARY,PTHREAD_SCOPE_SYSTEM,PTHREAD_SCOPE_GLOBAL,D,The POSIX Pthread API allows specifying SCS using PTHREAD_SCOPE_SYSTEM.
"On many-to-many threading systems, what is the effect of using PTHREAD_SCOPE_PROCESS for user-level thread scheduling?",It creates and binds an LWP for each user-level thread.,It causes the kernel to schedule user-level threads directly.,It schedules user-level threads onto LWPs managed by the thread library.,It forces all threads to run in System-Contention Scope.,It disables time slicing for equal-priority threads.,C,"On many-to-many systems, PTHREAD_SCOPE_PROCESS schedules user-level threads onto LWPs managed by the thread library."
"When PTHREAD_SCOPE_SYSTEM is used on a many-to-many threading system, what is the resulting behavior?",User-level threads are scheduled by the thread library onto available LWPs.,It effectively uses a one-to-one policy by creating and binding an LWP for each user-level thread.,Threads compete for the CPU only within their own process.,The system switches to a many-to-one model.,User-level threads are not mapped to kernel-level threads.,B,"PTHREAD_SCOPE_SYSTEM on many-to-many systems creates and binds an LWP for each user-level thread, effectively using a one-to-one policy."
Which pair of functions is used in the Pthread API to set and get the contention scope of a thread's attributes?,pthread_create() and pthread_join(),pthread_setscope() and pthread_getscope(),pthread_attr_setscope() and pthread_attr_getscope(),pthread_config_scope() and pthread_query_scope(),set_thread_scope() and get_thread_scope(),C,The functions for setting/getting contention scope are pthread_attr_setscope() and pthread_attr_getscope().
Which operating systems are mentioned as only allowing PTHREAD_SCOPE_SYSTEM for thread scheduling?,Windows and Solaris,Linux and macOS,UNIX and BSD,Android and iOS,IBM AIX and HP-UX,B,"Some systems (e.g., Linux, macOS) only allow PTHREAD_SCOPE_SYSTEM."
Which concept is introduced in systems with multiple processing cores to distribute workload?,Context switching,Virtualization,Load sharing,Paging,Inter-process communication,C,Scheduling in systems with multiple processing cores introduces load sharing and increased complexity.
The term 'multiprocessor' commonly applies to which of the following architectures?,Single-core CPUs only,Dedicated I/O processors,"Multicore CPUs, multithreaded cores, NUMA systems, and heterogeneous multiprocessing",GPU clusters without a CPU,Embedded systems with a single microcontroller,C,"The term multiprocessor now applies to multicore CPUs, multithreaded cores, NUMA systems, and heterogeneous multiprocessing."
"In asymmetric multiprocessing, what is the role of the single main server processor?",It exclusively executes user code.,"It handles all scheduling, I/O, and system activities.",It is responsible only for load balancing.,It serves as a backup processor in case of failure.,It manages private per-processor ready queues.,B,"Asymmetric multiprocessing features a single main server processor handling all scheduling, I/O, and system activities, while other processors execute user code."
Which of the following is a potential drawback of asymmetric multiprocessing?,Increased data sharing complexity,The main server can become a performance bottleneck.,Poor utilization of other processors,Lack of support in modern operating systems,High switching cost due to pipeline flushing,B,A disadvantage of asymmetric multiprocessing is that the main server can become a performance bottleneck due to its centralized role.
Which characteristic best defines symmetric multiprocessing (SMP)?,A single processor manages all system resources.,Each processor is self-scheduling.,"Processors are dedicated to specific tasks (e.g., I/O, user code).",It only supports a common ready queue.,It is primarily used in embedded systems.,B,"Symmetric multiprocessing (SMP) is defined by each processor being self-scheduling, meaning they examine a ready queue and select a thread to run."
What is a potential issue with using a common ready queue in Symmetric Multiprocessing (SMP) systems?,Reduced processor affinity,It eliminates the need for locking mechanisms.,"It can lead to race conditions requiring locking, which may be a performance bottleneck.",It is only suitable for heterogeneous multiprocessing.,Processors become idle more frequently.,C,"When all threads are in a common ready queue in SMP, potential race conditions require locking, which can become a performance bottleneck."
Which strategy for organizing threads in SMP systems typically avoids locking overhead and benefits from processor affinity?,"A single, global dispatcher queue",Common ready queue,Private per-processor queues,Distributed hash table queues,FIFO queues for all processes,C,"Private per-processor queues avoid locking overhead and are common in SMP systems, benefiting from processor affinity."
What is the primary benefit of contemporary hardware using multicore processor chips compared to systems with separate physical CPU chips?,They support only coarse-grained multithreading.,They are slower but consume more power.,They are faster and consume less power.,They eliminate the need for an operating system.,They require only one level of scheduling.,C,"Multicore processors are faster and consume less power than systems with separate physical CPU chips, as stated in the text."
What is a 'memory stall' in the context of processor operation?,When a processor is idled by an operating system scheduler.,"When a processor waits for data from memory (e.g., due to cache miss), wasting significant time.",When a processor runs out of tasks to execute.,When a processor is unable to switch between hardware threads.,When a processor attempts to access protected memory.,B,"A memory stall occurs when a processor waits for data from memory (e.g., due to cache miss), wasting significant time."
How do many processor designs mitigate the issue of memory stalls?,By increasing cache size only.,By implementing single-threaded processing cores.,By implementing multithreaded processing cores with two or more hardware threads per core.,By reducing the number of cores on the chip.,By using only coarse-grained multithreading.,C,"To mitigate memory stalls, many designs implement multithreaded processing cores with two or more hardware threads per core, allowing the core to switch threads if one stalls."
Which term is synonymous with 'chip multithreading' (CMT)?,Asymmetric multiprocessing,Soft affinity,Hyper-threading or Simultaneous Multithreading (SMT),Push migration,NUMA scheduling,C,Chip multithreading (CMT) is also known as hyper-threading or simultaneous multithreading (SMT).
What distinguishes coarse-grained multithreading from fine-grained multithreading?,"Coarse-grained switches threads at instruction cycle boundaries, while fine-grained switches on long-latency events.","Coarse-grained has low switching cost due to architectural design, while fine-grained has high switching cost due to pipeline flushing.","Coarse-grained switches threads on long-latency events (e.g., memory stall) with high switching cost due to pipeline flushing, while fine-grained switches at a finer granularity (e.g., instruction cycle boundary) with low switching cost.","Coarse-grained is for single-core processors, while fine-grained is for multicore processors.","Coarse-grained is a software-only approach, while fine-grained is hardware-based.",C,"Coarse-grained multithreading switches threads on long-latency events (e.g., memory stall) with high switching cost due to pipeline flushing. Fine-grained multithreading switches threads at a finer granularity (e.g., instruction cycle boundary) with low switching cost due to architectural design."
"In multithreaded, multicore processors, what are the two levels of scheduling required?",Process scheduling and thread preemption.,"Operating system scheduling software threads onto hardware threads, and each core deciding which hardware thread to run.",User-level scheduling and kernel-level scheduling.,I/O scheduling and CPU scheduling.,Batch scheduling and interactive scheduling.,B,"Multithreaded, multicore processors require two levels of scheduling: the operating system schedules software threads onto hardware threads (logical CPUs), and each core decides which hardware thread to run."
What is the primary goal of load balancing in an SMP system?,To ensure all processors are idle as much as possible.,To keep the workload evenly distributed across all processors.,To force all threads to run on a single processor.,To eliminate the need for processor affinity.,To centralize all scheduling decisions on one core.,B,Load balancing attempts to keep the workload evenly distributed across all processors in an SMP system.
Load balancing is generally necessary for SMP systems with which type of ready queue organization?,Common ready queues,Private per-processor ready queues,Only systems using asymmetric multiprocessing,Systems with no hardware threads,Systems with only one level of scheduling,B,Load balancing is necessary for systems with private per-processor ready queues because otherwise some processors might become idle while others are overloaded. It is unnecessary for common ready queues as the load is naturally balanced by processors pulling from the shared queue.
"In load balancing, what is 'push migration'?",An idle processor requesting a task from a busy processor.,A task periodically checks processor loads and moves threads from overloaded to idle/less-busy processors.,A process being forced to switch contexts due to a higher priority task.,The operating system automatically assigning new processes to the least busy core.,A processor initiating a memory stall mitigation technique.,B,"With push migration, a specific task periodically checks the load on each processor and—if it finds an imbalance—evenly distributes the load by moving (or pushing) threads from overloaded to idle or less-busy processors."
What is 'pull migration' in the context of load balancing?,An overloaded processor sending threads to another processor.,A central scheduler reassigning threads to balance load.,An idle processor pulling a waiting task from a busy processor.,Threads autonomously migrating to different processors.,A process requesting to be moved to a specific processor.,C,Pull migration occurs when an idle processor pulls a waiting thread from a busy processor.
What is 'processor affinity'?,The ability of a processor to run multiple threads simultaneously.,A process's preference to run on the processor where it is currently running or recently ran.,The feature that allows multiple processors to share a common ready queue.,A mechanism to balance load across all available processors.,The process of disabling a processor to save power.,B,"Processor affinity means a process has an affinity for the processor on which it is currently running, largely due to the 'warm cache' benefit."
What is the primary benefit of processor affinity for a process?,It guarantees the process will always run on the same processor.,It allows the process to utilize more CPU cores concurrently.,"It benefits from a 'warm cache', where recently accessed data is already in the processor's cache.",It reduces the need for context switching.,It simplifies the operating system's scheduling algorithm.,C,"Processor affinity benefits from 'warm cache', meaning data recently accessed by the thread populates the processor's cache, leading to faster access times."
"Which type of processor affinity allows the OS to attempt to keep a process on the same processor but does not guarantee it, allowing for migration during load balancing?",Hard affinity,Absolute affinity,Soft affinity,Fixed affinity,Zero affinity,C,"Soft affinity is when the OS attempts to keep a process on the same processor but does not guarantee it, allowing migration during load balancing."
What is 'hard affinity'?,The default affinity setting for all processes.,When a process has an affinity for any available processor.,When system calls allow a process to specify a subset of processors on which it can run.,An affinity that is easily changed by the operating system.,An affinity that prevents any form of load balancing.,C,"Hard affinity is when system calls allow a process to specify a subset of processors on which it can run, effectively restricting its execution to those processors."
"In modern multicore NUMA systems, what is the tension between load balancing and minimizing memory access times?","Load balancing typically enhances processor affinity, while minimizing memory access times requires migration.","Load balancing often counteracts processor affinity benefits (which optimize memory access), as migrating a thread can incur cache invalidation costs.","Load balancing only applies to systems without NUMA, thus no tension exists.","Minimizing memory access times is solely a hardware concern, unrelated to load balancing.",Both load balancing and memory access optimization are achieved by keeping threads on their initial processor.,B,"Load balancing often counteracts processor affinity benefits because migrating a thread to another processor incurs the cost of invalidating and repopulating caches, which increases memory access times, creating a tension with the goal of minimizing memory access times."
What defines heterogeneous multiprocessing (HMP)?,Systems where different cores run different instruction sets.,Systems with cores that vary in clock speed and power management but run the same instruction set.,Systems where only one main processor handles all system tasks.,Systems designed exclusively for cloud computing.,"Systems with only a single, high-performance core.",B,Heterogeneous multiprocessing (HMP) refers to systems with cores that run the same instruction set but vary in clock speed and power management.
Which of the following statements correctly distinguishes heterogeneous multiprocessing (HMP) from asymmetric multiprocessing?,"HMP uses a single main server processor, while asymmetric multiprocessing uses multiple self-scheduling processors.","HMP aims for better power consumption, while asymmetric multiprocessing focuses on high performance.","In HMP, both system and user tasks can run on any core, whereas in asymmetric multiprocessing, only one processor accesses system data structures and others run user threads.","HMP supports only 'big' cores, while asymmetric multiprocessing supports 'LITTLE' cores.","HMP is an older concept, while asymmetric multiprocessing is newer.",C,"HMP allows both system and user tasks to run on any core, differentiating it from asymmetric multiprocessing where a single main server processor handles system activities and others execute user code."
What is the primary intention behind heterogeneous multiprocessing (HMP) designs like ARM's big.LITTLE architecture?,To maximize raw processing power across all tasks.,To simplify the operating system scheduler.,To achieve better power consumption management by assigning tasks to cores based on their demands.,To completely eliminate the need for memory caches.,To ensure all cores run at the same clock speed.,C,"The intention of HMP, as seen in big.LITTLE, is better power consumption management by assigning tasks to cores based on their demands (e.g., high-performance tasks to 'big' cores, background tasks to 'LITTLE' cores)."
"In ARM's big.LITTLE architecture, what is the typical role of the 'LITTLE' cores?","Handling short, high-performance tasks with higher energy consumption.",Managing I/O operations exclusively.,Executing longer background tasks with lower energy consumption.,Serving as redundant processors for fault tolerance.,Managing the main server processes for scheduling.,C,"In ARM's big.LITTLE architecture, 'LITTLE' cores are designed for lower energy consumption and are typically used for longer background tasks."
Which of the following operating systems is mentioned as supporting Heterogeneous Multiprocessing (HMP) scheduling?,MS-DOS,Windows 95,Windows 10,Unix System V,Classic Mac OS,C,Windows 10 is explicitly mentioned as supporting HMP scheduling.
Which of the following best describes a 'soft real-time system'?,"It guarantees that a critical real-time process will be serviced by its deadline, or it's considered a failure.",It provides a strict guarantee on when a critical real-time process will be scheduled.,It offers preference to critical real-time processes over noncritical ones but provides no strict guarantee on scheduling time.,It prioritizes noncritical processes to ensure overall system stability.,"It uses a first-come, first-served policy for all processes, irrespective of priority.",C,"Soft real-time systems provide no guarantee on when a critical real-time process will be scheduled, only preference over noncritical processes."
What is the defining characteristic of a 'hard real-time system'?,"It prioritizes processes based on their CPU burst time, not their deadlines.",It allows critical processes to miss their deadlines occasionally without system failure.,"It has stricter requirements, where a task must be serviced by its deadline, or it is considered a failure.",It guarantees that all processes will receive an equal share of CPU time.,It relies solely on a round-robin scheduling policy for all tasks.,C,"Hard real-time systems have stricter requirements; a task must be serviced by its deadline, or it's considered a failure."
What does 'event latency' refer to in real-time systems?,The total time a process waits in the ready queue.,The time elapsed from when an event occurs to when it is serviced.,The delay between a user input and system response in non-real-time systems.,The time it takes for a CPU to switch between two different processes.,The period during which interrupts are disabled.,B,Event latency is defined as the time elapsed from when an event occurs to when it is serviced.
Which type of latency measures the time from an interrupt arrival at the CPU to the start of the interrupt service routine (ISR)?,Dispatch latency,Event latency,Scheduling latency,Interrupt latency,Processing latency,D,Interrupt latency is the time from interrupt arrival at CPU to the start of the interrupt service routine (ISR).
"For hard real-time systems, how must interrupt latency be managed?",It can vary widely and does not need to be bounded.,It must be maximized to allow other processes to run.,It must be minimized and bounded.,It is not a critical factor in hard real-time systems.,Interrupts should be disabled for extended periods.,C,"Interrupt latency must be minimized and bounded for hard real-time systems. Also, interrupts should be disabled for very short periods."
What is 'dispatch latency'?,The time a process waits for I/O completion.,The time for the scheduling dispatcher to stop one process and start another.,The total time from process creation to termination.,The delay introduced by context switching overhead in non-real-time systems.,The time it takes for an interrupt to be recognized by the CPU.,B,Dispatch latency is the time for the scheduling dispatcher to stop one process and start another.
Which of the following is typically used to achieve minimized dispatch latency in real-time systems?,Non-preemptive kernels,Batch processing systems,Preemptive kernels,Maximizing interrupt disabling periods,Large time slices for all processes,C,"Minimized dispatch latency is achieved with preemptive kernels, which allow a higher-priority process to interrupt a lower-priority one immediately."
What are the two phases that comprise dispatch latency?,Arrival phase and service phase,Ready phase and running phase,Conflict phase and dispatch phase,Interrupt phase and processing phase,Blocking phase and unblocking phase,C,"Dispatch latency has a conflict phase (preemption of kernel processes, release of resources by low-priority processes) and a dispatch phase (scheduling high-priority process)."
What type of scheduling algorithm must real-time OS schedulers support to respond immediately to real-time processes?,Time-sharing with fixed time slices,"Non-preemptive, shortest-job-first",Priority-based with preemption,Round-robin without priorities,"First-come, first-served only",C,Real-time OS schedulers must support a priority-based algorithm with preemption to respond immediately to real-time processes.
"What does a preemptive, priority-based scheduler guarantee on its own?",Hard real-time functionality,Strict deadlines for all tasks,Optimal CPU utilization for all scenarios,Only soft real-time functionality,Equal CPU distribution among all processes,D,"Providing a preemptive, priority-based scheduler guarantees only soft real-time functionality. Hard real-time systems require additional features."
"For a periodic real-time task, if its processing time is $t$, deadline is $d$, and period is $p$, what is the correct relationship between these values?",$t > d > p$,$p > d > t$,$0 \le t \le d \le p$,$t + d = p$,$d = p - t$,C,"For periodic tasks, the relationship between processing time ($t$), deadline ($d$), and period ($p$) is $0 \le t \le d \le p$."
What is the 'rate' of a periodic real-time task?,Its processing time ($t$).,Its deadline ($d$).,The inverse of its period ($1/p$).,The sum of its processing time and deadline ($t+d$).,The frequency of its preemption.,C,"The rate of a periodic task is $1/p$, where $p$ is its period."
What is the purpose of an 'admission-control' algorithm in real-time scheduling?,To dynamically adjust the priorities of all running processes.,To ensure that all processes run for an equal amount of time.,"To allow a process to start only if its completion by deadline can be guaranteed; otherwise, it rejects the request.",To place processes into a FIFO queue upon arrival.,To determine the optimal CPU utilization bound for a system.,C,"An admission-control algorithm admits a process only if the scheduler can guarantee completion by its deadline; otherwise, it rejects the request."
How does the Rate-Monotonic scheduling algorithm assign priorities to periodic tasks?,"Dynamically, based on their remaining processing time.","Statically, based on their inverse period (shorter period = higher priority).","Dynamically, based on their earliest deadline.","Statically, based on their total processing time (longer time = higher priority).","Randomly, to ensure fairness.",B,"Rate-monotonic scheduling assigns priorities statically based on its period: shorter period = higher priority (i.e., inversely based on its period)."
What is a key assumption made by the Rate-Monotonic scheduling algorithm?,Processes are not periodic.,CPU utilization can always reach 100%.,Processing time is constant for each CPU burst.,Priorities are adjusted dynamically.,All tasks have the same deadline.,C,Rate-Monotonic scheduling assumes processing time is constant for each CPU burst.
What is the optimality claim for Rate-Monotonic scheduling?,It can schedule any set of processes to 100% CPU utilization.,It is optimal for dynamic priority policies.,"If a set of processes cannot be scheduled by rate-monotonic, it cannot be scheduled by any other static-priority algorithm.",It always guarantees earliest deadlines are met.,It minimizes dispatch latency in all cases.,C,"Rate-Monotonic is considered optimal because if a set of processes cannot be scheduled by rate-monotonic, it cannot be scheduled by any other static-priority algorithm."
What is the worst-case CPU utilization bound for $N$ processes under Rate-Monotonic scheduling?,$1/N$,$N 	imes (2^{1/N} - 1)$,$1 - N 	imes (2^{1/N} - 1)$,$100\% / N$,$N / (2^{1/N} - 1)$,B,"The worst-case CPU utilization for $N$ processes under Rate-Monotonic scheduling is $N\left( {{2^{1/N}} - 1} 
ight){
m{.}}$"
"As the number of processes ($N$) approaches infinity, what percentage does the worst-case CPU utilization under Rate-Monotonic scheduling approach?",100%,83%,69%,50%,0%,C,"As $N 	o \infty$, the worst-case CPU utilization approaches 69%."
How does Earliest-Deadline-First (EDF) scheduling assign priorities?,"Statically, based on the process's period.","Dynamically, based on the process's remaining execution time.","Dynamically, based on the earliest deadline (earlier deadline = higher priority).","Statically, based on the process's initial arrival time.","Randomly, to distribute CPU load evenly.",C,EDF scheduling assigns priorities dynamically based on deadline: earlier deadline = higher priority.
"Which of the following is NOT a requirement for processes under Earliest-Deadline-First (EDF) scheduling, unlike Rate-Monotonic scheduling?",Processes must be periodic.,Processes must announce their deadline when runnable.,Priorities are adjusted dynamically.,Processes can have varying CPU burst times.,The system aims to meet deadlines.,A,"Unlike rate-monotonic, EDF does not require processes to be periodic or have constant CPU burst times."
"Theoretically, what is the maximum CPU utilization achievable with Earliest-Deadline-First (EDF) scheduling?",69%,83%,90%,100%,Dependent on the number of processes.,D,"Theoretically, EDF is optimal and can schedule processes to meet deadlines with 100% CPU utilization."
How do Proportional Share schedulers allocate CPU time?,"They allocate shares among applications, so an application with N shares out of T total receives N/T of the total processor time.","They allocate CPU time strictly based on process priority, ignoring shares.","They use a round-robin approach, giving equal time slices to all applications.",They prioritize tasks with the shortest remaining processing time.,They only admit applications if they can achieve 100% CPU utilization.,A,Proportional share schedulers allocate shares among applications. An application with $N$ shares out of a total $T$ shares receives $N/T$ of the total processor time.
What is a key feature of Proportional Share scheduling regarding client admission?,It admits all clients regardless of resource availability.,It only admits clients if they are hard real-time tasks.,"It works with an admission-control policy, admitting a client only if sufficient shares are available.",It uses a FIFO queue for client admission.,It requires clients to be periodic tasks.,C,"Proportional share scheduling works with an admission-control policy, where a client is admitted only if sufficient shares are available."
"According to POSIX.1b, which scheduling class uses a first-come, first-served policy with no time slicing among equal-priority threads?",SCHED_RR,SCHED_OTHER,SCHED_FIFO,SCHED_DYNAMIC,SCHED_BATCH,C,"SCHED_FIFO defines a First-come, first-served policy with a FIFO queue and no time slicing among equal-priority threads."
Which POSIX real-time scheduling class is similar to SCHED_FIFO but provides time slicing among equal-priority threads?,SCHED_FIFO,SCHED_OTHER,SCHED_RR,SCHED_BATCH,SCHED_NORMAL,C,"SCHED_RR is a Round-robin policy, similar to SCHED_FIFO but provides time slicing among equal-priority threads."
Which POSIX API function is used to retrieve the scheduling policy of a thread attribute object?,pthread_attr_setschedpolicy,pthread_create,pthread_attr_getschedpolicy,pthread_join,pthread_exit,C,"pthread_attr_getschedpolicy(pthread_attr_t *attr, int *policy) is used for getting the scheduling policy."
"In the context of operating system scheduling examples, what does the term ""process scheduling"" generally refer to for Solaris and Windows operating systems?",User-level threads,Application processes,Kernel threads,Background services,Graphical user interface tasks,C,"The text states, ""The term 'process scheduling' is used generally, referring to kernel threads (Solaris, Windows) or tasks (Linux)."""
Which of the following best describes the evolution of the Linux scheduler prior to Version 2.6.23?,"Started with O(1) scheduler, then moved to traditional UNIX scheduling.",Always used the Completely Fair Scheduler (CFS) since its inception.,"Began with traditional UNIX scheduling, then introduced the O(1) scheduler, and later CFS.",Directly transitioned from traditional UNIX scheduling to CFS.,Focused only on single-processor systems before Version 2.5.,C,"Linux scheduling history shows: 'Prior to Version 2.5: Traditional UNIX scheduling', 'Version 2.5: Introduced O(1) scheduler', and 'Version 2.6.23: Completely Fair Scheduler (CFS) became default'."
What significant improvements were introduced with the Linux O(1) scheduler in Version 2.5?,Reduced real-time latency and dynamic priority adjustments.,"Constant time scheduling, improved SMP support, processor affinity, and load balancing.",Integration of user-mode scheduling and the Concurrency Runtime.,Introduction of scheduling domains and NUMA node awareness.,Strict adherence to POSIX real-time standards only.,B,"The O(1) scheduler 'Introduced O(1) scheduler (constant time regardless of tasks), improved SMP support, processor affinity, and load balancing'."
On what fundamental concept is Linux scheduling based?,A single global priority queue for all tasks.,Strict time-slicing with equal quantum for all tasks.,"Scheduling classes, each with a specific priority.",User-defined priorities that override kernel settings.,A purely round-robin algorithm for all tasks.,C,"The text states, 'Linux scheduling is based on scheduling classes, each with a specific priority'."
Which two standard scheduling classes are implemented by default in Linux kernels?,Interactive and Time-sharing,System and Fixed-priority,Fair share and Real-time,Default (CFS) and Real-time,Variable and Real-time,D,Standard Linux kernels implement two classes: default (CFS) and real-time.
"In the Linux Completely Fair Scheduler (CFS), how is a proportion of CPU time assigned to each task?","Based on its process ID, with lower IDs getting more time.","Strictly on a round-robin basis, irrespective of task type.","According to its nice value, which influences its relative scheduling priority.",By assigning a fixed time quantum to every runnable task.,By dynamically adjusting priority based on I/O wait times only.,C,The CFS scheduler 'Assigns a proportion of CPU time to each task based on its nice value'.
What is the range and meaning of the 'nice value' used in the Linux CFS scheduler?,"0 to 100, where higher values indicate higher priority.","-20 to +19, where a numerically lower value indicates higher priority.","1 to 32, where values correspond to fixed time quanta.","0 to 99, used exclusively for real-time tasks.","-10 to +10, where values are dynamically adjusted by the kernel.",B,The nice value range is '-20 to +19; lower value = higher priority'.
What concept in the Linux CFS scheduler refers to an interval during which every runnable task should run at least once?,Time quantum,Virtual run time,Targeted latency,Scheduling epoch,Dynamic priority adjustment,C,The text defines 'targeted latency' as 'an interval during which every runnable task should run at least once'.
How does the Linux CFS scheduler primarily select the next task to run?,It selects the task that has been waiting the longest.,It chooses the task with the highest static priority.,It selects the task with the smallest virtual run time (vruntime).,It randomly picks a task from the runnable queue.,It prioritizes tasks based on their memory usage.,C,The CFS scheduler 'Selects the task with the smallest vruntime to run next'.
How does the Linux CFS scheduler typically handle I/O-bound tasks compared to CPU-bound tasks?,It gives CPU-bound tasks higher priority due to their longer burst times.,It assigns both types of tasks equal priority to ensure fairness.,It gives I/O-bound tasks higher priority because their vruntime is lower due to shorter bursts.,It delays I/O-bound tasks to allow CPU-bound tasks to complete faster.,It migrates I/O-bound tasks to different NUMA nodes.,C,CFS 'Handles I/O-bound vs. CPU-bound tasks by giving I/O-bound tasks higher priority due to their lower vruntime (they run for shorter bursts)'.
"What data structure does the Linux CFS scheduler use to store runnable tasks, and how is it keyed?","A hash table, keyed by process ID.","A linked list, ordered by arrival time.","A red-black tree (balanced binary search tree), keyed by vruntime.","A simple array, indexed by nice value.","A priority queue, keyed by remaining time quantum.",C,"CFS 'Uses a red-black tree (balanced binary search tree) to store runnable tasks, keyed by vruntime'."
Which POSIX standards are used by Linux for real-time scheduling?,SCHED_BATCH or SCHED_IDLE,SCHED_OTHER or SCHED_RR,SCHED_FIFO or SCHED_RR,SCHED_NORMAL or SCHED_REALTIME,SCHED_PRIORITY or SCHED_QUANTUM,C,Linux real-time scheduling 'Uses POSIX standard (SCHED_FIFO or SCHED_RR)'.
"In Linux, what are the priority ranges for real-time and normal tasks, respectively?","100-139 for real-time, 0-99 for normal","0-15 for real-time, 16-31 for normal","0-99 for real-time (static), 100-139 for normal (based on nice values)","-20 to +19 for real-time, 0 to 99 for normal","0-100 for real-time, 101-200 for normal",C,"Priority ranges: '0-99 for real-time (static), 100-139 for normal (based on nice values)'. Lower numeric value means higher priority."
What are the primary goals of the CFS load balancing mechanism in Linux?,To prioritize I/O-bound tasks and reduce context switches.,To ensure every task runs once per targeted latency and reduce nice values.,"To equalize load among processing cores, be NUMA-aware, and minimize thread migration.",To increase the virtual run time of CPU-bound tasks and avoid preemption.,To solely balance threads based on their static real-time priorities.,C,"CFS load balancing 'Equalizes load among processing cores, is NUMA-aware, and minimizes thread migration'."
What is a 'scheduling domain' in the context of Linux CFS load balancing?,A specific priority level assigned to a scheduling class.,An interval of time during which load balancing must occur.,A set of CPU cores that can be balanced against each other based on shared resources.,A user-defined group of tasks that share a common nice value.,A network boundary that prevents thread migration.,C,"Scheduling domains are defined as 'sets of CPU cores balanced against each other based on shared resources (e.g., L1, L2, L3 caches, NUMA nodes)'."
What type of scheduling algorithm does Windows use?,Pure round-robin,"First-come, first-served","Priority-based, preemptive","Non-preemptive, shortest job first",Fair-share based on CPU shares,C,"Windows uses a 'priority-based, preemptive scheduling algorithm'."
"In Windows, what is the role of the 'dispatcher'?",To manage system calls and interrupts.,To handle memory allocation for new processes.,To handle scheduling of threads.,To perform disk I/O operations.,To translate user-mode instructions to kernel-mode.,C,"The text states, 'The dispatcher handles scheduling'."
"What is the 32-level priority scheme in Windows, and which priority level is reserved for the memory management thread?","Variable class (0-15), Real-time class (16-31); Priority 31 for memory management.","Variable class (16-31), Real-time class (1-15); Priority 0 for memory management.","Variable class (1-15), Real-time class (16-31); Priority 0 for memory management.","Variable class (0-31), no specific priority for memory management.","Fixed-priority class (1-15), Dynamic class (16-31); Priority 1 for memory management.",C,"The 32-level priority scheme includes 'Variable class: priorities 1-15', 'Real-time class: priorities 16-31', and 'Priority 0: memory management thread'."
What happens in Windows if the dispatcher does not find any ready thread to run?,The system enters a low-power sleep state.,The last running thread resumes execution.,It executes a special thread called the 'idle thread'.,It waits indefinitely for an interrupt.,It requests the user to start a new application.,C,The dispatcher 'executes an idle thread if no ready thread is found'.
"In Windows, under what circumstances is a variable-priority thread's priority typically lowered?",When it calls a blocking system call.,When it is released from a wait operation.,"When its time quantum expires, but never below its base priority.",When it becomes the foreground process.,When another thread with a numerically lower base priority becomes ready.,C,For variable-priority threads: 'priority lowered when quantum expires (never below base priority)'.
"What is User-Mode Scheduling (UMS) in Windows, introduced in Windows 7+?",A feature that allows the kernel to manage threads more efficiently by reducing context switches.,A system that enables applications to directly interact with hardware without kernel intervention.,A feature that allows applications to create and manage threads independently of the kernel.,A debugging tool that provides insights into kernel thread behavior.,A framework for managing graphical user interface events and processes.,C,UMS is defined as 'A Microsoft Windows 7 feature that allows applications to create and manage threads independently of the kernel'.
"What was the predecessor to User-Mode Scheduling (UMS) in Windows, and what was its limitation?",Processes; they had no way to manage threads.,Kernel threads; they required too much kernel intervention.,Fibers; they had limited use due to a shared thread environment block.,Tasks; they could not be mapped to kernel threads.,Lightweight processes; they lacked true concurrency.,C,Predecessor: 'fibers (limited use due to shared thread environment block)'.
What are SMT sets in the context of Windows multiprocessor scheduling?,A collection of threads waiting for I/O operations.,"Sets of logical processors on the same CPU core, such as hyper-threaded cores.",Groups of processes that share a common base priority.,A type of memory caching mechanism for faster thread access.,Virtual machines running on a single physical processor.,B,"SMT sets are defined as 'sets of logical processors on the same CPU core, e.g., hyper-threaded cores'."
How many scheduling classes does Solaris use for its priority-based thread scheduling?,Two,Three,Four,Five,Six,E,"Solaris uses priority-based thread scheduling with six classes: Time sharing (TS), Interactive (IA), Real time (RT), System (SYS), Fair share (FSS), Fixed priority (FP)."
"Which of the following is the default scheduling class in Solaris, and how does it manage priorities?",Real time; uses static priorities for guaranteed response.,System; reserved for kernel threads with fixed priorities.,Time sharing; dynamically alters priorities and time slices using a multilevel feedback queue.,Fair share; uses CPU shares instead of priorities for groups of processes.,Fixed priority; has dynamically adjusted priorities but fixed time slices.,C,Default class: 'Time sharing. Dynamically alters priorities and time slices using a multilevel feedback queue'.
"In Solaris, what is the inverse relationship observed between priority and time quantum for Time sharing (TS) and Interactive (IA) threads?",Higher priority means larger time quantum.,Higher priority means smaller time quantum.,Priority and time quantum are unrelated.,"Only time quantum changes, priority remains static.",Time quantum only applies to real-time threads.,B,"The dispatch table for TS/IA threads indicates an 'Inverse relationship with priority' for the time quantum, meaning higher priority gets a smaller time slice as per the 'multilevel feedback queue' description for Time sharing."
Which Solaris scheduling class has the highest priority and guarantees bounded response time for processes?,Time sharing (TS),Interactive (IA),Real time (RT),System (SYS),Fixed priority (FP),C,"Real-time class: 'Highest priority; real-time processes run before any other class, guaranteeing bounded response time'."
What is the primary characteristic that differentiates the Fixed-priority class from the Time-sharing class in Solaris 9+?,"Fixed-priority uses CPU shares, while Time-sharing uses priorities.","Fixed-priority allows dynamic priority adjustments, while Time-sharing does not.","Fixed-priority has static priorities that are not dynamically adjusted, unlike Time-sharing.","Fixed-priority is for kernel threads, Time-sharing is for user threads.","Fixed-priority guarantees bounded response time, Time-sharing does not.",C,"Fixed-priority class (Solaris 9+): 'Same priority range as time-sharing, but priorities are not dynamically adjusted'."
"In Solaris's Fair-share class (Solaris 9+), what concept is used instead of priorities to make scheduling decisions?",Nice values,Virtual run time,CPU shares,Targeted latency,Process IDs,C,Fair-share class (Solaris 9+): 'Uses CPU shares (entitlement to CPU resources) instead of priorities'.
How did the thread model in Solaris change with Solaris 9?,It introduced a many-to-one model.,It switched from a one-to-one model to a many-to-many model.,It switched from a many-to-many model to a one-to-one model.,It started supporting only user-mode threads.,It eliminated the concept of threads entirely.,C,"Solaris 'traditionally used many-to-many model, switched to one-to-one model with Solaris 9'."
What is the definition of 'scheduling classes' in Linux?,A method to group tasks based on their CPU utilization.,"A system where each class is assigned a specific priority, forming the basis of scheduling.",A historical term for kernel threads in older Linux versions.,A set of CPU cores balanced against one another.,A mechanism for real-time task priority adjustments.,B,The glossary defines 'scheduling classes' as 'Scheduling in the Linux system is based on scheduling classes - each class is assigned a specific priority.'
What does a numerically lower 'nice value' signify in Linux scheduling?,A lower CPU utilization.,A lower relative scheduling priority.,A higher relative scheduling priority.,A longer virtual run time.,A larger time quantum.,C,The glossary defines 'nice value' as 'where a numerically lower nice value indicates a higher relative scheduling priority.'
"According to the glossary, what is 'targeted latency' in Linux scheduling?",The maximum time a task can wait before execution.,The total CPU time allocated to a task.,An interval during which every runnable thread should run at least once.,The time it takes for a task to complete an I/O operation.,The delay introduced by context switching.,C,The glossary defines 'targeted latency' as 'an interval of time during which every runnable thread should run at least once.'
What is 'virtual run time' (vruntime) in Linux scheduling?,The actual CPU time a task has spent running.,"A metric that records how long each task has run, decaying based on priority.",A measure of a task's priority relative to other tasks.,The remaining time quantum for a task.,The time a task spends in a blocked state.,B,The glossary defines 'virtual run time' as 'A Linux scheduling aspect in which it records how long each task has run by maintaining the virtual run time of each task.' The main text adds that it 'decays based on priority (lower priority = higher decay rate)'.
What is the purpose of a 'scheduling domain' in Linux?,To define the scope of real-time priorities.,To group tasks with similar nice values.,To specify which threads can access shared memory.,To define a set of CPU cores that can be balanced against one another.,To control the dynamic adjustment of priorities.,D,The glossary defines 'scheduling domain' as 'A set of CPU cores that can be balanced against one another.'
"In Windows, what is an 'idle thread'?",A thread that is waiting for an I/O operation.,A thread that has terminated and is awaiting cleanup.,A special thread executed by the dispatcher if no ready thread is found.,A background thread performing low-priority system maintenance.,A thread that has been preempted but not yet scheduled.,C,"The glossary defines 'idle thread' as 'If no ready thread is found, the dispatcher will execute a special thread called the idle thread that runs on the CPU until the CPU is needed for some other activity.'"
What is Microsoft Windows' 'Concurrency Runtime' (ConcRT) primarily designed for?,Managing network connections and protocols.,Providing a framework for task-based parallelism on multicore processors in C++.,Handling graphical rendering and user interface events.,Implementing a secure boot process for Windows.,Optimizing memory usage for single-threaded applications.,B,The glossary defines 'Concurrency Runtime (ConcRT)' as 'A Microsoft Windows concurrent programming framework for C++ that is designed for task-based parallelism on multicore processors.'
"In Solaris's Fair-share class, what are 'shares'?",The number of processes within a given project.,A measure of a process's current CPU utilization.,A concept of CPU entitlement used instead of priorities for scheduling decisions.,The proportion of memory allocated to a process.,The time quantum assigned to a thread.,C,"The glossary defines 'shares' as 'A scheduling concept in which CPU shares instead of priorities are used to make scheduling decisions, providing an entitlement to CPU time for a process or a set of processes.'"
What is a 'project' in the context of Solaris scheduling?,"A single, high-priority process.",A group of CPU cores assigned to a specific task.,"A set of processes grouped together for scheduling purposes, particularly in the Fair-share class.",A system-level daemon responsible for resource management.,A temporary state a thread enters when waiting for I/O.,C,The glossary defines 'project' as 'A Solaris scheduling concept in which processes are grouped into a project and the project is scheduled.'
What is the initial crucial step in selecting a CPU-scheduling algorithm?,Implementing the algorithm directly into the OS.,Running simulations based on estimated workloads.,"Defining the criteria for selection, such as maximizing CPU utilization or throughput.",Using deterministic modeling with a predetermined workload.,Applying Little's formula to analyze queue lengths.,C,"The text states that the 'First step: Define criteria for selection (e.g., maximizing CPU utilization under response time constraints, maximizing throughput with proportional turnaround time).'"
Why is selecting a CPU-scheduling algorithm considered challenging?,The algorithms are too simple to differentiate effectively.,"There are limited parameters to consider, making comparisons difficult.",The process requires extensive mathematical proofs which are rarely available.,There is a wide variety of algorithms and numerous parameters to evaluate.,Real-world systems always use fixed scheduling algorithms.,D,"The text states, 'Selecting a CPU-scheduling algorithm is challenging due to various algorithms and parameters.'"
Which of the following best describes 'analytic evaluation'?,A method that involves coding an algorithm and testing it under real operating conditions.,An evaluation method that describes a system as a network of servers with queues.,A technique that uses an algorithm and system workload to produce a formula or number for performance assessment.,A process primarily focused on monitoring real systems to record event sequences.,A testing method to confirm that changes haven't introduced new bugs.,C,The text defines 'analytic evaluation' as: 'Uses an algorithm and system workload to produce a formula or number for performance evaluation.'
"What is 'deterministic modeling' a type of, and what does it typically involve?","It is a type of simulation, involving programming a model of the computer system.","It is a type of queueing model, using statistical distributions for variable processes.",It is a type of analytic evaluation that uses a predetermined workload to define an algorithm's performance.,"It is a real-world implementation technique, focusing on testing under actual operating conditions.",It is a method for generating random numbers based on probability distributions.,C,"The text states, 'Deterministic modeling: A type of analytic evaluation that takes a predetermined workload and defines each algorithm's performance for that workload.'"
Which characteristic is a key aspect of deterministic modeling?,It accounts for real-world variability through random number generation.,"It requires the input to be an exact, predetermined workload.",It is the most accurate evaluation method available.,Its results are broadly generalizable across different system environments.,It relies on complex mathematical formulas for probabilistic outcomes.,B,"A limitation of deterministic modeling is that it 'Requires exact input, results apply only to the specific workload,' implying a predetermined and exact workload is essential."
Which of the following is listed as an advantage of deterministic modeling?,It can accurately reflect real-world variability.,Its results are generalizable to any workload.,It provides exact performance numbers and is simple and fast.,It eliminates the need for any kind of predefined input.,"It is primarily used for complex, dynamic scheduling scenarios.",C,"The text lists advantages: 'Simple, fast, provides exact numbers, useful for describing algorithms and identifying trends.'"
A significant limitation of deterministic modeling is that its results:,Are always approximations and never exact.,"Only apply to the specific, predetermined workload used.",Require highly complex mathematics to compute.,Are difficult to obtain quickly due to computational overhead.,Tend to overestimate performance under typical conditions.,B,"The text states under limitations: 'results apply only to the specific workload, may not reflect real-world variability.'"
"Based on the example provided for a given workload of 5 processes, which scheduling algorithm yielded the lowest average waiting time?",Round Robin (quantum = 10ms),"First-Come, First-Served (FCFS)",Shortest-Job-First (SJF),A combination of FCFS and RR,All algorithms performed equally.,C,The example states: 'FCFS average waiting time: 28 milliseconds. SJF average waiting time: 13 milliseconds. RR average waiting time: 23 milliseconds.' SJF has the lowest.
When are queueing models particularly useful for CPU-scheduling algorithm evaluation?,When all processes have a fixed and predictable CPU burst time.,"When comparing algorithms on identical, real-world inputs captured via trace files.","When processes vary daily, but distributions of CPU/I/O bursts and arrival times can be estimated.","When an exact, predetermined workload is available for testing.","When the evaluation needs to be the most accurate possible, using real operating conditions.",C,"The text states: 'Useful when processes vary daily, but distributions of CPU/I/O bursts and arrival times can be measured/estimated.'"
"In the context of queueing models, how is the computer system typically described?","As a single, monolithic processing unit.","As a network of servers (CPU, I/O) with associated queues.","As a collection of independent, non-interacting processes.",As a fixed set of predefined tasks with no variability.,As a black box where only inputs and outputs are observed.,B,"The text states: 'System described as a network of servers (CPU, I/O) with queues.'"
What is the primary purpose of 'queueing-network analysis'?,To monitor real systems and record sequences of actual events.,To program a model of the computer system with software data structures.,"To compute metrics like utilization, average queue length, and average wait time from arrival and service rates.",To confirm that system changes haven't introduced new bugs.,To define criteria for CPU-scheduling algorithm selection.,C,"The text defines 'queueing-network analysis' as: 'Area of study to compute utilization, average queue length, average wait time, etc., from arrival and service rates.'"
"In Little's formula, n = lambda * W, what does the variable 'n' represent?",The average arrival rate for new processes.,The average waiting time in the queue.,The average long-term queue length (excluding the serviced process).,The system's CPU utilization percentage.,The total number of processes in the system.,C,The text explicitly defines 'n' as: 'average long-term queue length (excluding serviced process).'
A significant advantage of Little's formula (n = lambda * W) is its validity under what conditions?,"It is only valid for First-Come, First-Served (FCFS) scheduling.","It requires very specific, deterministic arrival distributions.",It is valid for any scheduling algorithm and any arrival distribution.,It only applies to systems with a single server.,It is only accurate for very short-term queue measurements.,C,The text states: 'Valid for any scheduling algorithm and arrival distribution.'
Which of the following is a limitation of queueing models for algorithm evaluation?,"They provide exact numbers, similar to deterministic modeling.",They are simple and fast to implement.,They often rely on unrealistic independent assumptions and may provide approximations.,They are able to handle an unlimited range of algorithms and distributions.,They are the most accurate evaluation method.,C,"The text lists limitations: '...often relies on unrealistic independent assumptions, results may be approximations.'"
How do simulations compare in accuracy to analytic methods for algorithm evaluation?,Simulations are generally less accurate than analytic methods.,Simulations provide more accurate evaluation than analytic methods.,Simulations and analytic methods offer comparable accuracy.,Simulations are only accurate when using random number generators.,Simulations are only accurate for very simple systems.,B,The text states: 'Provides more accurate evaluation than analytic methods.'
What is a core aspect of how simulations evaluate computer systems?,They directly integrate new algorithms into the OS kernel.,They involve programming a model of the computer system using software data structures.,They rely exclusively on mathematical formulas for performance calculation.,They analyze system behavior without requiring a clock variable.,They are limited to evaluating only batch processing systems.,B,The text states: 'Involves programming a model of the computer system with software data structures representing components.'
Which two primary methods are used for generating data to drive simulations?,Deterministic workloads and real-world system logs.,Random-number generators based on probability distributions and trace files.,Manual data input and user-defined parameters.,Analytical formulas and queueing-network analysis.,Regression testing and direct system implementation.,B,The text lists 'Random-number generator based on probability distributions' and 'Trace files' under 'Data generation'.
What are 'trace files' used for in the context of simulation-based algorithm evaluation?,To define the criteria for selecting a CPU-scheduling algorithm.,To confirm that system changes haven't introduced new bugs.,To record the sequence of actual events from a real system and use it to drive a simulation.,To apply Little's formula for calculating average queue lengths.,To program models of computer system components using abstract data structures.,C,"The text defines 'trace files' as: 'Monitoring real system to record sequence of actual events, then using this sequence to drive the simulation.'"
What is a key advantage of using trace files in simulations for algorithm comparison?,They significantly reduce the complexity of simulation design and coding.,"They provide excellent comparison of algorithms on identical real inputs, yielding accurate results for those inputs.","They allow for testing a broad, unlimited class of algorithms and distributions.",They eliminate the need for computer time and storage.,They can easily generalize performance improvements across different system environments.,B,"The text states: 'Advantages: Trace files provide excellent comparison of algorithms on identical real inputs, producing accurate results for those inputs.'"
What is a notable limitation of using simulations for evaluating scheduling algorithms?,They are less accurate than simple analytic methods.,They cannot use random number generators for data input.,"They can be expensive in terms of computer time and storage, and complex to design and debug.","They only apply to very specific, predetermined workloads.",They fail to account for any real-world variability.,C,"The text lists limitations: 'Can be expensive (computer time, storage for trace files), complex to design, code, and debug.'"
Which evaluation method is considered the most accurate for CPU-scheduling algorithms?,Deterministic modeling,Queueing models,Simulations driven by trace files,Actual implementation and testing under real operating conditions,Analytic evaluation using theoretical formulas,D,"The text states: 'The most accurate evaluation method: code the algorithm, integrate it into the OS, and test under real operating conditions.'"
What are some of the costs associated with evaluating a scheduling algorithm through actual implementation?,Only the cost of software licenses for simulation tools.,Primarily the mathematical complexity of deriving formulas.,"Coding the algorithm, modifying the OS, and extensive testing (often in virtual machines).",The expense of purchasing predetermined workload data sets.,The time spent on defining criteria and selecting parameters.,C,"The text lists costs as: 'Coding, modifying OS, testing (often in virtual machines).'"
What is the primary purpose of 'regression testing' in the context of algorithm implementation?,To measure the average waiting time of processes in a queue.,To compute the average arrival rate of new processes.,To confirm that changes haven't introduced new bugs or reintroduced old ones.,To simulate various system workloads using random number generators.,To analyze the performance of an algorithm against a theoretical workload.,C,"The text defines 'Regression testing' as: 'Confirms that the changes haven't made anything worse, and haven't caused new bugs or caused old bugs to be recreated (for example because the algorithm being replaced solved some bug and changing it caused that bug to reoccur).'"
Which of the following is identified as a challenge when implementing and testing a CPU-scheduling algorithm in a real operating system?,The simplicity of designing and coding complex algorithms.,The fixed and unchanging nature of real-world operating environments.,"The fact that scheduler performance can influence user behavior, or users may try to circumvent algorithms.",The guaranteed generalizability of performance improvements from API-based tuning.,The low cost and ease of obtaining exact input data for real systems.,C,"Challenges include: 'Scheduler performance can influence user behavior (e.g., breaking large processes into smaller ones if short processes are prioritized)' and 'Human/program behavior can attempt to circumvent scheduling algorithms.'"
How can flexible scheduling algorithms be altered by system managers or users?,Only through recompiling the entire operating system kernel.,By modifying the hardware registers directly.,Through commands like Solaris's dispadmin or various APIs (Java/POSIX/Windows).,They cannot be altered once implemented.,By adjusting the quantum in deterministic models.,C,"The text states: 'Flexible scheduling algorithms can be altered by system managers or users (e.g., Solaris's dispadmin command, Java/POSIX/Windows APIs).'"
What is a known downfall of API-based tuning for scheduling algorithms?,It is often too expensive to implement.,It requires highly complex mathematical derivations.,Performance improvements are often not generalizable.,It always introduces new bugs into the system.,It does not allow for real-time monitoring.,C,The text states: 'Downfall of API-based tuning: performance improvements are often not generalizable.'
What is the primary task of CPU scheduling?,Managing memory allocation for processes.,Selecting a waiting process from the ready queue and allocating the CPU to it.,Ensuring data consistency across multiple processors.,Handling I/O operations for active processes.,Terminating processes that have completed their execution.,B,CPU scheduling is the task of selecting a waiting process from the ready queue and allocating the CPU to it.
Which component is responsible for allocating the CPU to the selected process in a CPU scheduling system?,The kernel,The memory manager,The dispatcher,The process control block,The interrupt handler,C,The CPU is allocated to the selected process by the dispatcher.
In which type of scheduling algorithm can the CPU be taken away from a process before it voluntarily relinquishes control?,Nonpreemptive scheduling,Cooperative scheduling,Preemptive scheduling,Batch scheduling,"First-come, first-served (FCFS) scheduling",C,Preemptive scheduling is where the CPU can be taken away from a process.
What is a characteristic of almost all modern operating systems regarding CPU scheduling?,They primarily use nonpreemptive scheduling.,They require processes to voluntarily relinquish the CPU.,They are preemptive.,"They only use First-come, first-served (FCFS) scheduling.",They prioritize I/O-bound tasks over CPU-bound tasks in all scenarios.,C,Almost all modern operating systems are preemptive.
Which of the following is NOT one of the standard criteria used to evaluate CPU scheduling algorithms?,CPU utilization,Throughput,Memory footprint,Turnaround time,Response time,C,"The five criteria listed are CPU utilization, throughput, turnaround time, waiting time, and response time. Memory footprint is not among them."
"What is a primary disadvantage of First-come, first-served (FCFS) scheduling, despite its simplicity?",It requires complex CPU burst prediction.,It cannot be used in preemptive systems.,It can cause short processes to wait for very long processes.,It does not consider process priority.,It has the highest overhead among all algorithms.,C,FCFS scheduling is simple but can cause short processes to wait for very long processes.
Which CPU scheduling algorithm is provably optimal in providing the shortest average waiting time?,"First-come, first-served (FCFS)",Round-robin (RR),Shortest-job-first (SJF),Priority scheduling,Earliest-deadline-first (EDF),C,"Shortest-job-first (SJF) scheduling is provably optimal, providing the shortest average waiting time."
What is the main reason why implementing Shortest-job-first (SJF) scheduling is difficult in practice?,It requires constant context switching.,It does not support preemptive execution.,Predicting the length of the next CPU burst is difficult.,It suffers from starvation for long processes.,It is only suitable for batch systems.,C,Implementing SJF scheduling is difficult because predicting the length of the next CPU burst is difficult.
How does Round-robin (RR) scheduling handle a process that does not relinquish the CPU before its time quantum expires?,The process is terminated immediately.,The process's priority is reduced.,"The process is preempted, and another process is scheduled.",The time quantum is extended for that process.,The process is moved to a lower-priority queue.,C,"If the process does not relinquish the CPU before its time quantum expires, the process is preempted, and another process is scheduled."
"When multiple processes have the same priority in a priority scheduling system, how are they typically scheduled?",They are assigned random CPU times.,They are scheduled using only Round-robin (RR).,"They are scheduled using only First-come, first-served (FCFS).",They can be scheduled in FCFS order or using RR scheduling.,They are immediately terminated to resolve the conflict.,D,Processes with the same priority can be scheduled in FCFS order or using RR scheduling.
"Which scheduling approach partitions processes into several separate queues arranged by priority, with the scheduler executing processes in the highest-priority queue?",Round-robin scheduling,Multilevel queue scheduling,Proportional share scheduling,Earliest-deadline-first (EDF) scheduling,Shortest-job-first (SJF) scheduling,B,"Multilevel queue scheduling partitions processes into several separate queues arranged by priority, and the scheduler executes the processes in the highest-priority queue."
A key feature of multilevel queue scheduling is that:,All queues must use the same scheduling algorithm.,Processes can freely migrate between different queues.,Only batch processes can be in the highest-priority queue.,Different scheduling algorithms may be used in each queue.,It only supports nonpreemptive scheduling.,D,Different scheduling algorithms may be used in each queue in multilevel queue scheduling.
What is the distinguishing characteristic of multilevel feedback queues compared to standard multilevel queues?,They only allow nonpreemptive scheduling.,They only support two priority levels.,A process may migrate between different queues.,All processes must have a fixed priority.,They do not use time quantums.,C,"Multilevel feedback queues are similar to multilevel queues, except that a process may migrate between different queues."
"From the perspective of the operating system, what does each hardware thread on a multicore processor appear to be?",A virtual machine,A separate physical CPU,A logical CPU,An I/O device,A memory module,C,"From the perspective of the operating system, each hardware thread appears to be a logical CPU."
"While load balancing on multicore systems aims to equalize loads, what potential drawback is associated with migrating threads between cores?",It always reduces overall system throughput.,It increases the likelihood of deadlocks.,It may invalidate cache contents and therefore may increase memory access times.,It significantly reduces the number of available logical CPUs.,It can only be done in nonpreemptive systems.,C,Migrating threads between cores to balance loads may invalidate cache contents and therefore may increase memory access times.
What is the primary objective of soft real-time scheduling?,To provide absolute timing guarantees for tasks.,To ensure all tasks complete within their deadlines without fail.,To give priority to real-time tasks over non-real-time tasks.,To exclusively schedule periodic tasks.,To minimize the average waiting time for all processes.,C,Soft real-time scheduling gives priority to real-time tasks over non-real-time tasks.
What is a defining characteristic of hard real-time scheduling?,It allows for occasional deadline misses.,It focuses on maximizing CPU utilization.,It provides timing guarantees for real-time tasks.,It prioritizes non-real-time tasks when system load is high.,It is primarily used in general-purpose operating systems.,C,Hard real-time scheduling provides timing guarantees for real-time tasks.
Rate-monotonic real-time scheduling schedules periodic tasks using what type of policy?,"Dynamic priority, nonpreemptive","Static priority, nonpreemptive","Dynamic priority, preemptive","Static priority, preemptive",FCFS based on task arrival time,D,Rate-monotonic real-time scheduling schedules periodic tasks using a static priority policy with preemption.
How does Earliest-deadline-first (EDF) scheduling assign priorities to tasks?,Based on the shortest processing time.,According to their memory requirements.,Randomly to ensure fairness.,"According to deadline, where earlier deadlines mean higher priority.",Based on the task's arrival time.,D,"EDF scheduling assigns priorities according to deadline. The earlier the deadline, the higher the priority."
"If an application is allocated N shares of time in a proportional share scheduling system, what is it ensured of receiving?",Exactly N time quantums in every cycle.,A guaranteed minimum of N processes running simultaneously.,"N / T of the total processor time, where T is the total number of shares.",Priority over all applications with fewer than N shares.,Exclusive access to the CPU for a duration of N seconds.,C,"If an application is allocated N shares of time, it is ensured of having N / T of the total processor time."
What is the primary concept behind the Linux Completely Fair Scheduler (CFS)?,It uses a 32-level priority scheme.,It strictly adheres to FCFS for all tasks.,It assigns a proportion of CPU processing time to each task based on its `virtual runtime (vruntime)` value.,It schedules tasks based on their I/O burst length.,It partitions tasks into multiple queues with fixed priorities.,C,The Linux Completely Fair Scheduler (CFS) assigns a proportion of CPU processing time to each task based on the `virtual runtime (vruntime)` value associated with each task.
What type of priority scheme does Windows scheduling use to determine the order of thread scheduling?,"A nonpreemptive, 5-level priority scheme.","A preemptive, 32-level priority scheme.","A dynamic, 10-level priority scheme based on I/O.","A static, 6-class priority scheme.",A FCFS-based scheme with aging.,B,"Windows scheduling uses a preemptive, 32-level priority scheme to determine the order of thread scheduling."
"In Solaris scheduling, how are CPU-intensive threads generally prioritized and given time quantums?",They are assigned higher priorities and shorter time quantums.,They are assigned lower priorities and longer time quantums.,They are scheduled using an FCFS approach exclusively.,They are migrated between queues frequently to improve response time.,They are given guaranteed timing budgets.,B,"In Solaris, CPU-intensive threads are generally assigned lower priorities (and longer time quantums)."
"In Solaris scheduling, how are I/O-bound threads typically prioritized and given time quantums?","Lower priority, longer time quantums.","Higher priority, longer time quantums.","Lower priority, shorter time quantums.","Higher priority, shorter time quantums.","They are handled by a separate I/O scheduler, not CPU scheduler.",D,"In Solaris, I/O-bound threads are usually assigned higher priorities (with shorter time quantums)."
"What methods, besides analytical modeling, can be used to evaluate a CPU scheduling algorithm?",Only direct observation in a live production environment.,Modeling and simulations.,User surveys exclusively.,Hardware benchmark tests only.,Financial cost analysis.,B,Modeling and simulations can be used to evaluate a CPU scheduling algorithm.
