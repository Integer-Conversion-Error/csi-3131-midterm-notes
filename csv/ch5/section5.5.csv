Question,Option A,Option B,Option C,Option D,Option E,Answer,Explanation
Which concept is introduced in systems with multiple processing cores to distribute workload?,Context switching,Virtualization,Load sharing,Paging,Inter-process communication,C,Scheduling in systems with multiple processing cores introduces load sharing and increased complexity.
The term 'multiprocessor' commonly applies to which of the following architectures?,Single-core CPUs only,Dedicated I/O processors,"Multicore CPUs, multithreaded cores, NUMA systems, and heterogeneous multiprocessing",GPU clusters without a CPU,Embedded systems with a single microcontroller,C,"The term multiprocessor now applies to multicore CPUs, multithreaded cores, NUMA systems, and heterogeneous multiprocessing."
"In asymmetric multiprocessing, what is the role of the single main server processor?",It exclusively executes user code.,"It handles all scheduling, I/O, and system activities.",It is responsible only for load balancing.,It serves as a backup processor in case of failure.,It manages private per-processor ready queues.,B,"Asymmetric multiprocessing features a single main server processor handling all scheduling, I/O, and system activities, while other processors execute user code."
Which of the following is a potential drawback of asymmetric multiprocessing?,Increased data sharing complexity,The main server can become a performance bottleneck.,Poor utilization of other processors,Lack of support in modern operating systems,High switching cost due to pipeline flushing,B,A disadvantage of asymmetric multiprocessing is that the main server can become a performance bottleneck due to its centralized role.
Which characteristic best defines symmetric multiprocessing (SMP)?,A single processor manages all system resources.,Each processor is self-scheduling.,"Processors are dedicated to specific tasks (e.g., I/O, user code).",It only supports a common ready queue.,It is primarily used in embedded systems.,B,"Symmetric multiprocessing (SMP) is defined by each processor being self-scheduling, meaning they examine a ready queue and select a thread to run."
What is a potential issue with using a common ready queue in Symmetric Multiprocessing (SMP) systems?,Reduced processor affinity,It eliminates the need for locking mechanisms.,"It can lead to race conditions requiring locking, which may be a performance bottleneck.",It is only suitable for heterogeneous multiprocessing.,Processors become idle more frequently.,C,"When all threads are in a common ready queue in SMP, potential race conditions require locking, which can become a performance bottleneck."
Which strategy for organizing threads in SMP systems typically avoids locking overhead and benefits from processor affinity?,"A single, global dispatcher queue",Common ready queue,Private per-processor queues,Distributed hash table queues,FIFO queues for all processes,C,"Private per-processor queues avoid locking overhead and are common in SMP systems, benefiting from processor affinity."
What is the primary benefit of contemporary hardware using multicore processor chips compared to systems with separate physical CPU chips?,They support only coarse-grained multithreading.,They are slower but consume more power.,They are faster and consume less power.,They eliminate the need for an operating system.,They require only one level of scheduling.,C,"Multicore processors are faster and consume less power than systems with separate physical CPU chips, as stated in the text."
What is a 'memory stall' in the context of processor operation?,When a processor is idled by an operating system scheduler.,"When a processor waits for data from memory (e.g., due to cache miss), wasting significant time.",When a processor runs out of tasks to execute.,When a processor is unable to switch between hardware threads.,When a processor attempts to access protected memory.,B,"A memory stall occurs when a processor waits for data from memory (e.g., due to cache miss), wasting significant time."
How do many processor designs mitigate the issue of memory stalls?,By increasing cache size only.,By implementing single-threaded processing cores.,By implementing multithreaded processing cores with two or more hardware threads per core.,By reducing the number of cores on the chip.,By using only coarse-grained multithreading.,C,"To mitigate memory stalls, many designs implement multithreaded processing cores with two or more hardware threads per core, allowing the core to switch threads if one stalls."
Which term is synonymous with 'chip multithreading' (CMT)?,Asymmetric multiprocessing,Soft affinity,Hyper-threading or Simultaneous Multithreading (SMT),Push migration,NUMA scheduling,C,Chip multithreading (CMT) is also known as hyper-threading or simultaneous multithreading (SMT).
What distinguishes coarse-grained multithreading from fine-grained multithreading?,"Coarse-grained switches threads at instruction cycle boundaries, while fine-grained switches on long-latency events.","Coarse-grained has low switching cost due to architectural design, while fine-grained has high switching cost due to pipeline flushing.","Coarse-grained switches threads on long-latency events (e.g., memory stall) with high switching cost due to pipeline flushing, while fine-grained switches at a finer granularity (e.g., instruction cycle boundary) with low switching cost.","Coarse-grained is for single-core processors, while fine-grained is for multicore processors.","Coarse-grained is a software-only approach, while fine-grained is hardware-based.",C,"Coarse-grained multithreading switches threads on long-latency events (e.g., memory stall) with high switching cost due to pipeline flushing. Fine-grained multithreading switches threads at a finer granularity (e.g., instruction cycle boundary) with low switching cost due to architectural design."
"In multithreaded, multicore processors, what are the two levels of scheduling required?",Process scheduling and thread preemption.,"Operating system scheduling software threads onto hardware threads, and each core deciding which hardware thread to run.",User-level scheduling and kernel-level scheduling.,I/O scheduling and CPU scheduling.,Batch scheduling and interactive scheduling.,B,"Multithreaded, multicore processors require two levels of scheduling: the operating system schedules software threads onto hardware threads (logical CPUs), and each core decides which hardware thread to run."
What is the primary goal of load balancing in an SMP system?,To ensure all processors are idle as much as possible.,To keep the workload evenly distributed across all processors.,To force all threads to run on a single processor.,To eliminate the need for processor affinity.,To centralize all scheduling decisions on one core.,B,Load balancing attempts to keep the workload evenly distributed across all processors in an SMP system.
Load balancing is generally necessary for SMP systems with which type of ready queue organization?,Common ready queues,Private per-processor ready queues,Only systems using asymmetric multiprocessing,Systems with no hardware threads,Systems with only one level of scheduling,B,Load balancing is necessary for systems with private per-processor ready queues because otherwise some processors might become idle while others are overloaded. It is unnecessary for common ready queues as the load is naturally balanced by processors pulling from the shared queue.
"In load balancing, what is 'push migration'?",An idle processor requesting a task from a busy processor.,A task periodically checks processor loads and moves threads from overloaded to idle/less-busy processors.,A process being forced to switch contexts due to a higher priority task.,The operating system automatically assigning new processes to the least busy core.,A processor initiating a memory stall mitigation technique.,B,"With push migration, a specific task periodically checks the load on each processor and—if it finds an imbalance—evenly distributes the load by moving (or pushing) threads from overloaded to idle or less-busy processors."
What is 'pull migration' in the context of load balancing?,An overloaded processor sending threads to another processor.,A central scheduler reassigning threads to balance load.,An idle processor pulling a waiting task from a busy processor.,Threads autonomously migrating to different processors.,A process requesting to be moved to a specific processor.,C,Pull migration occurs when an idle processor pulls a waiting thread from a busy processor.
What is 'processor affinity'?,The ability of a processor to run multiple threads simultaneously.,A process's preference to run on the processor where it is currently running or recently ran.,The feature that allows multiple processors to share a common ready queue.,A mechanism to balance load across all available processors.,The process of disabling a processor to save power.,B,"Processor affinity means a process has an affinity for the processor on which it is currently running, largely due to the 'warm cache' benefit."
What is the primary benefit of processor affinity for a process?,It guarantees the process will always run on the same processor.,It allows the process to utilize more CPU cores concurrently.,"It benefits from a 'warm cache', where recently accessed data is already in the processor's cache.",It reduces the need for context switching.,It simplifies the operating system's scheduling algorithm.,C,"Processor affinity benefits from 'warm cache', meaning data recently accessed by the thread populates the processor's cache, leading to faster access times."
"Which type of processor affinity allows the OS to attempt to keep a process on the same processor but does not guarantee it, allowing for migration during load balancing?",Hard affinity,Absolute affinity,Soft affinity,Fixed affinity,Zero affinity,C,"Soft affinity is when the OS attempts to keep a process on the same processor but does not guarantee it, allowing migration during load balancing."
What is 'hard affinity'?,The default affinity setting for all processes.,When a process has an affinity for any available processor.,When system calls allow a process to specify a subset of processors on which it can run.,An affinity that is easily changed by the operating system.,An affinity that prevents any form of load balancing.,C,"Hard affinity is when system calls allow a process to specify a subset of processors on which it can run, effectively restricting its execution to those processors."
"In modern multicore NUMA systems, what is the tension between load balancing and minimizing memory access times?","Load balancing typically enhances processor affinity, while minimizing memory access times requires migration.","Load balancing often counteracts processor affinity benefits (which optimize memory access), as migrating a thread can incur cache invalidation costs.","Load balancing only applies to systems without NUMA, thus no tension exists.","Minimizing memory access times is solely a hardware concern, unrelated to load balancing.",Both load balancing and memory access optimization are achieved by keeping threads on their initial processor.,B,"Load balancing often counteracts processor affinity benefits because migrating a thread to another processor incurs the cost of invalidating and repopulating caches, which increases memory access times, creating a tension with the goal of minimizing memory access times."
What defines heterogeneous multiprocessing (HMP)?,Systems where different cores run different instruction sets.,Systems with cores that vary in clock speed and power management but run the same instruction set.,Systems where only one main processor handles all system tasks.,Systems designed exclusively for cloud computing.,"Systems with only a single, high-performance core.",B,Heterogeneous multiprocessing (HMP) refers to systems with cores that run the same instruction set but vary in clock speed and power management.
Which of the following statements correctly distinguishes heterogeneous multiprocessing (HMP) from asymmetric multiprocessing?,"HMP uses a single main server processor, while asymmetric multiprocessing uses multiple self-scheduling processors.","HMP aims for better power consumption, while asymmetric multiprocessing focuses on high performance.","In HMP, both system and user tasks can run on any core, whereas in asymmetric multiprocessing, only one processor accesses system data structures and others run user threads.","HMP supports only 'big' cores, while asymmetric multiprocessing supports 'LITTLE' cores.","HMP is an older concept, while asymmetric multiprocessing is newer.",C,"HMP allows both system and user tasks to run on any core, differentiating it from asymmetric multiprocessing where a single main server processor handles system activities and others execute user code."
What is the primary intention behind heterogeneous multiprocessing (HMP) designs like ARM's big.LITTLE architecture?,To maximize raw processing power across all tasks.,To simplify the operating system scheduler.,To achieve better power consumption management by assigning tasks to cores based on their demands.,To completely eliminate the need for memory caches.,To ensure all cores run at the same clock speed.,C,"The intention of HMP, as seen in big.LITTLE, is better power consumption management by assigning tasks to cores based on their demands (e.g., high-performance tasks to 'big' cores, background tasks to 'LITTLE' cores)."
"In ARM's big.LITTLE architecture, what is the typical role of the 'LITTLE' cores?","Handling short, high-performance tasks with higher energy consumption.",Managing I/O operations exclusively.,Executing longer background tasks with lower energy consumption.,Serving as redundant processors for fault tolerance.,Managing the main server processes for scheduling.,C,"In ARM's big.LITTLE architecture, 'LITTLE' cores are designed for lower energy consumption and are typically used for longer background tasks."
Which of the following operating systems is mentioned as supporting Heterogeneous Multiprocessing (HMP) scheduling?,MS-DOS,Windows 95,Windows 10,Unix System V,Classic Mac OS,C,Windows 10 is explicitly mentioned as supporting HMP scheduling.
