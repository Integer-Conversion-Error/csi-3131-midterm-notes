Question,Option A,Option B,Option C,Option D,Option E,Answer,Explanation
Which of the following best defines virtual memory?,A technique that compiles programs directly into physical memory addresses.,"A method to restrict processes to a fixed, small amount of physical memory.",A technique allowing the execution of processes that are not entirely loaded into physical memory.,A system feature that encrypts memory content for security purposes.,A type of cache memory used to speed up CPU access to frequently used data.,C,Virtual memory is defined as a technique that allows execution of processes even if they are not entirely present in physical memory.
What is considered a major advantage of utilizing virtual memory?,It eliminates the need for any physical RAM in a computer system.,"It restricts programs to a size smaller than the available physical memory, ensuring stability.",It allows programs to be much larger than the available physical memory.,It automates the process of data backup and recovery.,"It provides a direct, unmanaged interface to physical memory addresses.",C,A major advantage of virtual memory is that it allows programs to be larger than the actual physical memory available.
How does virtual memory abstract main memory?,"It converts it into a series of independent, disconnected memory modules.","It presents it as a large, uniform storage array.","It segments it into private, non-sharable regions for each process.",It treats it as a read-only device for security.,It limits its size to only the actively used portions of a program.,B,"Virtual memory abstracts main memory into a large, uniform storage array, simplifying the view for programmers."
Virtual memory fundamentally separates which two types of memory?,Cache memory from main memory.,Solid-state drives from hard disk drives.,Logical memory (programmer's view) from physical memory.,Read-only memory (ROM) from random-access memory (RAM).,Volatile memory from non-volatile memory.,C,A core aspect of virtual memory is the separation of logical memory (the programmer's conceptual view) from physical memory (the actual hardware memory).
Which benefit does virtual memory provide to programmers regarding memory management?,It requires programmers to manually manage memory allocation and deallocation.,It introduces complex constraints on program size.,It frees programmers from concern about the physical memory-storage limitations.,"It forces programmers to write smaller, more efficient code.",It eliminates the need for any memory access instructions in code.,C,"Virtual memory frees programmers from the burden of managing physical memory limitations, allowing them to write code for a much larger, abstract memory space."
Virtual memory facilitates which of the following sharing capabilities?,Sharing of CPU registers between unrelated processes.,Sharing of keyboard and mouse input exclusively.,"Sharing of files, system libraries, and implementation of shared memory regions.",Sharing of network bandwidth only.,Sharing of display monitor resolutions.,C,"Virtual memory enables processes to share common resources like files and system libraries efficiently, and it provides mechanisms for implementing shared memory for inter-process communication."
How does virtual memory typically impact the efficiency of process creation?,It significantly slows down process creation due to complex mapping overhead.,It has no impact on process creation efficiency.,It serves as an efficient mechanism for process creation.,It requires manual intervention to create new processes.,"It only allows for the creation of very small, simple processes.",C,"Virtual memory provides an efficient mechanism for process creation, especially by allowing pages to be shared during operations like fork()."
What is a potential drawback or challenge associated with virtual memory implementation?,It simplifies hardware requirements significantly.,It guarantees optimal performance under all circumstances.,Its implementation can be complex and may decrease performance if used carelessly.,It eliminates all forms of memory fragmentation.,"It requires all data to be stored on external, slow storage devices.",C,The text states that virtual memory implementation can be complex and may decrease performance if used carelessly.
"Under traditional memory management, what was a key characteristic regarding process execution?",Processes could execute even if only partially loaded into memory.,The entire process had to be in physical memory for execution.,Memory was dynamically allocated from a shared pool across networks.,Only the kernel's code was allowed in physical memory.,Programs were always smaller than the available cache memory.,B,Traditional memory management required the entire process to be loaded into physical memory before execution could begin.
What was the primary limitation imposed by traditional memory management concerning program size?,Program size was limited by the available hard disk space.,Program size was limited by the speed of the CPU.,Program size was limited by the amount of physical memory.,Program size was limited by network bandwidth.,Program size was limited by the number of CPU cores.,C,"In traditional memory management, the program's size was directly constrained by the amount of physical memory installed in the system."
Which of the following is a reason why real programs often do not need their entire code in physical memory at once?,All programs are designed to be run entirely from ROM.,"Error handling code is seldom executed, and arrays/lists often allocate more memory than needed.",Modern compilers automatically remove unused code segments before execution.,"Operating systems always swap out entire programs, regardless of need.","Physical memory is infinitely expandable, so code is always loaded entirely.",B,"Real programs often contain components like error handling code that are seldom executed, or data structures like arrays/lists that are allocated more memory than immediately required, making full loading unnecessary."
What is a direct benefit of allowing only a partial program to reside in memory during execution?,It requires less complex memory management hardware.,"Programs are not constrained by the physical memory size, enabling a large virtual address space.",It eliminates the need for secondary storage devices.,It guarantees that all programs will run faster.,It prevents any other programs from running concurrently.,B,"A key benefit is that programs are no longer constrained by the physical memory size, as they can utilize a much larger virtual address space."
How does executing only part of a program at a time affect CPU utilization and throughput?,It decreases CPU utilization and throughput by increasing I/O operations.,It has no effect on CPU utilization or throughput.,It increases CPU utilization and throughput because more programs can run concurrently.,"It dedicates the CPU to a single program, reducing multitasking efficiency.","It requires more physical memory per program, thus limiting concurrency.",C,"By using less physical memory per program, virtual memory allows more programs to run concurrently, which in turn increases CPU utilization and system throughput."
What is the impact of partial program execution in memory on response time and turnaround time?,It significantly increases both response time and turnaround time.,It slightly decreases response time but increases turnaround time.,It results in no increase in response time or turnaround time.,It guarantees an exponential decrease in both times.,It causes system freezes due to excessive swapping.,C,"Despite allowing more programs to run concurrently, partial program execution in memory does not result in an increase in response or turnaround time."
How does partial program execution in memory contribute to faster program execution?,By increasing the number of I/O operations for loading and swapping.,By reducing the need for CPU processing power.,By decreasing the amount of I/O required for loading and swapping.,By requiring programs to be compiled into smaller executable files.,By transferring entire programs to the GPU for processing.,C,"Less I/O is needed for loading and swapping when only parts of a program are in memory, which directly leads to faster program execution."
"In the context of virtual memory, what is the 'virtual address space'?",The physical memory available to the operating system kernel.,A logical view of how a process's storage is organized in memory.,The total capacity of all secondary storage devices in a system.,A dedicated high-speed cache for storing frequently accessed instructions.,The area of memory reserved exclusively for I/O operations.,B,"The virtual address space is defined as the logical view of how a process is stored in memory, as perceived by the programmer."
How is a process's logical memory typically organized in its virtual address space?,It is scattered randomly across different physical memory locations.,It starts at a logical address of 0 and appears to be contiguous.,It is always identical to its physical memory layout.,"It is organized as a single, large, read-only segment.",It requires manual linking to specific hardware addresses.,B,"Typically, a process views its memory as starting at logical address 0 and occupying a contiguous block, even if physically fragmented."
"In the context of virtual memory, how is physical memory organized?","As a single, contiguous block for all processes.","In page frames, which are not necessarily contiguous.","Only as a large, linear array directly accessible by the CPU.","As separate, isolated modules with no inter-connection.",As a volatile cache that empties after each operation.,B,"Physical memory is organized into fixed-size page frames, and these frames do not need to be contiguous to hold a process's logical pages."
What component is responsible for mapping logical pages to physical page frames in a virtual memory system?,The Central Processing Unit (CPU).,The Graphics Processing Unit (GPU).,The Memory-Management Unit (MMU).,The Disk Controller Unit (DCU).,The Network Interface Card (NIC).,C,"The Memory-Management Unit (MMU) is the hardware component that translates logical addresses generated by the CPU into physical addresses, mapping logical pages to physical page frames."
"In a typical virtual address space layout, how do the heap and stack sections grow?",Both the heap and stack grow upward towards higher addresses.,Both the heap and stack grow downward towards lower addresses.,"The heap grows upward, and the stack grows downward.","The heap grows downward, and the stack grows upward.",Neither the heap nor the stack changes size dynamically.,C,"Conventionally, the heap grows upwards towards higher memory addresses, while the stack grows downwards towards lower memory addresses."
What is the significance of the large blank space typically found between the heap and stack in a virtual address space?,It is unused memory that can never be allocated.,It is a reserved area for the operating system kernel only.,It is part of the virtual address space but requires physical pages only if the heap or stack grows into it.,It signifies memory that has been corrupted and cannot be used.,It is a fixed-size region used for inter-process communication.,C,This blank space is part of the virtual address space and acts as a buffer. Physical pages are only allocated for it on demand as the heap or stack expands into this region.
What does it mean for a virtual address space to be 'sparse'?,It means the address space is completely filled with contiguous data.,It describes an address space that contains many 'holes' or unallocated regions.,It indicates that the address space is exclusively used for read-only data.,It refers to an address space that is smaller than the physical memory.,It implies that the address space is encrypted for security purposes.,B,"A 'sparse' address space is characterized by having many 'holes' or unallocated, unused regions between allocated memory segments."
What is a benefit of sparse address spaces regarding memory growth?,They prevent the heap and stack from ever growing.,They ensure that physical memory is always fully utilized.,They allow holes to be filled dynamically as the stack or heap grows.,They eliminate the need for any memory allocation routines.,They require all memory to be pre-allocated at program startup.,C,"Sparse address spaces allow for flexibility, where the 'holes' can be dynamically filled with physical pages as the stack or heap sections of a process expand."
What dynamic capability is facilitated by sparse address spaces?,Static linking of all program libraries at compile time.,Exclusive use of shared memory segments for kernel processes.,Dynamic linking of libraries and shared objects during program execution.,Pre-allocation of all possible memory addresses for a program.,The ability to run programs without an operating system.,C,"Sparse address spaces are beneficial because they provide ample room for dynamic linking of libraries and other shared objects during a program's execution, by allowing them to be mapped into the 'holes'."
"How does virtual memory allow system libraries (e.g., standard C library) to be shared among processes?",By duplicating the entire library code for each process.,"By storing them exclusively on the hard drive, accessed by all processes.","By mapping them into the virtual address space of multiple processes, sharing physical pages.",By requiring each process to recompile the library from source code.,By embedding the library directly into the CPU's firmware.,C,"Virtual memory enables system libraries to be shared by mapping their physical pages into the virtual address space of multiple processes, rather than having separate copies."
What characteristic allows physical pages of shared libraries to be efficiently utilized by multiple processes?,They are typically mapped as write-only.,They are always copied into each process's private memory.,"They are mapped as read-only, allowing physical pages to be shared.",They are stored on a network file system and accessed remotely.,They are encrypted and decrypted by each process independently.,C,"Shared libraries are typically mapped as read-only, which means multiple processes can safely share the same physical pages without fear of one process modifying the library for others."
"Beyond libraries, for what other purpose can processes share memory regions using virtual memory?",To directly access each other's private data segments without permission.,To decrease system security by exposing memory content.,For inter-process communication (IPC).,To perform hardware diagnostics and repairs.,To increase the system's physical memory capacity.,C,"Processes can utilize virtual memory to share specific memory regions, providing an efficient mechanism for inter-process communication (IPC)."
"How does virtual memory contribute to speeding up process creation, particularly with operations like fork()?",By requiring the parent process to be entirely copied before creation.,By allowing physical pages to be shared between the parent and child processes initially.,By always creating completely independent and separate memory spaces immediately.,By offloading process creation entirely to a separate hardware unit.,By disabling memory protection during creation.,B,"During process creation (e.g., via fork()), virtual memory enables faster execution by allowing the parent and child processes to initially share the same physical pages, avoiding immediate full copying."
"In the context of memory management, what does the term 'sparse' also describe besides an address space with holes?",A type of physical memory chip with limited capacity.,"A page table with noncontiguous, scattered entries.",A CPU register file that is rarely used.,A process that consumes very little CPU time.,A network protocol designed for minimal data transmission.,B,"The glossary defines 'sparse' as describing both an address space with many holes and a page table with noncontiguous, scattered entries."
How does the Linux operating system manage virtual memory?,By allocating fixed-size partitions to processes.,Through a segmentation-only approach.,Using demand paging.,With a pure swapping mechanism.,Primarily via static memory allocation.,C,The text states that 'Linux manages virtual memory using demand paging.'
From which source does Linux allocate pages for virtual memory management?,A dedicated swap partition only.,A dynamically growing heap segment.,A list of free frames.,Pre-allocated contiguous memory blocks.,The user-space memory pool directly.,C,Linux 'Allocates pages from a list of free frames.'
What kind of global page-replacement policy does Linux implement?,"First-In, First-Out (FIFO) with a global scope.",Optimal Page Replacement Algorithm.,Least Recently Used (LRU) with exact tracking.,A global LRU-approximation clock algorithm (second-chance).,Most Frequently Used (MFU) across all processes.,D,Linux uses a 'Global page-replacement policy: similar to LRU-approximation clock algorithm (second-chance).'
Which two page lists does the Linux kernel maintain for virtual memory management?,kernel_list and user_list,read_list and write_list,active_list and inactive_list,cache_list and buffer_list,primary_list and secondary_list,C,Linux 'Maintains two page lists: active_list and inactive_list.'
"In Linux, what kind of pages are considered to be in use and are held in the `active_list`?",Pages that are currently being written to secondary storage.,Pages that have not been referenced recently and are candidates for reclamation.,Pages considered in use.,Pages that store kernel internal data structures only.,Pages reserved for future allocations.,C,The `active_list` holds 'pages considered in use.'
What is the primary purpose of the `inactive_list` in Linux's virtual memory management?,To store pages that are part of the kernel's critical working set.,To hold pages that are not recently referenced and are eligible for reclamation.,To buffer pages before they are moved to the `active_list`.,To maintain a history of all page faults.,To temporarily store pages that have been swapped out.,B,"The `inactive_list` contains 'pages not recently referenced, eligible for reclamation.'"
What is the function of the 'accessed' bit for a page in Linux's virtual memory management?,It indicates if the page has been modified since it was last written to disk.,It controls read/write permissions for the page.,It is set when the page is referenced.,It marks the page as exclusively for kernel use.,It signifies that the page is currently being swapped out.,C,Each page has an 'accessed' bit (set when referenced).
"When a page is first allocated in Linux, what happens to its `accessed` bit and its placement in the page lists?","The `accessed` bit is cleared, and it's added to the front of the `inactive_list`.","The `accessed` bit is set, and it's added to the rear of the `active_list`.","The `accessed` bit is set, and it's added to the front of the `active_list`.","The `accessed` bit is cleared, and it's added to the rear of the `inactive_list`.","The `accessed` bit remains unchanged, and it's placed in a separate allocation queue.",B,"When 'Page first allocated: accessed bit set, added to rear of active_list.'"
"If a page that is already in the `active_list` in Linux is referenced again, what is the effect on that page?","Its `accessed` bit is cleared, and it moves to the front of the list.",It immediately migrates to the `inactive_list`.,"Its `accessed` bit is set, and it moves to the rear of the list.","Its `accessed` bit is set, but its position in the list remains unchanged.",It triggers a page-out operation.,C,"If a 'Page in active_list referenced: accessed bit set, moves to rear of list.'"
What periodic action does Linux perform on the `accessed` bits of pages located in the `active_list`?,They are inverted.,They are reset.,They are copied to a separate log.,They are randomly toggled.,They are permanently locked.,B,Periodically: 'accessed bits for pages in active_list reset.'
"What happens to the least recently used page, which is at the front of the `active_list`, in Linux's memory management?",It is immediately swapped out to disk.,It is promoted to a higher priority queue.,It remains in the `active_list` indefinitely.,It may migrate to the rear of the `inactive_list`.,Its `accessed` bit is automatically set to 1.,D,"The 'Least recently used page: at front of active_list, may migrate to rear of inactive_list.'"
"In Linux, if a page that is currently in the `inactive_list` is referenced, where does it move?",It is immediately reclaimed to the free list.,It moves to the front of the `inactive_list`.,It moves back to the rear of the `active_list`.,It becomes part of a kernel-reserved memory area.,It remains in the `inactive_list` but its `accessed` bit is set.,C,If a 'Page in inactive_list referenced: moves back to rear of active_list.'
When does Linux move pages from the front of the `active_list` to the `inactive_list` to make them eligible for reclamation?,When the `inactive_list` grows larger than the `active_list`.,"Periodically, regardless of list sizes.",When the `active_list` grows larger than the `inactive_list`.,When total free memory reaches a critical high threshold.,Only when a process explicitly requests memory freeing.,C,When the 'active_list grows larger than inactive_list: pages from front of active_list move to inactive_list (eligible for reclamation).'
What is the name of the page-out daemon process in the Linux kernel?,pagedaemon,memmgrd,kswapd,vmmgr,activepaged,C,The Linux kernel has a 'page-out daemon process kswapd.'
What action does the Linux `kswapd` process take if the amount of free memory falls below a certain threshold?,It signals all active processes to release memory.,It immediately swaps out the largest idle process.,It scans the `inactive_list` and reclaims pages for the free list.,It increases the size of the `active_list`.,It requests more physical memory from hardware.,C,"If free memory falls below threshold: 'kswapd scans inactive_list, reclaims pages for free list.'"
Which system architectures and bit systems does Windows 10 support according to the text?,Only 64-bit Intel systems.,"32-bit and 64-bit systems, including Intel and ARM.","Mainly ARM, with limited Intel support.",Only 32-bit systems for legacy applications.,Only specialized server architectures.,B,"Windows 10 'supports 32- and 64-bit systems (Intel, ARM).'"
What are the default virtual address space and physical memory limits for 32-bit systems running Windows?,"1 GB virtual address space, 2 GB physical memory.","2 GB virtual address space (extendable to 3 GB), 4 GB physical memory.","4 GB virtual address space, 8 GB physical memory.","128-TB virtual address space, 24 TB physical memory.","Unlimited virtual address space, 4 GB physical memory.",B,"32-bit systems have 'default 2 GB virtual address space (extendable to 3 GB), 4 GB physical memory.'"
"What are the typical virtual address space and physical memory limits for 64-bit systems running Windows, according to the text?","4 GB virtual address space, 16 GB physical memory.","2 GB virtual address space, 4 GB physical memory.","128-TB virtual address space, up to 24 TB physical memory (Windows Server up to 128 TB).","1 TB virtual address space, 64 GB physical memory.","Unlimited virtual address space, 128 TB physical memory.",C,"64-bit systems have '128-TB virtual address space, up to 24 TB physical memory (Windows Server up to 128 TB).'"
Which of the following memory management features are implemented by Windows?,Fixed partitioning and segmentation without demand paging.,Pure swapping and contiguous allocation.,"Shared libraries, demand paging, copy-on-write, paging, memory compression.",Pre-paging and static allocation only.,Only demand paging and shared libraries.,C,"Windows 'Implements: shared libraries, demand paging, copy-on-write, paging, memory compression.'"
How does Windows implement demand paging in its virtual memory system?,Through a strict FIFO algorithm.,Using a technique called 'prefetching'.,With 'clustering'.,By employing an optimal page replacement strategy.,Via direct memory access (DMA) only.,C,Windows' 'Virtual memory: demand paging with clustering.'
What is the definition of 'clustering' in the context of Windows' virtual memory management?,Grouping similar processes together for efficient scheduling.,Bringing in multiple instances of the same shared library page.,Handling page faults by bringing in the faulting page plus several immediately preceding/following pages.,Consolidating fragmented free memory spaces.,Allowing processes to share physical memory frames without logical contiguity.,C,Clustering is defined as handling 'page faults by bringing in faulting page + several immediately preceding/following pages.'
"When a page fault occurs for a data page in Windows, how many pages are brought into memory due to clustering?",1 page (the faulting page only).,2 pages (the faulting page + one more).,3 pages (faulting + one before + one after).,5 pages (faulting + two before + two after).,"7 pages, regardless of page type.",C,For a 'Data page: 3 pages (faulting + one before + one after).'
"For page faults other than data pages, how many pages does Windows typically bring into memory using clustering?",1 page.,3 pages.,5 pages.,7 pages.,The number varies randomly.,D,For 'Other page faults: 7 pages.'
What is identified as a key component of memory management in Windows?,Strict static partitioning.,Direct hardware register manipulation.,Working-set management.,Exclusive use of segmentation.,Pre-allocation of all required memory.,C,A 'Key component: working-set management.'
"When a process is created in Windows, what are the default `working-set minimum` and `working-set maximum` values assigned?","Minimum 10 pages, Maximum 100 pages.","Minimum 25 pages, Maximum 250 pages.","Minimum 50 pages, Maximum 345 pages.","Minimum 100 pages, Maximum 500 pages.",No default values; they must be configured manually.,C,Process creation: 'assigned working-set minimum (50 pages) and working-set maximum (345 pages).'
"In Windows, what does the term 'working-set minimum' signify?",The maximum number of pages a process is allowed to use.,The minimum number of pages guaranteed to a process in memory.,The total physical memory installed in the system.,The number of pages currently being swapped out for a process.,A threshold below which processes are terminated.,B,'Working-set minimum: minimum pages guaranteed in memory.'
What does the 'working-set maximum' represent in Windows memory management?,The minimum number of frames a process requires to run.,The total available virtual address space.,The maximum number of pages allowed if sufficient memory.,The number of pages a process is currently using.,A fixed limit that cannot be exceeded under any circumstances.,C,'Working-set maximum: maximum pages allowed if sufficient memory.'
What is a characteristic of 'hard working-set limits' in Windows?,They always override all other memory management policies.,"If configured, the working-set minimum and maximum values may be ignored.",They automatically expand a process's working set.,They only apply to kernel processes.,They guarantee a process will never exceed its maximum.,B,"'Hard working-set limits: if configured, values may be ignored.'"
Can a process in Windows grow its memory usage beyond its `working-set maximum`?,"No, this limit is absolute.",Only if explicitly approved by a system administrator.,"Yes, if sufficient memory is available.",Only for critical system processes.,Only if a 'hard working-set limit' is set.,C,A 'Process can grow beyond maximum if memory available.'
Under what circumstances can memory allocated to a process in Windows shrink below its `working-set minimum`?,Never; the minimum is a strict guarantee.,Only if the process voluntarily releases memory.,During times of high memory demand.,When the process is idle for an extended period.,If the system crashes unexpectedly.,C,Memory allocated to a process 'can shrink below minimum during high demand.'
What page replacement algorithm and policies does Windows utilize?,FIFO with only a global policy.,Optimal with only a local policy.,LRU-approximation clock algorithm (second-chance) with local and global policies.,Most Recently Used (MRU) with both local and global policies.,Random page replacement without specific policies.,C,Page replacement: 'LRU-approximation clock algorithm (second-chance) with local and global policies.'
What does the Windows Virtual Memory Manager maintain to facilitate page allocation?,"A single, fixed-size buffer for all processes.",A detailed log of all page accesses.,A free page frames list with a threshold.,Separate free lists for each process.,A table of pre-assigned physical addresses.,C,The 'Virtual memory manager: maintains free page frames list with threshold.'
"If a process in Windows incurs a page fault while its working set is below its `working-set maximum`, what action is taken?",The process is immediately swapped out.,A page is allocated from the free list.,It triggers an automatic working-set trimming.,The system pauses until more memory becomes available.,A page is forcefully removed from another process.,B,If a 'Page fault for process below working-set maximum: allocates page from free list.'
"When a process in Windows incurs a page fault while already at its `working-set maximum`, but sufficient free memory is available, what happens?","The page fault is denied, and the process stalls.","A free page is allocated, allowing the process to grow beyond its maximum.",The process's `working-set maximum` is automatically increased.,The system immediately triggers global page replacement.,The faulting page is fetched from secondary storage without affecting working set.,B,"If 'Process at working-set maximum, page fault, sufficient memory: allocated free page (grows beyond maximum).'"
"If there is insufficient free memory in Windows and a page fault occurs, which policy is applied to choose a page for replacement from the faulting process's working set?",A global FIFO policy.,A local LRU policy.,A random page selection policy.,A system-wide least frequently used policy.,No page replacement occurs until more memory is freed manually.,B,If there's 'Insufficient free memory: kernel selects page from process's working set for replacement (local LRU policy).'
What global replacement tactic is activated in Windows when free memory falls below a certain threshold?,Manual memory defragmentation.,Automatic working-set trimming.,Demand paging acceleration.,Page prefetching enhancement.,Process prioritization boost.,B,If 'Free memory falls below threshold: global replacement tactic automatic working-set trimming.'
What is the primary objective of 'automatic working-set trimming' in Windows?,To increase the `working-set maximum` for all processes.,To evaluate pages allocated to processes and reclaim memory.,To reorder pages within a process's working set for faster access.,To prevent any process from ever exceeding its `working-set minimum`.,To log all memory access patterns for debugging.,B,'Automatic working-set trimming: evaluates pages allocated to processes.'
"How does 'automatic working-set trimming' typically reclaim pages in Windows, and which processes are targeted first?","It removes pages from active processes until free memory is sufficient, targeting smaller processes first.",It randomly removes pages from all processes until the memory threshold is met.,"It removes pages from processes with more than their `working-set minimum` until sufficient memory or the process reaches its minimum, prioritizing larger, idle processes.",It only trims pages from processes that have exceeded their `working-set maximum`.,It only reclaims kernel-allocated pages.,C,"If process has more pages than working-set minimum: 'removes pages until sufficient memory or process reaches minimum. Larger, idle processes targeted before smaller, active processes.'"
How long does 'automatic working-set trimming' continue in Windows?,It stops immediately once any page is reclaimed.,"It continues until sufficient free memory is achieved, even if processes shrink below their `working-set minimum`.",It runs only once per hour to minimize overhead.,It continues until all processes reach their `working-set maximum`.,It stops as soon as a single process reaches its `working-set minimum`.,B,"Trimming 'continues until sufficient free memory, even if below working-set minimum.'"
On which types of processes does Windows perform memory trimming?,Only user-mode processes.,Only system processes.,Only processes with `hard working-set limits` configured.,Both user-mode and system processes.,Only processes that are currently inactive.,D,Windows performs trimming on 'user-mode and system processes.'
"When a thread incurs a page fault in Solaris, how does the kernel initially assign a page?",It allocates a page from the thread's private memory pool.,It requests the page directly from the disk controller.,It assigns a page from the free list.,It triggers a process swap immediately.,It attempts to re-use a recently freed page from another process.,C,When a 'Thread incurs page fault: kernel assigns page from free list.'
What is an imperative goal for the Solaris kernel regarding memory management?,To keep all physical memory completely full.,To keep sufficient free memory.,To prioritize disk I/O over memory availability.,To minimize the use of virtual memory.,To ensure every process has its maximum working set at all times.,B,It is 'Imperative: kernel keeps sufficient free memory.'
"In Solaris, what is the `lotsfree` parameter used for, and what is its typical value?","It defines the maximum number of free pages allowed, typically 1/2 physical memory.","It is the threshold to begin paging, typically 1/64 of physical memory.",It represents the total amount of available swap space.,It dictates the minimum number of pages a process can hold.,It controls the rate at which pages are swapped out to disk.,B,'lotsfree parameter: threshold to begin paging (typically 1/64 physical memory).'
How often does the Solaris kernel check the amount of free memory against the `lotsfree` parameter?,Once per second.,Four times per second.,Every minute.,Only when a page fault occurs.,"Continuously, without a specific interval.",B,The 'Kernel checks free memory vs. lotsfree four times per second.'
When does the `pageout` process typically start in Solaris?,When a new process is created.,If the number of free pages falls below the `lotsfree` threshold.,During system startup.,When CPU utilization drops below a certain level.,Only on explicit user request.,B,If 'free pages < lotsfree: pageout process starts.'
"Which algorithm is the Solaris `pageout` process similar to, and what distinct mechanism does it employ?","FIFO, using a single pointer.","LRU, using a linked list.","Second-chance algorithm, using two hands.","Optimal, using predictive analysis.","Random, using a hash table.",C,"The 'Pageout process: similar to second-chance algorithm, uses two hands.'"
What is the specific action performed by the 'front hand' in Solaris's `pageout` process?,It examines reference bits and appends pages to the free list if needed.,It scans all pages and sets their reference bit to 0.,It writes modified pages to secondary storage.,It reclaims pages from the free list for active processes.,It calculates the average `scanrate`.,B,"The 'Front hand: scans all pages, sets reference bit to 0.'"
What is the role of the 'back hand' in Solaris's `pageout` process?,It initializes all page reference bits to 1.,It exclusively handles the writing of clean pages to disk.,It scans all pages and resets their reference bit to 0.,"It examines the reference bit; if still 0, it appends the page to the free list, writing to secondary storage if modified.",It determines the `scanrate` based on system load.,D,"The 'Back hand: examines reference bit; if still 0, appends page to free list, writes to secondary storage if modified.'"
How does Solaris manage 'minor page faults'?,It always swaps out the entire process.,"It treats them the same as major page faults, requiring disk I/O.",A process reclaims a page from the free list if it was accessed before being reassigned.,It reloads the entire program into memory.,"It ignores them, expecting the application to handle them.",C,Solaris 'manages minor page faults: process reclaims page from free list if accessed before reassigned.'
What parameter does the Solaris `pageout` algorithm use to control the rate at which pages are scanned?,freethreshold,scanspeed,scanrate,pagerate,sweepfrequency,C,The 'Pageout algorithm: uses parameters to control scanrate (pages per second).'
What is the typical range for the `scanrate` in Solaris's pageout algorithm?,From 1 page/sec to 100 pages/sec.,From `slowscan` to `fastscan`.,"A fixed value, determined at boot time.",From 0 pages/sec to system maximum RAM size.,It is dynamically calculated and has no fixed range.,B,The 'scanrate ranges from slowscan to fastscan.'
"At what rate does Solaris begin scanning pages when free memory falls below `lotsfree`, and what is its default value?","At `fastscan`, with a default of 8,192 pages/sec.","At `slowscan`, with a default of 10 pages/sec.","At `slowscan`, with a default of 100 pages/sec.","At a dynamic rate, starting from 0 pages/sec.",It starts scanning immediately at maximum speed.,C,When 'Free memory falls below lotsfree: scanning at slowscan (default 100 pages/sec).'
What is the maximum value for `fastscan` in Solaris's pageout process?,100 pages/sec.,1024 pages/sec.,4096 pages/sec.,8192 pages/sec.,Limited only by physical memory size.,D,"Progresses to 'fastscan (total physical pages/2, max 8,192 pages/sec).'"
What system parameter determines the distance between the 'front hand' and 'back hand' in Solaris's pageout process?,scanrate,handspread,page_distance,twin_offset,proximity_param,B,The 'Distance between hands: determined by handspread system parameter.'
What factors determine the time between a page's reference bit being cleared by the front hand and checked by the back hand in Solaris's pageout process?,The current CPU utilization only.,The amount of free memory and the system clock.,The `scanrate` and the `handspread` system parameter.,The number of active processes and their priority.,The total physical memory and swap space.,C,The 'Time between clearing and checking bit: depends on scanrate and handspread.'
"If free memory in Solaris falls below the `desfree` threshold, how often does the `pageout` process run?",Once per second.,Four times per second.,10 times per second.,100 times per second.,Continuously without a fixed interval.,D,If free memory falls below desfree (desired free memory): 'pageout runs 100 times per second.'
What is the primary goal of the Solaris kernel in relation to the `desfree` parameter?,To ensure `desfree` memory is always consumed by active processes.,To keep at least `desfree` memory available.,To use `desfree` as a trigger for process termination.,To increase `desfree` during periods of high memory demand.,To ignore `desfree` if other parameters are met.,B,The 'Goal: keep at least desfree memory available.'
"Under what condition will the Solaris kernel swap processes to free all their pages, and what kind of processes does it look for?","If `minfree` is exceeded for 10 seconds, targeting critical system processes.","If unable to maintain `desfree` for a 30-second average, looking for idle processes.","If `lotsfree` is never reached, targeting active processes.","Only when the system explicitly runs out of physical memory, targeting random processes.","When a user requests a full system reset, targeting all processes.",B,If 'unable to maintain desfree for 30-second average: kernel swaps processes (freeing all pages). Kernel looks for idle processes.'
When is the `pageout` process called for every new page request in Solaris?,If the `scanrate` is at its maximum.,If the system is unable to maintain `minfree`.,Only during system boot.,If `desfree` is consistently maintained.,When a process attempts to grow beyond its allocated swap space.,B,If system unable to maintain `minfree`: 'pageout process called for every new page request.'
"Which type of pages does the Solaris page-scanning algorithm intentionally skip, even if they would otherwise be eligible for reclamation?",Temporary file pages.,User data pages.,Shared library pages.,Kernel stack pages.,Recently written pages.,C,The 'Page-scanning algorithm skips shared library pages (even if eligible).'
How does Solaris distinguish between different types of pages for memory management purposes?,"It only treats all pages equally, regardless of their origin.",It categorizes pages based on their age in memory.,It distinguishes between pages for processes and regular data files.,It differentiates between read-only and writeable pages only.,It uses distinct lists for kernel pages and user pages exclusively.,C,Solaris 'Distinguishes between pages for processes and regular data files.'
"What term describes Solaris's approach of prioritizing the selection of victim frames based on criteria, such as avoiding shared library pages?",Selective paging,Adaptive paging,Priority paging,Intelligent paging,Exclusive paging,C,This approach is 'Known as priority paging.'
What is the definition of 'clustering' in memory management?,A method to group CPU cores for parallel processing.,Paging in a group of contiguous pages when a single page is requested via a page fault.,The process of reorganizing fragmented disk space.,Creating multiple virtual machines on a single physical server.,Combining small memory blocks into larger ones.,B,Clustering is defined as 'Paging in a group of contiguous pages when a single page is requested via a page fault.'
"In Windows, what does the term 'working-set minimum' define?",The maximum number of frames a process is allowed to use.,The total amount of available physical memory.,The minimum number of frames guaranteed to a process in Windows.,The recommended starting size for a process's memory.,The number of frames a process uses when it's idle.,C,Working-set minimum is defined as 'Minimum number of frames guaranteed to a process in Windows.'
What does the term 'working-set maximum' mean in Windows memory management?,The minimum number of frames guaranteed to a process.,The maximum amount of virtual address space a process can access.,The maximum number of frames allowed to a process in Windows.,The number of frames a process is currently using.,A fixed memory allocation that cannot be changed.,C,Working-set maximum is defined as 'Maximum number of frames allowed to a process in Windows.'
What does a 'hard working-set limit' represent in Windows?,The minimum amount of physical memory a process must use.,The total available physical memory in the system.,A suggestion for memory usage that can be exceeded.,The maximum amount of physical memory a process is allowed to use in Windows.,The total amount of virtual memory available to all processes.,D,Hard working-set limit is defined as 'Maximum amount of physical memory a process is allowed to use in Windows.'
What is 'automatic working-set trimming' in Windows?,Increasing working-set frames for processes during high demand.,A mechanism to prioritize I/O operations for active processes.,Decreasing working-set frames for processes if minimum free memory threshold is reached.,A process of defragmenting memory automatically.,Automatically adjusting the swap file size based on system needs.,C,"Automatic working-set trimming is defined as 'In Windows, decreasing working-set frames for processes if minimum free memory threshold is reached.'"
What is 'priority paging'?,A system where all pages are treated equally for replacement.,A technique to randomly select victim frames for replacement.,"Prioritizing selection of victim frames based on criteria, e.g., avoiding shared library pages.",A method to pre-load pages into memory before they are needed.,Assigning higher priority to processes with smaller working sets.,C,"Priority paging is defined as 'Prioritizing selection of victim frames based on criteria, e.g., avoiding shared library pages.'"
What is the fundamental characteristic of virtual memory?,It is a hardware component that caches frequently accessed data.,It directly expands the physical RAM capacity of a system.,It abstracts physical memory into an extremely large uniform array of storage.,It is a software layer that eliminates the need for physical memory.,It manages CPU scheduling and process prioritization.,C,Virtual memory is defined as abstracting physical memory into an 'extremely large uniform array of storage'.
Which of the following is a key benefit provided by virtual memory?,Programs must be entirely loaded into physical memory before execution.,It restricts processes from sharing memory for security reasons.,It enables a program to be larger than the available physical memory.,It eliminates the need for any form of backing store for program data.,It solely focuses on optimizing disk I/O operations.,C,One of the listed benefits of virtual memory is that 'Program can be larger than physical memory'.
How does virtual memory impact the requirement for a program to be fully in memory?,It ensures the entire program is always resident in memory.,"It requires the program to be entirely in memory, but in a compressed format.",It means the program does not need to be entirely in memory to execute.,"It forces the program to be split into multiple smaller, independent executables.","It only loads the program's data segments, leaving code on disk.",C,A stated benefit is that 'Program does not need to be entirely in memory'.
"From the perspective of processes, what is a benefit facilitated by virtual memory?",Processes are completely isolated and cannot communicate or share resources.,Processes can share memory with each other.,Processes are forced to use only contiguous blocks of physical memory.,Processes can only access memory allocated to the kernel.,Processes are always allocated the same fixed amount of memory.,B,The text lists 'Processes can share memory' as a benefit of virtual memory.
In what way does virtual memory contribute to process creation?,It makes process creation more complex and time-consuming.,It allows processes to be created more efficiently.,It eliminates the need for distinct address spaces for new processes.,It requires manual memory mapping for every new process.,It restricts the number of concurrently active processes.,B,A benefit mentioned is that 'Processes can be created more efficiently'.
What is the core principle behind 'demand paging'?,All pages for a program are pre-loaded into memory before execution.,Pages are loaded into memory only when they are explicitly requested by the user.,Pages are loaded into memory only when they are demanded during program execution.,Pages are loaded based on a predictive algorithm to anticipate future needs.,"Pages are never loaded into memory; instead, they are accessed directly from backing store.",C,Demand paging is defined as 'pages loaded only when demanded during program execution'.
What is a direct consequence of using demand paging?,All pages of a program will eventually be loaded into memory.,Pages that are never demanded during execution are never loaded into memory.,The system experiences an increased number of page faults for all page accesses.,Programs must be designed to fit entirely within physical memory.,Memory access becomes significantly slower due to constant disk I/O.,B,"The text states, 'Pages never demanded are never loaded,' which is a direct consequence of demand paging."
When does a 'page fault' occur?,When a page is successfully written back to the backing store.,When a program attempts to access a page that is not currently in memory.,When the operating system is unable to allocate more physical memory.,When a program tries to access a memory location outside its allowed range.,When a new process is created and requires memory allocation.,B,A page fault 'occurs when page not in memory is accessed'.
"Upon the occurrence of a page fault, what is the required action from the system?",The process that caused the fault is immediately terminated.,The page is marked as invalid and becomes inaccessible.,The page must be brought from the backing store into an available page frame.,All other pages in memory are reloaded to prevent further faults.,The system attempts to compress the faulting page to fit it into memory.,C,"The text indicates that when a page fault occurs, the 'Page must be brought from backing store into available page frame'."
Which statement accurately describes the initial state when 'copy-on-write' is used for a child process?,The child process receives an entirely separate and independent copy of the parent's memory space.,The child process shares the same address space as the parent.,"The child process can only execute code from the parent, not its own data.","The parent process's memory is locked, preventing any modification by the child.",The child process immediately copies all read-only pages from the parent.,B,Copy-on-write specifies that 'child process shares same address space as parent'.
"Under the 'copy-on-write' mechanism, when is an actual copy of a shared page created?",Only when the parent process writes to the page.,Only when the child process writes to the page.,When either the child or the parent process attempts to modify the page.,Immediately upon the creation of the child process.,When the operating system needs to free up physical memory.,C,"The text explains, 'If child or parent modifies page, copy of page is made'."
When does a page-replacement algorithm typically select an existing page to replace?,During program initialization.,When a program explicitly requests a page swap.,When available memory is low.,After every successful page load from backing store.,Only when a process terminates.,C,Page-replacement algorithms are invoked 'When available memory low'.
What is the primary objective of a page-replacement algorithm?,To pre-load all necessary pages into memory for faster access.,To prevent any page faults from occurring during execution.,To select an existing page in memory to be swapped out.,To determine the optimal size for memory pages.,To manage the allocation of contiguous memory blocks.,C,The algorithm 'selects existing page to replace'.
Which set of algorithms are explicitly mentioned as page-replacement algorithms?,"LIFO, Random, Adaptive","FIFO, Optimal, LRU","Best Fit, First Fit, Worst Fit","Priority, Round Robin, Shortest Job First","Merge Sort, Quick Sort, Bubble Sort",B,"The text lists 'FIFO, optimal, LRU' as page-replacement algorithms."
Why is implementing pure LRU (Least Recently Used) often considered impractical?,It frequently leads to 'thrashing' due to its aggressive replacement policy.,It is conceptually too complex for modern operating systems.,It requires significant hardware or computational overhead to perfectly track page usage.,It always results in higher page fault rates compared to simpler algorithms.,It only works effectively with very small amounts of physical memory.,C,"The text states 'Pure LRU: impractical to implement; most systems use LRU-approximation algorithms,' implying the difficulty in tracking 'least recently used' perfectly."
"In a 'global page-replacement algorithm', from which set of pages can a page be chosen for replacement?",Only from the pages belonging to the currently faulting process.,Only from pages that are explicitly marked as 'dirty'.,From any process currently in memory.,Only from pages belonging to the kernel.,From pages that have been in memory for the longest time.,C,Global page-replacement algorithms 'select page from any process for replacement'.
What distinguishes a 'local page-replacement algorithm' from a global one?,Local algorithms only select pages from the operating system's internal memory.,Local algorithms select a page for replacement exclusively from the faulting process.,Local algorithms prioritize pages based on their physical location in memory.,Local algorithms are designed to minimize the total number of page faults across all processes.,Local algorithms allow pages to be replaced only if they are not shared.,B,Local page-replacement algorithms 'select page from faulting process'.
What is the definition of 'thrashing' in the context of virtual memory?,When the CPU is entirely idle due to a lack of runnable processes.,When the system spends more time executing processes than performing paging operations.,When the system spends more time paging than executing useful work.,When memory pages are consistently allocated in contiguous blocks.,"When a process exclusively accesses data within its cache, avoiding disk I/O.",C,Thrashing is defined as 'system spends more time paging than executing'.
What does the term 'locality' refer to in memory management?,The physical proximity of memory chips to the CPU.,The total amount of memory that can be accessed by a process.,A set of pages that are actively used together by a process.,The degree to which a process's memory is fragmented.,The geographical distribution of data centers in a cloud environment.,C,Locality is defined as a 'set of pages actively used together'.
How do processes generally interact with 'locality' during their execution?,"Processes remain confined to a single, fixed locality throughout their lifecycle.","Process execution involves constant, random jumps across all available memory pages.",Process execution typically moves from one locality to another over time.,Localities are only relevant during the initial loading phase of a process.,Processes do not exhibit any specific pattern related to memory access localities.,C,"The text states, 'Process execution: moves from locality to locality'."
"Based on the concept of locality, what is a 'working set'?",The maximum amount of physical memory reserved for a process.,"The complete collection of all pages associated with a program, whether loaded or not.",The set of pages currently in active use by a process.,The total number of page frames available in physical memory.,The portion of a program that resides exclusively on the backing store.,C,"Working set is defined as 'based on locality, set of pages currently in use by a process'."
What is the core operation performed by 'memory compression' as described?,It physically reduces the size of RAM modules.,It compresses multiple pages into a single page to save space.,It expands a single compressed page into several larger pages.,It encrypts memory content to secure data access.,It automatically deletes unused data from memory to free space.,B,Memory compression 'compresses number of pages into single page'.
Memory compression is mentioned as an alternative to which common virtual memory technique?,Segmentation,Caching,Swapping,Paging,Direct Memory Access (DMA),D,It is stated as an 'Alternative to paging'.
"On which types of systems is memory compression commonly utilized, especially when they lack paging support?",Large-scale enterprise servers.,High-performance computing clusters.,Desktop workstations with abundant RAM.,Mobile systems.,Traditional mainframe computers.,D,Memory compression is used 'on mobile systems without paging support'.
How is kernel memory allocation distinguished from user-mode process allocation?,"Kernel memory is always allocated in fixed, small, non-contiguous blocks.",Kernel memory is allocated using the same demand paging mechanism as user processes.,Kernel memory is allocated differently and in contiguous chunks of varying sizes.,Kernel memory is never explicitly allocated; it's a fixed part of the OS.,Kernel memory is entirely managed by user-level applications.,C,Kernel memory is 'allocated differently than user-mode processes' and 'Allocated in contiguous chunks of varying sizes'.
Which of the following describes the nature of kernel memory allocation?,It is typically non-contiguous and of uniform fixed size.,It is allocated in contiguous chunks of varying sizes.,It always uses virtual addresses that do not map to physical memory.,It is handled exclusively by hardware without software intervention.,It is designed to be easily swappable to disk.,B,Kernel memory is 'Allocated in contiguous chunks of varying sizes'.
What are the two common techniques specifically mentioned for kernel memory allocation?,First Fit and Best Fit,FIFO and LRU,Buddy system and Slab allocation,Demand Paging and Copy-on-write,Round Robin and Shortest Job First,C,The two common techniques listed are 'Buddy system' and 'Slab allocation'.
What does 'TLB reach' quantify?,The physical size of the Translation Lookaside Buffer (TLB).,The speed at which the TLB can perform address translations.,The total amount of memory that can be accessed without a TLB miss.,The number of entries available in the TLB.,The ratio of TLB hits to TLB misses.,C,TLB reach is defined as the 'amount of memory accessible from TLB'.
How is TLB reach calculated?,By dividing the total physical memory by the number of TLB entries.,By multiplying the number of entries in the TLB by the page size.,By adding the number of TLB entries to the page size.,By subtracting the TLB miss rate from the hit rate.,By summing the sizes of all active processes.,B,TLB reach is 'Equal to number of entries in TLB × page size'.
What technique is suggested to increase TLB reach?,Decreasing the page size.,Reducing the number of entries in the TLB.,Increasing the page size.,Increasing the frequency of TLB flushes.,"Using a smaller, faster CPU cache.",C,A technique to increase TLB reach is to 'increase page size'.
"How do Linux, Windows, and Solaris generally manage virtual memory, according to the text?",They utilize completely distinct and incompatible virtual memory systems.,They manage virtual memory similarly.,"Linux uses a unique approach, while Windows and Solaris are similar.",They primarily rely on memory compression instead of paging.,"They do not employ virtual memory, opting for direct physical memory access.",B,"The text states, 'Linux, Windows, Solaris: manage virtual memory similarly'."
"Which specific virtual memory techniques are common across Linux, Windows, and Solaris?","Segmentation, pure LRU, and LIFO.","Buddy system, Slab allocation, and Best Fit.","Demand paging, copy-on-write, and variations of LRU approximation (clock algorithm).","Thrashing prevention, direct memory access, and fixed-size partitions.","Memory compression, optimal page replacement, and strict memory isolation.",C,"They 'Use demand paging, copy-on-write, and variations of LRU approximation (clock algorithm)'."
What is the primary characteristic of 'demand paging' as an alternative to loading an entire program into physical memory?,It preloads all program pages into physical memory before execution begins.,It loads pages into memory only when they are accessed or 'demanded' during execution.,"It stores the entire program exclusively in secondary storage, eliminating the need for physical memory.","It loads only the kernel's pages into physical memory, leaving user programs in swap space.",It continuously swaps all program pages between physical memory and cache memory.,B,"Demand paging is defined as loading pages into memory only when they are 'demanded' or accessed during execution, leading to more efficient memory use."
What common problem with traditional program loading does demand paging aim to solve?,The inability to execute programs larger than physical memory.,The high cost of CPU registers required for program execution.,The inefficiency of loading an entire program into memory when only parts may be immediately needed.,The difficulty in synchronizing multiple processes accessing the same memory region.,The lack of hardware support for memory address translation.,C,"The text states that the problem with loading an entire program is that 'May not need entire program initially (e.g., unselected options)', which demand paging addresses by loading pages only as needed."
"In a system utilizing demand paging, what happens to pages of a program that are never accessed during its execution?",They are loaded into physical memory at the start and then immediately swapped out.,They remain in secondary storage and are never loaded into physical memory.,They are marked as valid but are not available for use.,They are moved to a special cache for future use in subsequent executions.,They are discarded from secondary storage to free up space.,B,"A key characteristic of demand paging is that 'Unaccessed pages never loaded into physical memory', contributing to efficient memory use."
Which hardware support is essential for demand paging to distinguish between pages residing in physical memory and those in secondary storage?,A dedicated high-speed cache controller.,A specialized CPU register for virtual address translation.,A valid-invalid bit scheme within the page table.,Direct Memory Access (DMA) controllers for all I/O operations.,A network interface card for remote memory access.,C,The text explicitly states: 'Hardware support needed to distinguish: valid-invalid bit scheme.'
"In the context of demand paging's valid-invalid bit scheme, what does a 'valid bit' set in a page-table entry signify?",The page is legal but currently located in secondary storage.,The page is illegal and cannot be accessed by the process.,The page is legal and currently resides in physical memory.,The page has been modified since it was loaded into memory.,The page is protected from write access.,C,The text defines a 'Valid bit' as indicating that 'page legal and in memory'.
Under what condition would a page-table entry typically be marked with an 'invalid bit' in a demand paging system?,The page is actively being used by the CPU.,The page is part of the process's logical address space but is currently in secondary storage.,The page is designated as read-only.,The page is legal and currently in physical memory but awaiting a security scan.,The page is being transferred to a different process.,B,An 'Invalid bit' indicates that 'page not valid (not in logical address space) or valid but in secondary storage'.
What specific event occurs when a process attempts to access a page that is marked as invalid in its page table (assuming the access is for a valid part of its logical address space)?,A memory access violation leading to immediate process termination.,"A cache hit, retrieving the data from the CPU cache.","A 'page fault', triggering a specific handling procedure.",A successful read operation from physical memory.,"A segmentation fault, indicating an illegal memory address.",C,The text states: 'Access to invalid page -> page fault'.
What is the immediate consequence of a page fault occurring during program execution?,The process immediately suspends all operations until the page is loaded.,"A trap is generated, transferring control to the operating system.",The system automatically reboots to clear the memory error.,The CPU attempts to re-read the instruction multiple times.,All other processes are terminated to free up memory.,B,The text states: 'Page fault causes trap to OS'.
"According to the provided text, what is the very first step the operating system performs when it receives a trap due to a page fault?",It finds a free frame in physical memory to load the missing page.,It schedules an I/O operation to read the page from secondary storage.,It checks an internal table (like the process control block) to determine if the memory access was valid or truly invalid.,It restarts the instruction that caused the page fault.,It updates the page table entry to mark the page as valid.,C,The first step in the 'Page fault handling procedure' is 'Check internal table (process control block) for valid/invalid memory access'.
"During the page fault handling procedure, if the operating system determines that the memory access was genuinely invalid (i.e., not within the process's logical address space), what action does it take?",It attempts to load the page from a different secondary storage device.,It terminates the process that caused the invalid access.,"It allocates a new, larger free frame and retries the access.",It marks the page as 'dirty' and attempts to write it to swap space.,It requests user input on how to proceed.,B,"Step 2 of the page fault handling procedure states: 'If invalid, terminate process'."
What is the final action performed by the operating system after successfully handling a page fault and loading the required page into memory?,It broadcasts a notification to all other running processes.,It increments a counter for total page faults.,It deallocates the free frame used for the page.,"It restarts the interrupted instruction, allowing the process to continue as if no fault occurred.",It scans the entire page table for consistency errors.,D,The final step (6) is 'Restart interrupted instruction; process accesses page as if always in memory'.
Which of the following best describes 'pure demand paging'?,A system where the operating system preloads all executable code into memory at startup.,"A demand paging system where a process starts execution with no pages in memory, and page faults occur for every page needed.",A paging system that never uses secondary storage for any pages.,"A system designed exclusively for running small, self-contained programs.","A method where pages are loaded into memory based on a fixed schedule, not on demand.",B,The text defines 'Pure demand paging' as: 'start process with no pages in memory; fault for pages as needed'.
Why does the phenomenon of 'locality of reference' contribute to reasonable performance in demand paging systems?,It ensures that all program pages are always present in physical memory.,"It means processes tend to reference memory in clustered patterns, reducing the overall page-fault rate.","It allows the system to predict exactly which pages will be needed next, eliminating page faults.",It speeds up the process of transferring pages from secondary storage.,"It eliminates the need for a page table, simplifying memory management.",B,"The text states: 'Programs tend to have locality of reference -> reasonable demand paging performance.' The glossary further explains locality of reference as 'Tendency of processes to reference memory in patterns, not randomly,' which leads to fewer page faults over time."
What is the primary function of 'secondary memory' (also known as swap device or swap space) in the context of demand paging?,To act as a high-speed cache for frequently accessed data.,To store the operating system kernel and its modules permanently.,To hold pages that are part of a process's logical address space but are not currently resident in main memory.,To serve as the primary storage for all executing program instructions.,To manage the allocation of physical memory frames.,C,"The text specifies: 'Secondary memory: holds non-main-memory pages (swap device, swap space)'."
"What crucial capability must a system possess for demand paging to function correctly, particularly after a page fault?",The ability to discard all unsaved process data upon a page fault.,The ability to compress all memory contents to prevent page faults.,"The ability to restart any interrupted instruction from the exact point of interruption, with the required page now in memory.",The ability to run multiple copies of the same program simultaneously without interference.,The ability to disable virtual memory for critical operations.,C,"The text highlights this as a 'Crucial requirement: ability to restart any instruction after page fault. ...Restart process in exact same place/state, with desired page in memory'."
"What challenge do instructions that modify multiple memory locations (e.g., IBM System 360/370 MVC) pose for demand paging, and what is one described solution?",They can cause data corruption if a page fault occurs mid-instruction; a solution is to make all pages read-only.,They might partially modify data before a page fault occurs; one solution involves microcode accessing both ends of blocks before modification.,They are inherently incompatible with demand paging; the solution is to use simpler instruction sets.,They lead to an infinite loop if a page fault happens; the solution is to increase CPU clock speed.,They require all memory to be non-paged; the solution is to use direct physical addressing.,B,"The text discusses the 'Difficulty: instructions modifying multiple locations' and offers 'Solution 1: Microcode accesses both ends of blocks before modification; if fault, happens before modification'."
To what extent should the underlying paging mechanism be visible or exposed to the running processes?,"It should be fully visible, allowing processes to manage their own paging.","It should be partially visible, providing hints for performance optimization.",It should be completely transparent to the process.,It should only be visible to the process when a page fault occurs.,"It should be managed by a separate, user-level daemon.",C,The text explicitly states: 'Paging should be transparent to process'.
What is the primary role of the 'free-frame list' maintained by the operating system?,To track pages that are currently resident in secondary storage.,"To serve as a pool of available physical memory frames for page faults and segment expansion (e.g., stack/heap).",To store information about all active processes in the system.,To manage the allocation of virtual address space to processes.,To log all memory access attempts for debugging purposes.,B,The text describes: 'OS maintains free-frame list: pool of free frames for page faults. Free frames also allocated for stack/heap segment expansion'.
"What security-related practice, known as 'zero-fill-on-demand,' is commonly used by operating systems when allocating free frames?",Encrypting the contents of the memory frame before allocation.,"Writing zeros into a page before making it available to a process, to prevent information leakage.",Compressing the page content to reduce its memory footprint.,Storing only zero-initialized data on demand.,Deallocating memory frames immediately after they are used.,B,The text states: 'Most OS use zero-fill-on-demand: frames 'zeroed-out' before allocation (security)'.
"At system startup, what is the initial state of the 'free-frame list'?","It is empty, and frames are added only as they become available from terminated processes.","It contains a small, fixed number of frames reserved for the kernel.",All available physical memory is initially placed on the free-frame list.,It contains a copy of the entire swap space content.,It dynamically grows and shrinks based on immediate memory demands.,C,The text specifies: 'System startup: all available memory on free-frame list'.
How does demand paging significantly impact system performance?,It always drastically improves performance by keeping all necessary data in cache.,"It has negligible impact on performance, primarily affecting memory utilization.","It can significantly affect performance, requiring a very low page-fault rate to be efficient.",It slows down performance only if the system has an abundance of physical memory.,It only impacts performance during the initial loading phase of a program.,C,The text states: 'Demand paging significantly affects performance.' and 'Low page-fault rate crucial for demand-paging performance'.
"When calculating the 'effective access time' for demand-paged memory, which two primary factors are weighed?",The CPU's clock speed and the number of active processes.,The memory-access time (ma) and the page fault time.,The disk seek time and the data transfer rate.,The size of the page table and the number of valid bits.,The program's compile time and its total execution time.,B,"The formula for effective access time is given as: '$(1 - p) 	imes ma + p 	imes 	ext{page fault time}$', where $ma$ is memory-access time and 'page fault time' is the cost of handling a fault."
"Approximately how long does it take for a typical hard disk drive (HDD) to service a page-switch (read a page into memory), considering latency, seek time, and transfer time?",10 nanoseconds,200 nanoseconds,1 to 100 microseconds,8 milliseconds,3 seconds,D,"The text specifies: 'HDD page-switch time: ~8 milliseconds (3ms latency, 5ms seek, 0.05ms transfer)'."
What is the direct relationship between the 'effective access time' and the 'page-fault rate' in a demand paging system?,"They are inversely proportional; as one increases, the other decreases.",The effective access time is constant regardless of the page-fault rate.,The effective access time is directly proportional to the page-fault rate.,"They are unrelated, as performance depends solely on the CPU's processing power.",The effective access time is logarithmically related to the page-fault rate.,C,"The text states: 'Effective access time directly proportional to page-fault rate', which is evident from the formula showing $p$ as a multiplier for the large page fault time."
"Given a memory-access time of 200 ns and a page-fault service time of 8 ms, what must the page-fault rate (p) approximately be to limit the performance slowdown to less than 10% (i.e., effective access time < 220 ns)?","p < 1/1,000","p < 1/10,000","p < 1/399,990","p < 1/2,000,000",p < 1/200,C,"The text provides the calculation: 'To keep slowdown < 10% (e.g., 220 ns effective access time): ... p < 0.0000025 (fewer than 1 fault per 399,990 accesses)'."
"How does I/O from swap space generally compare in speed to I/O from the file system, and what is a primary reason for this difference?",Swap space I/O is slower due to more complex data structures.,"Swap space I/O is about the same speed as file system I/O, as they both use the same disk.",Swap space I/O is generally faster due to larger block transfers and avoiding file lookups.,File system I/O is always faster because it leverages file caching mechanisms.,The speeds are indistinguishable in modern operating systems.,C,"The text states: 'Swap space I/O generally faster than file system I/O (larger blocks, no file lookups)'."
Which strategy for swap space usage is commonly adopted by operating systems like Linux and Windows for general pages?,Copying the entire program file image to swap space at startup.,"Demand-paging directly from the file system initially, and writing pages to swap space only when they are replaced and modified.",Avoiding swap space entirely by keeping all pages in physical memory.,Loading all program pages into a compressed memory area instead of swap space.,Pre-fetching all anticipated pages into swap space before they are needed.,B,"The text specifies: 'Demand-page from file system initially, write pages to swap space when replaced (Linux, Windows)'."
"In Linux and BSD UNIX, how are binary executable pages typically handled when demand-paged, particularly regarding their backing store?",Their entire image is copied to swap space at program launch.,"They are demand-paged directly from the file system, which serves as their backing store because they are typically never modified.",They are always kept in physical memory and never swapped out.,"They use a dedicated, high-speed SSD separate from the main swap space.",Their pages are automatically compressed and stored in anonymous memory.,B,"The text states: 'Demand-page binary executables directly from file system; overwrite frames when replaced (never modified); file system acts as backing store (Linux, BSD UNIX)'."
"What is the primary backing store for 'anonymous memory' segments, such as the stack and heap, when they are paged out in a demand paging system?",The original executable file on the file system.,"A dedicated, read-only section of the file system.",The CPU's internal cache.,Swap space.,A temporary RAM disk.,D,"The text clearly states: 'Anonymous memory (stack, heap) still uses swap space'."
"How do mobile operating systems, such as iOS, typically handle memory paging and swapping?",They aggressively swap all memory types to persistent flash storage.,They rely heavily on compressed memory as their primary virtual memory mechanism.,They generally do not use swapping; they demand-page from the file system and reclaim read-only pages when memory is constrained.,They load entire applications into memory upon launch to avoid any paging.,"They utilize a large, dedicated swap partition on external SD cards.",C,"The text mentions: 'Mobile OS (e.g., iOS) typically no swapping: demand-page from file system, reclaim read-only pages if memory constrained'."
What is mentioned as an alternative to swapping that is sometimes employed in mobile systems to manage memory constraints?,Utilizing extremely large quantities of physical RAM to avoid paging altogether.,Completely disabling virtual memory features.,Implementing compressed memory.,Moving all application data to cloud storage.,Requiring all applications to be written in assembly language for maximum efficiency.,C,The text states: 'Compressed memory is an alternative to swapping in mobile systems'.
"According to the glossary, what is the definition of 'demand paging'?",The act of pre-loading an entire program into memory regardless of immediate need.,Bringing in pages from storage as needed rather than entirely at process load time.,A method of memory management that eliminates the need for secondary storage.,A technique for encrypting memory contents on demand.,The process of continually transferring all pages between CPU registers and cache.,B,Glossary definition: 'Bringing in pages from storage as needed rather than entirely at process load time'.
"Based on the glossary, what is a 'page fault'?",An error that occurs when physical memory runs out of space.,A successful retrieval of a page from the CPU cache.,A fault that occurs from referencing a page that is not currently resident in memory.,A software bug that corrupts the page table.,A security breach detected during memory access.,C,Glossary definition: 'Fault from reference to a non-memory-resident page'.
How does the glossary define 'pure demand paging'?,Demand paging where pages are brought into memory in large batches.,Demand paging where no page is brought into memory until it is referenced.,A paging system that only loads read-only pages on demand.,A system where all program pages are pre-loaded at process creation.,A method that avoids using page faults entirely by predicting memory needs.,B,Glossary definition: 'Demand paging where no page is brought into memory until referenced'.
What does the glossary describe as 'locality of reference'?,The random access patterns of memory by a process.,"The tendency of processes to reference memory in patterns, not randomly.",The physical location of data on a hard disk drive.,The process of referencing data directly from secondary storage.,The speed at which a process can access any memory location.,B,"Glossary definition: 'Tendency of processes to reference memory in patterns, not randomly'."
"According to the glossary, what is 'swap space'?",The dedicated area of the CPU for storing temporary data.,The primary physical memory (RAM) in a computer system.,Secondary storage backing-store space for paged-out memory.,A high-speed buffer for I/O operations.,An area of memory reserved for kernel-only processes.,C,Glossary definition: 'Secondary storage backing-store space for paged-out memory'.
What is the 'free-frame list' as defined in the glossary?,A list of all allocated physical memory frames.,A kernel-maintained list of available free physical memory frames.,A record of all pages currently in secondary storage.,A queue of processes waiting for memory allocation.,A cache of recently used frames.,B,Glossary definition: 'Kernel-maintained list of available free physical memory frames'.
What is the meaning of 'zero-fill-on-demand' according to the glossary?,Compressing a page's content if it contains mostly zeros.,Writing zeros into a page before making it available to a process for security reasons.,A technique to pre-fetch pages that are known to be all zeros.,Automatically clearing the contents of a page when it is deallocated.,A method to calculate the checksum of a page containing zeros.,B,Glossary definition: 'Writing zeros into a page before making it available to a process'.
"Based on the glossary, what is 'effective access time'?",The minimum time required to access a memory location.,The theoretical maximum speed of a memory device.,"The measured or calculated time to access something, such as memory.",The time it takes to transfer data from a hard disk to RAM.,The latency experienced by the CPU when executing an instruction.,C,"Glossary definition: 'Measured/calculated time to access something (e.g., memory)'."
What does the glossary define as the 'page-fault rate'?,The total number of page faults occurring over the system's uptime.,A measure of how often a page fault occurs per memory access attempt.,The speed at which pages are moved into and out of physical memory.,The percentage of physical memory occupied by currently active pages.,The maximum number of page faults a system can tolerate before crashing.,B,Glossary definition: 'Measure of how often a page fault occurs per memory access attempt'.
"According to the glossary, what is 'anonymous memory'?",Memory that is shared anonymously between different user accounts.,"Memory not associated with a file; typically used for stack and heap, and stored in swap space if dirty and paged out.",Read-only memory that contains system boot information.,Memory that has been encrypted and cannot be directly accessed.,Memory that is permanently resident in physical RAM and never swapped.,B,Glossary definition: 'Memory not associated with a file; stored in swap space if dirty and paged out'.
What is a primary benefit of using copy-on-write with the `fork()` system call for process creation?,Allocating all pages immediately for both parent and child.,Maximizing memory usage by duplicating all data.,Minimizing new pages allocated to the child process.,Ensuring full parent-child memory isolation from the start.,Bypassing demand paging entirely for all operations.,C,Copy-on-write is designed to minimize the number of new pages allocated to the child process by allowing initial sharing.
"In a system employing the copy-on-write technique for process creation, what is the initial state of pages shared between parent and child processes?",Fully copied into the child's address space.,"Completely independent, with no shared memory.",Shared between both processes.,Write-protected for both processes.,Only parent pages are accessible to the child.,C,Copy-on-write explicitly states that parent and child processes initially share the same pages.
"Under the copy-on-write mechanism, when is a copy of a shared page created?",When the child process starts execution.,When the parent process resumes from suspension.,When either the parent or child process attempts to read the page.,When either the parent or child process attempts to write to the shared page.,Only when the child process exits.,D,The core principle of copy-on-write is that a copy is made only when a write operation is attempted on a shared page.
"When a child process modifies a page that was initially shared via copy-on-write, which statement is true about the modification?",The modification is directly visible to the parent process.,The parent's original page is also modified.,"A new copy of the page is created for the child, and the modification occurs on this copy.","The page is immediately discarded, and a new empty page is allocated.",The entire address space is re-copied for the child.,C,"Upon a write attempt, the OS creates a copy of the shared page for the modifying process (e.g., child), and the modification occurs on this new copy, leaving the original page untouched for the other process (parent)."
"What was a significant drawback of the traditional `fork()` behavior, especially when the child process immediately called `exec()`?",It led to excessive page sharing that caused data corruption.,It prevented the child from modifying its own pages.,It made process creation extremely slow due to unnecessary copying of the entire address space.,It suspended the parent process indefinitely.,It required manual page allocation for the child process.,C,"Traditionally, `fork()` copied the parent's address space, which was often unnecessary and inefficient if the child immediately called `exec()`, as the copied data would simply be discarded."
"In a copy-on-write system, which type of pages can remain shared between the parent and child processes even after the child starts executing?",Only stack pages.,Only data pages that are actively being modified.,All modified pages.,"Unmodified pages, such as executable code segments.",Only heap pages.,D,"Unmodified pages, like executable code, do not trigger a copy-on-write event and can therefore remain shared between the parent and child."
"When a process attempts to write to a page marked as copy-on-write, what is the operating system's typical sequence of actions?",It immediately terminates the process for attempting a write to a shared page.,It flags the page as read-only for all processes.,"It searches for a free memory frame, copies the page to that frame, and then maps the new copy to the process's address space for modification.",It forces the process to wait until the other sharing process modifies the page first.,"It discards the page and reloads it from disk, then allows the write.",C,"As described in the example, the OS handles a write fault on a copy-on-write page by allocating a new frame, copying the page content, and updating the process's page table to point to the new copy."
How does `fork()` process creation relate to demand paging initially?,It always requires all pages to be loaded via demand paging before execution.,It prevents any form of demand paging from being used.,It can bypass demand paging initially by sharing pages directly.,It is completely independent of demand paging concepts.,It only uses demand paging for stack pages.,C,"The text states that 'Process creation using `fork()` can bypass demand paging initially,' often by sharing pages via copy-on-write rather than loading them immediately."
Copy-on-write is a common technique implemented in which of the following operating systems?,MS-DOS and Windows 95 only.,"Linux, macOS, and Windows.",Unixware and Solaris (but not Linux or macOS).,Windows NT 3.51 only.,All embedded systems regardless of complexity.,B,"The text explicitly states that copy-on-write is a common technique in Windows, Linux, and macOS."
Which of the following statements accurately describes a key difference between `vfork()` and a `fork()` implementation utilizing copy-on-write?,`vfork()` always creates a full copy of the parent's address space immediately.,`vfork()` utilizes copy-on-write for enhanced efficiency.,`vfork()` does not employ the copy-on-write mechanism.,`vfork()` allows the parent to continue execution concurrently with the child.,`vfork()` is an exclusive system call for Windows operating systems.,C,The text explicitly states that '`vfork()` does not use copy-on-write.'
"When a child process is created using `vfork()`, what happens to the parent process?",It continues execution concurrently with the child.,It terminates immediately.,"It is suspended, and the child uses the parent's address space.",It shares its address space in a copy-on-write manner with the child.,"It allocates a completely new, empty address space for the child.",C,"The text specifies that with `vfork()`, 'Parent process suspended; child uses parent's address space.'"
"If a child process created with `vfork()` modifies a page in its address space before calling `exec()`, what is the implication for the parent process?",The changes are isolated to the child and not visible to the parent.,"A copy of the modified page is automatically created for the child, isolating the change.",The changes are discarded when the parent resumes.,The changes made by the child will be visible to the parent upon its resumption.,The parent process immediately terminates due to a memory conflict.,D,"The text states: 'Child process changes to parent's address space are visible to parent upon resumption' when `vfork()` is used, highlighting the caution required."
For what specific scenario is `vfork()` primarily intended to be used?,When a child process needs to share data extensively and continuously with its parent.,When a child process needs a completely isolated and independent address space from the parent.,When the child process is expected to call `exec()` immediately after creation.,When the parent process needs to modify the child's address space directly.,When maximum memory safety and protection between parent and child is required.,C,The text clearly states that `vfork()` is 'Intended for use when child calls `exec()` immediately after creation.'
What makes `vfork()` an extremely efficient method for process creation?,It performs extensive page copying from parent to child.,It uses a sophisticated demand paging mechanism for all pages.,"It does not involve any page copying, as the child directly uses the parent's address space.",It uses a different CPU scheduling algorithm specifically for child processes.,"It runs only on single-core processors, simplifying synchronization.",C,`vfork()` is efficient because there is 'no page copying' as the child directly uses the parent's address space.
What critical caution must be observed when using `vfork()`?,The parent process must not be suspended.,The child process must not call `exec()` at all.,"The child process must not modify the parent's address space, as these changes will be reflected in the parent.",The parent must always share its heap with the child after resumption.,"`vfork()` should only be used in Windows environments, not UNIX-like systems.",C,"The text explicitly warns: 'Use with caution: child must not modify parent's address space,' because such changes are visible to the parent."
In which operating system families is `vfork()` a variation of `fork()` commonly found?,Microsoft Windows only.,"macOS, Linux, and BSD UNIX.",IBM z/OS.,Google Fuchsia.,VxWorks and other real-time operating systems exclusively.,B,"`vfork()` is described as a 'Variation of `fork()` in UNIX (Linux, macOS, BSD UNIX).'"
Which statement best defines the 'copy-on-write' mechanism as described in the glossary?,"Data is always copied before any modification, regardless of sharing.",Data is copied only when a read operation occurs on shared data.,"A copy of data is made only when a write operation is attempted on shared data, with the write occurring on the copy.",Data is copied on system boot-up to ensure rapid access.,Data is never copied; only pointers are shared indefinitely.,C,"The glossary defines copy-on-write as: 'Write causes data to be copied then modified; on shared page write, page copied, write to copy.'"
What accurately describes the 'virtual memory fork' (`vfork()`) system call according to the glossary?,It creates an entirely new and independent address space for the child process.,It uses copy-on-write for efficient page sharing between parent and child.,It allows the child to share the parent's address space for read/write operations while the parent is suspended.,It creates a child process that cannot interact with the parent's memory in any way.,It is a deprecated system call no longer in use in modern operating systems.,C,"The glossary defines `virtual memory fork` as: '`vfork()` system call; child shares parent's address space for read/write, parent suspended.'"
The copy-on-write technique for rapid process creation is described as being similar to what other memory management technique?,Memory swapping.,Demand paging.,Page sharing.,Memory compression.,Thrashing.,C,The text states that copy-on-write is a 'Technique similar to page sharing for rapid process creation.'
How are pages initially shared between parent and child processes typically marked in a copy-on-write system?,As read-only to prevent any modification.,"As executable, but not writable.","As demand-paged, meaning they are loaded only when accessed.","As copy-on-write, indicating their special handling on write access.","As fully copied, despite being shared.",D,The text states: 'Shared pages marked as copy-on-write.'
"In a copy-on-write implementation, if a child process modifies a page that was initially shared, what is the impact on the parent's original page?",The parent's page is also modified concurrently.,The parent's page becomes read-only and cannot be modified further.,"The parent's page remains unchanged, as the child's modification occurs on a newly created copy.",The parent's page is deleted from memory.,The parent's page is swapped out to disk immediately.,C,"The text clarifies that when a child modifies a copy-on-write page, 'Child modifies its copied page, not parent's,' meaning the parent's original remains untouched."
Which of the following is a primary benefit of demand paging?,It pre-loads all pages into memory at process start.,It saves I/O by loading only used pages.,It eliminates the need for swap space.,It guarantees zero page faults during execution.,It simplifies hardware memory management unit (MMU) design.,B,Demand paging is designed to save I/O and memory by only loading pages into physical memory when they are actually needed (demanded).
How does 'over-allocating' memory primarily impact the degree of multiprogramming in a system?,It decreases the degree of multiprogramming by limiting available frames.,It has no direct impact on the degree of multiprogramming.,It increases the degree of multiprogramming by allowing more processes to share memory.,It requires less CPU utilization due to reduced context switching.,It completely eliminates the need for virtual memory.,C,"Over-allocating memory means allocating more virtual memory than physically available, which allows more processes to be resident in memory, thereby increasing the degree of multiprogramming."
What is a potential problem that can arise from over-allocating memory?,Increased CPU idle time.,Processes consistently using only a small fraction of their allocated pages.,System memory becoming exclusively used for I/O buffers.,Processes suddenly needing more pages than the total available physical frames.,Reduced overall system throughput.,D,"The problem with over-allocation is when processes, which were initially using only a small portion of their virtual memory, suddenly need all their pages, exceeding the total physical frames available."
"Besides process pages, what other component utilizes system memory, potentially increasing strain on memory-placement?",Secondary storage devices.,Network interface cards.,I/O buffers.,CPU registers.,The system clock.,C,"The text states that 'System memory also used for I/O buffers, increasing strain on memory-placement.'"
How does memory over-allocation typically manifest itself in a running system?,As a system crash with a memory access violation.,As a page fault when there are no free frames available.,As consistently low CPU utilization.,As unusually fast process execution times.,As an immediate system reboot.,B,The text explicitly states: 'Over-allocation manifests as page fault with no free frames.'
What does 'over-allocating' mean in the context of memory management?,Providing dedicated physical memory to each process.,"Providing access to more resources than physically available, such as allocating more virtual memory than physical memory.",Assigning more CPU cores than physically present in the system.,"Reserving all swap space for a single, critical application.",Loading all possible pages of a process into memory at startup.,B,"According to the glossary, 'over-allocating' means 'Providing access to more resources than physically available; allocating more virtual memory than physical memory.'"
"In basic page replacement, what is the initial action if a page fault occurs and no free frame is available?",Immediately terminate the process causing the fault.,Suspend all processes until more physical memory is installed.,Find a frame that is not currently in use and free its contents.,Ignore the page fault and continue execution.,Increase the size of the swap space dynamically.,C,"The procedure is: 'If no free frame, find one not in use and free it.'"
"When a frame is freed during page replacement, what happens to its contents if they were modified?",They are permanently deleted to save space.,They are written to secondary storage (swap space).,They are immediately transferred to another free frame.,"They are ignored, as the new page will overwrite them.",They are encrypted for security before deletion.,B,"Freeing a frame involves: 'write contents to swap space, change page table (page no longer in memory).'"
What is the primary role of a 'page-replacement algorithm' in the page-fault service routine?,To locate the desired page on secondary storage.,To update the CPU's general-purpose registers.,To select a 'victim frame' for replacement when no free frames are available.,To determine the optimal size of the page table.,To increase the degree of multiprogramming.,C,"If no free frame is found, the page-replacement algorithm is used to 'select victim frame'."
What is a 'victim frame' in the context of page replacement?,The frame that caused the page fault.,A frame that has been marked as corrupted by the hardware.,The frame selected by the page-replacement algorithm to be replaced.,A frame containing critical operating system code.,Any frame that is currently empty.,C,The glossary defines 'victim frame' as 'Frame selected by page-replacement algorithm to be replaced.'
How many page transfers typically occur during a page-fault service routine if there are no free frames available?,One (page-in only).,Two (page-out and page-in).,"Three (page-out, page-in, and page-validation).","Zero, as the fault implies no transfer is possible.",It varies depending on the specific page-replacement algorithm.,B,"The text states: 'No free frames → two page transfers (page-out, page-in) → doubles page-fault service time.'"
What is the primary function of the 'modify bit' (or 'dirty bit') in page replacement?,To indicate if a page is currently in use by the CPU.,To track the age of a page for FIFO replacement.,To determine if a page's contents need to be written back to secondary storage before replacement.,To signal a hardware error in the memory management unit.,To mark a page as read-only and prevent modifications.,C,"The modify bit is set by hardware if a page is written to, and if set, the page must be written to storage before replacement. If not set, no write is needed."
Who is responsible for setting the 'modify bit' for a page?,The operating system software.,The user application program.,The hardware (MMU).,The compiler during code generation.,The system administrator manually.,C,The text specifies: 'Hardware sets modify bit if page written to.'
"In page replacement, when is it NOT necessary to write the contents of a victim frame back to secondary storage?",When the victim frame is a system-critical page.,When the desired page is already in the CPU cache.,When the modify bit for that frame is not set.,When the system has abundant free frames.,When the page-replacement algorithm is FIFO.,C,"If the modify bit is not set, it means the page has not been changed since it was read into memory, so 'no need to write to storage (page unchanged).'"
What fundamental aspect of memory management does page replacement enable?,Strict separation of CPU and GPU memory.,Complete elimination of disk I/O.,Separation of logical and physical memory.,Exclusive use of solid-state drives for swap space.,Direct access to raw disk partitions for all applications.,C,The text explicitly states: 'Page replacement separates logical and physical memory.'
What are the two major problems that demand paging mechanisms must address?,CPU scheduling and process synchronization.,File system caching and network protocols.,Frame-allocation algorithm and Page-replacement algorithm.,Interrupt handling and device driver management.,User authentication and system security.,C,The text lists 'Frame-allocation algorithm' and 'Page-replacement algorithm' as the 'Two major problems for demand paging'.
What is the primary goal when designing or selecting a page-replacement algorithm?,To maximize CPU utilization.,To minimize disk space usage.,To achieve the lowest page-fault rate.,To simplify the operating system kernel.,To maximize the number of processes in memory.,C,The goal is clearly stated as 'Goal: lowest page-fault rate.'
How are page-replacement algorithms typically evaluated?,By running them on real-world systems under heavy load.,By simulating their behavior using a 'reference string'.,By measuring their execution time on a benchmark suite.,By analyzing their theoretical computational complexity.,By counting the number of context switches they cause.,B,Algorithms are evaluated 'using reference string (trace of memory accesses).'
What simplification is typically made when generating a 'reference string' for evaluating page replacement algorithms?,"Only the page number is considered, and immediate repeated references are ignored.","Only read accesses are recorded, write accesses are ignored.","All memory accesses are recorded, including instruction fetches.",The time of access is recorded in milliseconds.,"Only unique page numbers are kept, regardless of access order.",A,"The text states: 'Reference string simplification: only page number, ignore immediate repeated references.'"
"Generally, what is the relationship between the number of frames allocated to a process and its page-fault rate?",More frames generally lead to a higher page-fault rate.,More frames generally lead to a lower page-fault rate.,The number of frames has no impact on the page-fault rate.,"The relationship is inverse for most algorithms, except FIFO.","Only the type of page-replacement algorithm affects the rate, not the number of frames.",B,The text confirms: 'More frames → fewer page faults (generally).'
What is a 'frame-allocation algorithm'?,An algorithm that determines the optimal size of a page.,An OS algorithm for allocating physical memory frames among all demanding processes.,A hardware component that manages the page table.,An algorithm used to decide when to swap processes out to disk.,A method to compress data within memory frames.,B,"According to the glossary, it's an 'OS algorithm for allocating frames among all demands.'"
"Which page does the First-In, First-Out (FIFO) page replacement algorithm choose to replace?",The page that has been used most recently.,The page that has been in memory for the longest time.,The page that is predicted to be used furthest in the future.,The page with the fewest references since it was loaded.,A randomly selected page.,B,FIFO 'Replaces the oldest page (first one brought into memory).'
What is a major advantage of the FIFO page replacement algorithm?,It always provides the lowest page-fault rate.,It is easy to understand and program.,It never replaces actively used pages.,It is immune to Belady's anomaly.,It requires minimal hardware support.,B,The text states: 'Easy to understand and program.'
What is 'Belady's anomaly'?,A situation where a page-replacement algorithm always chooses the optimal page.,A condition where increasing the number of allocated frames causes the page-fault rate to decrease.,A phenomenon where the page-fault rate may increase as the number of allocated frames increases.,A hardware fault that causes pages to be written incorrectly to swap space.,The inability of an operating system to manage more than one process simultaneously.,C,The glossary defines Belady's anomaly as 'Page-fault rate may increase as allocated frames increase for some algorithms.'
Which specific page replacement algorithm is known to suffer from Belady's anomaly?,Least Recently Used (LRU).,Optimal (OPT).,"First-In, First-Out (FIFO).",Least Frequently Used (LFU).,Second-Chance.,C,The text explicitly states FIFO 'Suffers from Belady's anomaly: page-fault rate may increase as allocated frames increase.'
What is the primary rule for the 'Optimal (OPT or MIN) page-replacement algorithm'?,Replace the page that was brought into memory first.,Replace the page that has not been used for the longest period of time.,Replace the page that will not be used for the longest period of time.,Replace the page with the lowest number of past references.,Replace a randomly chosen page.,C,Its rule is: 'Replace the page that will not be used for the longest period of time.'
Which characteristic best describes the Optimal page-replacement algorithm?,It is the easiest to implement in practice.,It guarantees the lowest possible page-fault rate.,It frequently suffers from Belady's anomaly.,It relies heavily on hardware counters for implementation.,It is only suitable for small memory systems.,B,The text states it 'Guarantees lowest possible page-fault rate.'
Why is the Optimal (OPT) page-replacement algorithm difficult to implement in real operating systems?,It requires excessively large page tables.,It needs to predict future memory accesses (future knowledge of reference string).,It causes too many context switches.,"It results in high CPU utilization, slowing down other processes.",It is a proprietary algorithm and cannot be used freely.,B,It is 'Difficult to implement: requires future knowledge of reference string.'
For what primary purpose is the Optimal page-replacement algorithm used?,As the default algorithm in modern operating systems.,To minimize the physical memory footprint of applications.,Mainly for comparison studies to evaluate other algorithms.,To prevent unauthorized memory access.,To reduce the latency of disk I/O operations.,C,The text indicates it's 'Used mainly for comparison studies.'
Which page does the Least Recently Used (LRU) algorithm choose to replace?,The page that was loaded first into memory.,The page that has been used most frequently.,The page that has not been used for the longest period of time.,The page that is predicted to be used last in the future.,A page that has been modified most recently.,C,LRU 'Replaces the page that has not been used for the longest period of time.'
How does the LRU algorithm relate to the Optimal page replacement algorithm?,"LRU is identical to Optimal, just a different name.",LRU is a direct implementation of Optimal in hardware.,"LRU is an approximation of Optimal, looking backward in time.",LRU is a more complex version of FIFO.,"LRU suffers from Belady's anomaly, unlike Optimal.",C,LRU is described as an 'approximation of optimal' and 'Optimal algorithm looking backward in time.'
Does the Least Recently Used (LRU) algorithm suffer from Belady's anomaly?,"Yes, always.","No, it is a stack algorithm.",Only if the number of frames is very small.,Only if it is implemented using hardware counters.,It depends on the specific workload.,B,The text explicitly states: 'Does not suffer from Belady's anomaly (is a stack algorithm).'
Which of the following describes a method for implementing LRU using 'counters'?,Keeping a stack of page numbers and moving the referenced page to the top.,"Associating a time-of-use field with each page-table entry, updated by a CPU logical clock.",Shifting a reference bit into an 8-bit byte at timer interrupts.,Maintaining a circular queue and clearing reference bits.,Periodically resetting all page-table entries to zero.,B,Under 'Counters': 'associate time-of-use field with page-table entry; CPU logical clock increments; copy clock to field on reference. Replace page with smallest time value.'
What is the primary reason why true LRU implementation is considered expensive?,It requires large amounts of disk swap space.,It needs significant per-memory-reference updates.,It consumes excessive CPU cycles for initial setup.,It leads to frequent system crashes.,It is highly susceptible to thrashing.,B,The text states: 'True LRU implementation is expensive due to per-memory-reference updates.'
What is a 'stack algorithm' in the context of page replacement?,An algorithm that replaces pages based on their position in a LIFO stack.,A class of page-replacement algorithms that do not suffer from Belady's anomaly.,An algorithm that uses a physical stack data structure for page frames.,An algorithm that prioritizes pages allocated from the process stack.,An algorithm that always selects the most recently used page.,B,The glossary defines 'stack algorithm' as 'Class of page-replacement algorithms that do not suffer from Belady's anomaly.'
Why are LRU-approximation algorithms frequently used in systems?,They always outperform true LRU.,They are simpler to implement than FIFO.,Many systems lack the substantial hardware assistance for true LRU.,They eliminate the need for a memory management unit.,They are specifically designed for embedded systems.,C,The text states: 'Many systems lack hardware for true LRU.'
"In LRU-approximation, what is the purpose of the 'reference bit'?",It tracks the number of times a page has been accessed.,Hardware sets this bit when a page is referenced (accessed).,It indicates if a page is currently in the CPU cache.,It identifies pages that are write-protected.,It determines the priority of a page for replacement.,B,The text explains: 'hardware sets bit when page referenced.'
How does the 'Additional-reference-bits algorithm' track page usage history?,By maintaining a simple counter that increments on each access.,"By storing an 8-bit byte for each page, where a timer interrupt shifts the reference bit into the high-order bit.",By using a doubly linked list to order pages by last access time.,By associating a timestamp with each page and comparing them.,By randomly selecting pages and checking their modification status.,B,"It keeps an '8-bit byte for each page' and at 'Timer interrupt... OS shifts reference bit into high-order bit of byte, shifts others right.'"
The 'Second-chance page-replacement algorithm' is based on which other algorithm?,Optimal (OPT).,Least Recently Used (LRU).,"First-In, First-Out (FIFO).",Least Frequently Used (LFU).,Most Frequently Used (MFU).,C,It is described as 'Basic FIFO.'
"In the Second-chance algorithm, what action is taken if a selected page's reference bit is 1?",The page is immediately replaced.,The page is written to secondary storage and then replaced.,The page is given a 'second chance' by clearing its reference bit and resetting its arrival time.,The process owning the page is terminated.,The system enters a low-power state.,C,"If the reference bit is 1, it's given 'second chance: clear bit, reset arrival time to current time.'"
Under what condition does the Second-chance page-replacement algorithm degenerate to behaving like FIFO?,If the system runs out of swap space.,If all reference bits are consistently set to 1.,If the modify bit is always 0.,If it is implemented with a stack instead of a circular queue.,If there are very few page faults.,B,The text states: 'Degenerates to FIFO if all bits set.'
The 'Enhanced second-chance algorithm' considers which two bits as an ordered pair for page classification?,Present bit and Valid bit.,Read bit and Write bit.,Reference bit and Modify bit.,Accessed bit and Executable bit.,Cacheable bit and Pinning bit.,C,"It 'Considers (reference bit, modify bit) as ordered pair.'"
"In the Enhanced second-chance algorithm, which class of pages is considered the 'best to replace'?","(1, 1) - recently used and modified.","(1, 0) - recently used but clean.","(0, 1) - not recently used but modified.","(0, 0) - neither recently used nor modified.",Any page that is currently not in use.,D,"Class (0, 0) is described as 'neither recently used nor modified (best to replace).'"
Why does the Enhanced second-chance algorithm prefer to replace 'clean' pages (modify bit 0) over 'dirty' pages (modify bit 1)?,Clean pages are always smaller in size.,Replacing clean pages reduces the number of I/O operations.,Dirty pages are more likely to be used again soon.,It simplifies the hardware implementation.,Clean pages cause fewer page faults.,B,It 'Prefers clean pages to reduce I/Os' because dirty pages require a write-out to disk.
What is a 'reference bit'?,A bit in the page table that indicates if a page is valid.,An MMU bit indicating a page has been accessed.,A bit used to count the number of references to a page.,A bit that determines if a page can be modified.,A bit in the process control block indicating a process is ready.,B,The glossary defines 'reference bit' as 'MMU bit indicating a page has been referenced.'
Which page does the Least Frequently Used (LFU) algorithm replace?,The page that was loaded earliest.,The page with the highest reference count.,The page that has not been used for the longest time.,The page with the smallest (lowest) reference count.,A randomly selected page.,D,LFU replaces 'page with smallest count.'
What is a known problem with the basic Least Frequently Used (LFU) algorithm?,It suffers from Belady's anomaly.,It always requires significant hardware support.,A page heavily used initially may retain a high count even if unused later.,It needs future knowledge of memory accesses.,It causes excessive swapping in and out of pages.,C,"The text notes the problem: 'heavily used page initially, then unused, keeps high count.'"
Which page replacement algorithm assumes that the page with the smallest count was just brought in and should be replaced?,Least Recently Used (LRU).,"First-In, First-Out (FIFO).",Most Frequently Used (MFU).,Optimal (OPT).,Second-Chance.,C,MFU is described as replacing 'page with smallest count (assumes just brought in).'
Why are Least Frequently Used (LFU) and Most Frequently Used (MFU) algorithms generally not common in practice?,They suffer from excessive Belady's anomaly.,They are simple to implement but yield poor performance.,They are expensive to maintain and do not approximate Optimal well.,They require specialized hardware found only in supercomputers.,They increase the CPU's power consumption significantly.,C,"The text states: 'Neither LFU nor MFU common: expensive, don't approximate OPT well.'"
What is the primary benefit of maintaining a 'pool of free frames' in page-buffering algorithms?,It allows the operating system to pre-fetch pages before they are needed.,"It enables the desired page to be read into a free frame immediately, speeding up process restart.",It reduces the total amount of physical memory required by the system.,"It ensures that all frames are always occupied, preventing memory fragmentation.",It serves as a backup for corrupted page table entries.,B,"The text states: 'On page fault, desired page read into free frame from pool before victim written out. Process restarts faster.'"
"In page-buffering, what is the advantage of keeping a 'list of modified pages'?",It ensures that only clean pages are ever replaced.,"It allows modified pages to be written to secondary storage when the paging device is idle, increasing clean page probability.",It helps the OS prioritize which processes get more memory.,It is used to identify pages that have never been modified.,It prevents any page from being written out to swap space.,B,It 'Increases probability of clean page for replacement (no write-out needed)' because modified pages are written out during idle times.
How does a 'pool of free frames remembering old pages' benefit page-buffering?,It serves as a cache for frequently accessed kernel data.,It allows the OS to immediately free up frames when a process terminates.,It enables a needed old page to be reused directly from the pool without I/O if needed again before reuse.,It stores copies of all pages that have been written to secondary storage.,It helps in detecting memory leaks in applications.,C,"The text describes it: 'If old page needed before frame reused, can be reused directly from pool (no I/O).'"
Which page replacement algorithm is specifically mentioned as being used by UNIX in conjunction with page-buffering techniques?,Optimal.,LFU.,FIFO.,Second-chance.,LRU.,D,The text states: 'UNIX uses this with second-chance algorithm.'
"Why might some applications, like databases, perform worse with general-purpose OS virtual memory buffering?",They require more CPU cycles than the OS can provide.,They need to bypass the OS for direct hardware access.,Applications often have a better understanding of their specific memory and storage use patterns.,They are designed to operate exclusively on solid-state drives.,They are incompatible with demand paging.,C,The text explains: 'Applications understand their memory/storage use better than general-purpose OS algorithms.'
What is 'double buffering' in the context of OS and application I/O?,A technique to store data in two separate physical memory locations for redundancy.,"When both the operating system and the application perform I/O buffering, potentially causing inefficiency.",A method to read data from disk twice to ensure data integrity.,A process of copying data between kernel and user space memory buffers.,An error state where memory pages are duplicated unnecessarily.,B,Double buffering occurs 'if OS and application both buffer I/O.'
"For data warehouses that perform sequential reads followed by computations/writes, which page replacement algorithm might be more efficient than LRU and why?","FIFO, because it's simpler to implement.","Optimal, because it knows future accesses.","MFU, because older pages might be read again, and MFU replaces pages with low frequency (assuming they were just brought in).","Second-chance, due to its circular queue mechanism.","LFU, because it keeps frequently used pages in memory.",C,"The text suggests: 'LRU removes old pages, but older pages might be read again. MFU could be more efficient.' MFU replaces pages with smallest count, assuming they were just brought in and haven't accumulated references, which might suit the 'older pages read again' scenario better than LRU which removes pages based on recency."
What is 'raw disk' access?,Accessing secondary storage through standard file system calls only.,"Direct access to secondary storage as a large sequential array of logical blocks, bypassing file-system services.",Using a RAM disk for very fast temporary storage.,Accessing a network-attached storage device.,A method for encrypting data on disk for security.,B,"The glossary defines 'raw disk' as 'Direct access to secondary storage as array of blocks, no file system.'"
Which of the following file-system services are typically bypassed when using 'raw I/O'?,Only file naming and directory structures.,Only data encryption and decryption services.,"Demand paging, locking, prefetching, allocation, names, and directories.",Only network protocols and remote access.,Only kernel scheduling and interrupt handling.,C,"Raw I/O 'bypasses file-system services (demand paging, locking, prefetching, allocation, names, directories).'"
For which type of applications are 'raw partitions' considered most efficient?,General-purpose word processors.,Standard web browsers.,Specific applications that can manage their own storage needs directly.,"Small, embedded systems with limited memory.",Applications requiring maximum security and data integrity.,C,"The text indicates: 'Raw partitions efficient for specific apps, but most apps better with regular file-system services.'"
What is the primary issue addressed by the allocation of frames in an operating system?,How to distribute fixed free memory among competing processes.,How to determine the optimal size of a page for a process.,How to manage virtual memory addresses efficiently.,How to prevent deadlocks in memory access for frames.,How to reclaim memory from terminated processes only.,A,The text states the allocation issue is 'how to allocate fixed free memory among processes'.
"In a system with 128 frames where the operating system initially uses 35 frames, how many frames are typically placed on the free-frame list for user processes under a pure demand paging system?",128 frames,35 frames,93 frames,0 frames,"An unspecified number, depending on immediate demand.",C,"The example given is 128 total frames, OS takes 35, leaving 93 for user processes on the free-frame list."
"Under a pure demand paging system, what action is taken when a process experiences a page fault and free frames are available?",The process is immediately terminated.,A page-replacement algorithm is invoked.,The system allocates a free frame from the free-frame list.,The operating system takes frames from other processes.,The page is loaded from cache memory.,C,The text states that 'Page faults get free frames' from the free-frame list.
What mechanism is invoked if the free-frame list becomes exhausted when a page fault occurs?,The operating system panics and restarts.,A page-replacement algorithm is used to free a frame.,All user processes are temporarily suspended.,New frames are dynamically allocated from physical disk.,The process waits indefinitely for memory.,B,The text states: 'List exhausted → page-replacement algorithm used'.
"When a process terminates, what typically happens to the frames that were allocated to it?",They remain allocated to the process in case it restarts.,They are immediately zeroed out for security purposes.,They are returned to the free-frame list for reuse.,They are added to a special reserved pool for the OS.,They are swapped out to backing store.,C,The text states: 'Process terminates → frames back to free-frame list'.
"In some systems, the OS allocates buffer or table space from the free-frame list. How can this space be utilized when not in use by the OS?",It can be permanently locked for future OS use.,It is immediately returned to the backing store.,It can be used for user paging.,It must remain unused to prevent OS performance degradation.,It is converted into swap space.,C,The text mentions this variation: 'OS allocates buffer/table space from free-frame list (can be used for user paging when not in use)'.
A common variation in frame allocation strategy involves keeping a small number of free frames reserved. For what primary purpose are these reserved frames used?,To be immediately available for kernel operations only.,To ensure system security by holding critical data.,To quickly satisfy a page fault while a replacement frame is being selected.,To act as a buffer for network I/O operations.,To store frequently accessed system libraries.,C,"The text states: 'Keep 3 free frames reserved: free frame for page fault, replacement selected during swap'."
What is considered the basic strategy for allocating frames to a user process?,Allocating frames based on the process's priority.,Allocating frames proportionally to the process's size.,Allocating frames in a round-robin fashion.,Allocating any available free frame.,Allocating frames only from a dedicated user partition.,D,The text describes this as the 'Basic strategy: user process allocated any free frame'.
Which of the following is a constraint on frame allocation that prevents a process from acquiring too much memory?,"It must not exceed the total available frames, unless page sharing is involved.",It must be an even number of frames.,It cannot be less than the process's virtual memory size.,It is limited by the number of CPU cores available.,It must always be less than 10 frames per process.,A,One constraint listed is: 'Cannot exceed total available frames (unless page sharing)'.
"Besides not exceeding total available frames, what other fundamental constraint applies to the number of frames allocated to a process?",It must be an odd number to prevent alignment issues.,It must allocate at least a minimum number of frames for correct execution.,It must be divisible by the number of active CPUs.,It cannot be less than 100 frames for any user process.,It must be equal to the number of processes in the system.,B,The text states: 'Must allocate at least a minimum number of frames'.
What is a direct consequence of allocating fewer frames to a process for its execution?,Decreased page-fault rate and faster execution.,Higher page-fault rate and slower execution.,Increased system throughput and better resource utilization.,Reduced CPU utilization but improved I/O performance.,No significant impact on performance.,B,"The text states: 'Fewer frames → higher page-fault rate, slower execution'."
Why might an instruction need to be restarted if a page fault occurs during its execution?,The instruction's opcode itself was paged out.,The page fault occurred before the instruction could fully complete its operation.,The CPU cache was invalidated due to the fault.,The operating system always restarts processes after any page fault.,It's a security measure to prevent data corruption.,B,The text states: 'Page fault before instruction complete → instruction restart'.
What criterion must be met regarding frame allocation to ensure an instruction can complete without causing an immediate page fault?,The process must have access to all physical memory.,Enough frames must be allocated to hold all pages that the instruction can reference during its execution.,Only one frame is ever needed per instruction.,All instructions must be non-memory-referencing.,The instruction must be stored entirely in cache memory.,B,The text highlights the need for 'enough frames for all pages an instruction can reference'.
"For a single memory-reference instruction, what is the typical minimum number of frames required for a process to execute it?",One frame for the instruction.,One frame for the memory reference.,Two frames: one for the instruction and one for the memory reference.,Three frames for robustness.,"Zero frames, as instructions are executed directly by the CPU.",C,"The example given is: 'Single memory-reference instruction → 1 frame for instruction, 1 for memory reference'."
How many frames are considered the minimum necessary for a process that uses one-level indirect addressing?,One frame.,Two frames.,At least three frames.,Four frames.,Five frames.,C,The text specifies: 'One-level indirect addressing → at least 3 frames per process'.
What primarily defines the minimum number of frames that must be allocated to a process?,The current system load.,The size of the process's data segment.,The computer architecture.,The user's preference settings.,The number of concurrent processes.,C,The text explicitly states: 'Minimum frames defined by computer architecture'.
"According to the text, how many frames might be required for a complex instruction like a 'move instruction straddling two frames with two indirect operands'?",Two frames.,Three frames.,Four frames.,Six frames.,Eight frames.,D,"The example provided is: 'move instruction straddling two frames, two indirect operands → 6 frames'."
What characteristic of Intel architectures typically limits the minimum number of frames required for a process?,Their reliance on large CPU caches.,Their support for only register-to-register or memory-to-register operations.,Their use of a 64-bit address space.,Their strict adherence to the FIFO page replacement algorithm.,Their specialized hardware for handling page faults.,B,"The text states: 'Intel architectures: register-to-register/memory only, limits minimum frames'."
What ultimately defines the maximum number of frames that can be allocated in a system?,The operating system's kernel size.,The total available physical memory.,The number of active user processes.,The speed of the CPU.,The size of the swap space.,B,The text states: 'Maximum frames: defined by available physical memory'.
Which frame allocation method assigns an equal share of available frames to all requesting processes?,Proportional allocation.,Priority allocation.,Demand paging.,Equal allocation.,Local allocation.,D,"The glossary defines 'equal allocation' as assigning 'equal amounts of a resource to all requestors; in virtual memory, equal frames to each process'."
"If a system has 93 user frames available and 5 processes are running, how would frames be distributed using equal allocation, and how many would be leftover for purposes like a buffer?","20 frames each, 0 leftover.","15 frames each, 18 leftover.","18 frames each, 3 leftover.","93 frames to one process, 0 leftover for others.",Varies based on process priority.,C,"The example given is: '93 frames, 5 processes → 18 frames each, 3 leftover for buffer'."
What is a significant drawback of using equal allocation when processes have vastly different memory requirements?,It always results in system thrashing.,It favors large processes over small ones.,It can lead to wasted frames for processes with smaller memory needs.,It requires more complex scheduling algorithms.,It increases the likelihood of major page faults.,C,The text explains: 'Equal allocation (31 each) wastes frames for student process' if one is 10KB and another 127KB.
Which frame allocation method assigns page frames to processes based on their virtual memory size or 'needs'?,Equal allocation.,Proportional allocation.,Dynamic allocation.,Static allocation.,Threshold allocation.,B,The glossary defines 'proportional allocation' as assigning 'page frames in proportion to process size'.
"In proportional allocation, if $s_i$ is the virtual memory size of process $p_i$, $S$ is the total virtual memory size of all processes, and $m$ is the total available frames, which formula approximates the number of frames $a_i$ allocated to process $p_i$?",$a_i = s_i + m$,$a_i = (s_i / m) 	imes S$,$a_i \approx (s_i / S) 	imes m$,$a_i = S / s_i$,$a_i = m - s_i$,C,The text provides the formula: '$a_i \approx (s_i/S) 	imes m$ frames to $p_i$'.
"When applying proportional allocation, what are the crucial constraints on the calculated number of frames ($a_i$) for each process?","It must be a floating-point number, greater than total frames, and sum to exactly $m$.","It must be an integer, greater than minimum frames, and their sum must be less than or equal to $m$.","It must be a prime number, exactly equal to minimum frames, and sum to $S$.","It must be a multiple of 2, less than minimum frames, and sum to $s_i$.","It can be any positive number, ignoring minimum frames, and sum to more than $m$ if needed.",B,"The text states: '$a_i$ must be integer, greater than minimum frames, sum $\le m$'."
"Consider a system with 62 available frames. If one process has 10 virtual pages and another has 127 virtual pages (total 137 pages), approximately how many frames would the process with 10 virtual pages receive under proportional allocation?",10 frames,57 frames,31 frames,4 frames,1 frame,D,The calculation provided is: '(10/137) × 62 ≈ 4 frames'.
How does an increased level of multiprogramming generally affect the number of frames allocated to individual processes?,Processes gain more frames.,Processes lose frames.,The number of frames per process remains constant.,Frames are swapped to disk more frequently.,Only kernel processes are affected.,B,The text states: 'Increased level → processes lose frames'.
"Given that equal and basic proportional allocation methods treat high and low priority processes similarly, what is a proposed solution to account for process priority in frame allocation?",Mandatory use of local replacement.,Proportional allocation based on process priority or a combination of size and priority.,Exclusively using the OOM killer for low-priority processes.,Allowing only high-priority processes to access the free-frame list.,Disabling demand paging for low-priority processes.,B,The text suggests: 'Solution: proportional allocation based on process priority or size + priority'.
"In which page-replacement strategy can a process select a replacement frame from any frame in the system, even if that frame is currently allocated to another process?",Local replacement.,Static replacement.,Global replacement.,FIFO replacement.,LRU replacement.,C,"The glossary defines 'global replacement' as when a 'Process selects replacement frame from all frames in system, even if allocated to another process'."
Which page-replacement strategy limits a process to selecting a replacement frame only from its own set of allocated frames?,Global replacement.,Dynamic replacement.,Local replacement.,System-wide replacement.,Unified replacement.,C,The glossary defines 'local replacement' as when a 'Process selects replacement frame only from its own allocated frames'.
What is a direct consequence of using a global page-replacement algorithm for a process?,The process's allocated frames are guaranteed to remain constant.,The process can increase its number of frames by taking them from other processes.,The process is limited to its initial frame allocation.,It prevents any process from taking frames from another.,It always results in lower system throughput.,B,The text states: 'Global replacement: process can increase its frames by taking from others'.
What is the primary advantage of using a local replacement strategy in terms of process performance?,It maximizes system throughput.,"Process performance depends only on its own paging behavior, independent of other processes.",It allows high-priority processes to easily acquire frames from low-priority ones.,It simplifies the overall memory management system design.,It eliminates the need for a free-frame list.,B,The text states: 'Local replacement: performance depends only on its own paging behavior'.
Which page-replacement strategy is generally more common and leads to greater system throughput?,Local replacement.,Global replacement.,Hybrid replacement.,Fixed allocation.,Proportional allocation.,B,"The text states: 'Global replacement: generally greater system throughput, more common'."
What is a page fault generally defined as?,A situation where a process attempts to write to a read-only page.,An error where a page is corrupted in memory.,A reference to a page for which there is no valid logical mapping in memory.,A system crash caused by insufficient memory.,An instance where two processes try to access the same page simultaneously.,C,The text defines it as: 'Page fault: page no valid mapping'.
"In Windows operating systems, what are 'major page faults' and 'minor page faults' respectively referred to as?",Soft faults and hard faults.,Internal faults and external faults.,Hard faults and soft faults.,Kernel faults and user faults.,Primary faults and secondary faults.,C,The text states: 'Windows: hard and soft faults'.
Which type of page fault occurs when a referenced page is not in memory and requires reading data from the backing store?,Minor page fault.,Soft fault.,Non-fatal page fault.,Major page fault.,Cache fault.,D,The glossary defines 'major fault' as a 'Page fault resolved by I/O to bring page from secondary storage'.
"During the initial stages of demand paging for a new process, what type of page faults are typically observed to be high?",Minor faults.,Soft faults.,Major faults.,Cache misses.,Write-back faults.,C,The text states: 'Demand paging → initially high major faults'.
"Which type of page fault occurs when a process attempts to reference a page for which it has no logical mapping, but the page is already present in memory?",Major page fault.,Hard fault.,Secondary page fault.,Minor page fault.,Critical page fault.,D,The glossary defines 'minor fault' as a 'Page fault resolved without paging in data from secondary storage'.
One reason for a minor page fault is when a shared library is already in memory but a process does not have a mapping to it. How is this resolved?,The shared library is loaded again from disk.,The process's page table is updated to include the mapping.,The shared library is copied to the process's private memory space.,The process is terminated for attempting an invalid access.,The OS reboots the system to clear memory.,B,"The text explains: 'Shared library in memory, no mapping → update page table'."
Another reason for a minor page fault is when a page has been reclaimed to the free-frame list but has not been zeroed or reallocated. How is this particular minor fault resolved?,The page is written back to backing store.,The frame is removed from the free-frame list and reassigned to the process.,The system waits for a major page fault to occur.,The page is marked as invalid and remains on the free-frame list.,"The OS creates a new, empty frame for the process.",B,"The text explains: 'Page reclaimed to free-frame list, not zeroed/allocated → frame removed from list, reassigned'."
"In terms of time consumption, how do minor page faults compare to major page faults?",Minor faults are generally more time consuming.,Minor faults are generally less time consuming.,They are equally time consuming.,"Minor faults only consume CPU time, major faults only I/O time.",Time consumption is negligible for both.,B,The text states: 'Less time consuming than major faults'.
What Linux command is provided in the text to observe the number of minor and major page faults for processes?,`top -m`,`free -h`,"`ps -eo min_flt,maj_flt,cmd`",`vmstat 1`,`dmesg`,C,"The text explicitly gives the command: 'Linux command to observe: `ps -eo min_flt,maj_flt,cmd`'."
"What is a common observation regarding page faults on Linux systems, and what is the primary reason cited for this observation?",Both major and minor faults are consistently high due to inefficient memory management.,"Major faults are high, and minor faults are low, indicating frequent disk I/O.","Major faults are low, and minor faults are high, primarily due to heavy use of shared libraries.","Both major and minor faults are low, indicating ample physical memory.",Fault rates fluctuate wildly with no clear pattern.,C,"The text notes: 'Observation: major faults low, minor faults high. Linux processes use shared libraries heavily'."
"In a global page-replacement strategy for reclaiming pages, when are page replacements typically triggered to ensure sufficient free memory?",Only when the free-frame list is completely empty.,When the free-frame list falls below a predefined threshold.,"Periodically, regardless of the free-frame list status.",Upon system startup only.,When a process explicitly requests more memory.,B,"The text states: 'Trigger replacement when list below threshold, not at zero. Ensures sufficient free memory'."
"What is the primary purpose of triggering page replacement when the free-frame list falls below a certain threshold, rather than waiting for it to be completely exhausted?",To increase the system's susceptibility to thrashing.,To ensure a sufficient amount of free memory is always available for immediate requests.,To decrease the overall system throughput.,To force processes to use less memory.,To reduce the complexity of the page-replacement algorithm.,B,The text indicates it 'Ensures sufficient free memory'.
What are the kernel routines that scan memory and free frames to maintain a minimum amount of free memory known as?,Garbage collectors.,Memory defragmenters.,Reapers.,Page scrubbers.,Frame inspectors.,C,"The glossary defines 'reapers' as 'Routines that scan memory, freeing frames to maintain minimum free memory'."
When are 'reapers' typically triggered in a system that aims to keep free memory above a minimum threshold?,When free memory exceeds a maximum threshold.,When free memory falls below a minimum threshold.,Only during system idle periods.,After every major page fault.,Upon process termination only.,B,The text states: 'Below threshold → kernel routine (reapers) triggered'.
From which processes do reaper routines typically reclaim pages?,Only from low-priority processes.,Only from processes that are currently idle.,"From all processes, excluding the kernel.",Only from processes that have exceeded their allocated frame limit.,Only from processes that are causing thrashing.,C,The text states: 'Reclaims pages from all processes (excluding kernel)'.
What happens to a reaper routine when the amount of free memory reaches a maximum threshold?,It switches to a more aggressive page-replacement algorithm.,It increases its scanning frequency.,It is suspended until free memory falls below the minimum threshold again.,It terminates all inactive processes.,It begins writing frames to the swap space.,C,The text states: 'Reaches max threshold → reaper suspended. Resumes when free memory below min threshold'.
What page-replacement algorithm do reaper routines typically use in their continuous process of maintaining free memory?,"Pure FIFO (First-In, First-Out).",Optimal replacement.,LRU approximation.,LFU (Least Frequently Used).,Random replacement.,C,The text states: 'Reaper routine typically uses LRU approximation'.
"If a reaper routine is unable to maintain sufficient free frames using its typical algorithm, what might it do to reclaim memory more aggressively?",It will request more physical memory from the hardware.,It will notify the system administrator to manually free memory.,"It might switch to a less sophisticated but more aggressive algorithm, such as pure FIFO.",It will immediately trigger the OOM killer.,It will attempt to compress existing pages in memory.,C,"The text mentions: 'If unable to maintain free frames: reclaims more aggressively (e.g., pure FIFO)'."
"What is the Linux routine that terminates processes when free memory is critically low, in order to free up resources?",The Memory Manager.,The System Watchdog.,The Out-Of-Memory (OOM) killer.,The Process Reclaimer.,The Page Reaper.,C,The glossary defines 'out-of-memory (OOM) killer' as a 'Linux routine that terminates processes to free memory when free memory is very low'.
"In the context of the Linux OOM killer, what does a higher 'OOM score' for a process indicate?",A higher priority for receiving more memory.,A higher likelihood of being terminated by the OOM killer.,A measure of how efficiently the process uses memory.,The total amount of memory the process has ever requested.,That the process is protected from termination.,B,The text states: 'OOM score: higher score → higher termination likelihood'.
How is the OOM score for a process primarily calculated by the Linux OOM killer?,Based on its CPU utilization percentage.,Based on its total runtime.,Based on its memory usage percentage.,Based on its I/O activity.,Based on its process ID.,C,The text states: 'Calculated by memory usage percentage'.
Who is typically responsible for configuring the minimum and maximum thresholds for reaper routines in a system?,The application developer.,The end-user.,The system administrator.,The operating system kernel automatically.,The hardware manufacturer.,C,The text states: 'Min/max thresholds configurable by system administrator'.
What traditional assumption about memory access in virtual memory systems is increasingly challenged by modern multi-CPU architectures?,Memory access is always sequential.,"Memory access is always uniform in speed, regardless of CPU.",Memory access is always cached.,Memory access is limited to a single CPU core.,Memory access is managed entirely by hardware.,B,The text notes the 'Virtual memory assumption: uniform memory access. Not true for NUMA systems'.
What type of computer architecture is characterized by varying memory access times depending on which CPU core accesses the memory?,Symmetric Multiprocessing (SMP).,Uniform Memory Access (UMA).,Non-Uniform Memory Access (NUMA).,Massively Parallel Processing (MPP).,"Single Instruction, Multiple Data (SIMD).",C,The glossary defines 'non-uniform memory access (NUMA)' as an 'Architecture where memory access time varies based on CPU core'.
"In a NUMA system with multiple CPUs, how does a CPU typically access its local memory compared to memory located remotely (e.g., on another system board)?",Accesses local memory slower than remote memory.,Accesses local memory faster than remote memory.,Access times are identical for local and remote memory.,Remote memory access is not possible.,"Access speed depends solely on cache size, not location.",B,The text states: 'CPU accesses local memory faster than remote'.
"Despite memory access being slower than in uniform access systems, what primary benefit do NUMA systems offer?",They completely eliminate page faults.,"They allow for more CPUs, enabling greater throughput and parallelism.",They simplify memory management for the OS.,They reduce the total amount of physical memory required.,They are significantly cheaper to manufacture.,B,"The text states NUMA systems 'allow more CPUs, greater throughput/parallelism'."
What aspect is considered critical for performance in Non-Uniform Memory Access (NUMA) systems?,Minimizing the number of CPU cores.,Ensuring all memory is located on a single board.,Managing the page frame location effectively.,Disabling the use of virtual memory.,Increasing the frequency of I/O operations.,C,The text states: 'NUMA performance: managing page frame location critical'.
What is the primary goal of NUMA-aware allocation when a page fault occurs?,To allocate frames randomly across all available memory.,To allocate frames from the largest available memory bank.,To allocate frames 'as close as possible' to the CPU that caused the fault.,To allocate frames only from remote memory nodes.,To allocate frames only to the kernel.,C,The text states: 'NUMA-aware allocation: frames allocated 'as close as possible' to CPU'.
What specific information does a NUMA-aware scheduler track to optimize performance?,The process's total virtual memory size.,The number of page faults a process has incurred.,The last CPU on which a process executed.,The priority level of the process.,The total elapsed time since the system started.,C,The text states: 'NUMA consideration: scheduler tracks last CPU'.
"By scheduling a process on its previous CPU and allocating frames close to that CPU in a NUMA system, what two key performance metrics are primarily improved?",Increased power consumption and reduced battery life.,Reduced CPU utilization and increased I/O waiting time.,Improved cache hits and decreased memory access latency.,Higher process migration rates and increased context switching overhead.,Better disk utilization and slower network speeds.,C,"The text states: 'Schedule process on previous CPU + allocate frames close to CPU → improved cache hits, decreased memory access'."
What aspect of multi-threaded processes creates a significant memory allocation challenge in NUMA systems?,Threads are unable to share memory in NUMA.,Process threads might be scheduled and run on different system boards.,Threads consume too much CPU cache.,Threads always require more frames than single processes.,NUMA systems do not support multi-threading.,B,The text identifies: 'Threads complicate NUMA: process threads on different system boards. Memory allocation challenge'.
How does the Linux CFS scheduler address NUMA challenges related to thread migration?,It forces all threads of a process onto the same CPU.,It prevents thread migration across scheduling domains to avoid memory access penalties.,It migrates threads frequently to balance load.,It assigns a fixed memory region to each thread.,It prioritizes threads that migrate frequently.,B,The text states: 'CFS scheduler prevents thread migration across domains (avoids memory access penalties)'.
"In Linux's solution for NUMA, how is memory allocation facilitated for threads to optimize performance?","All threads share a single, large free-frame list.",Memory is allocated randomly from any available node.,"A separate free-frame list is maintained per NUMA node, allowing threads to allocate memory from their running node.",Memory is pre-allocated to threads at process startup.,Threads are not allowed to dynamically allocate memory.,C,The text states: 'Separate free-frame list per NUMA node → thread allocated memory from its running node'.
"In Solaris, what kernel construct gathers CPUs and memory into groups to optimize memory access in NUMA systems?",Memory partitions.,NUMA domains.,lgroups (locality groups).,CPU clusters.,Memory nodes.,C,The glossary defines 'lgroups' as 'Solaris locality groups in kernel; gather CPUs and memory for optimized access in NUMA'.
What is the hierarchical structure described for Solaris's lgroups?,They are organized as a flat list.,They form a strict tree structure.,"They can be nested within each other, forming a hierarchy.",They are independent and do not relate to each other.,They are formed dynamically and are non-persistent.,C,The text explicitly mentions: 'Hierarchy of lgroups'.
How does Solaris leverage its lgroups for scheduling threads and allocating memory?,It schedules threads and allocates memory randomly across all lgroups.,"It always schedules threads and allocates memory within the same lgroup; if not possible, it uses nearby lgroups.",It schedules threads in one lgroup and allocates memory in another to distribute load.,"It only uses lgroups for kernel threads, not user threads.",It ignores lgroups for memory allocation and relies on a global free list.,B,"The text states: 'Solaris schedules threads/allocates memory within lgroup; if not possible, uses nearby lgroups'."
What are the two primary benefits achieved by Solaris's lgroups strategy in a NUMA environment?,Increased memory latency and decreased CPU cache hit rates.,Maximized system downtime and reduced overall throughput.,Minimizing memory latency and maximizing CPU cache hit rates.,Simplifying hardware design and increasing power consumption.,Enabling more direct disk access and reducing network traffic.,C,"The text states: 'Minimizes memory latency, maximizes CPU cache hit rates'."
"What defines the condition of ""thrashing"" in an operating system?",The system spending more time executing than paging.,A low page-fault rate combined with high CPU utilization.,The system spending more time paging than executing.,A process having more frames than its working set requires.,Increased system throughput due to efficient memory access.,C,"Thrashing is defined as spending more time paging than executing, indicating severe performance problems due to excessive page faults."
"What is an immediate consequence for a process that does not have ""enough"" frames, specifically the minimum needed for its working set?",It will execute faster than other processes.,It will quickly page-fault.,It will cause other processes to get more frames.,Its working set will automatically shrink.,It will decrease the overall CPU utilization directly.,B,A process without enough frames for its working set will quickly page-fault because it cannot hold the necessary pages in memory.
"If a process replaces a page that is needed immediately, what is the direct result?",The process enters a suspended state.,The process's working set becomes infinitely large.,The process faults again and again.,CPU utilization sharply increases.,The paging device queue becomes empty.,C,"If a process replaces a page it needs immediately, it will continuously fault to retrieve that page, leading to repeated page faults."
What is the primary impact of thrashing on system performance?,Increased system throughput.,Minimized page-fault rate.,Severe performance problems.,Optimized CPU utilization.,Reduced effective memory-access time.,C,The text states that thrashing 'Results in severe performance problems'.
"In a scenario where the OS monitors CPU utilization and observes it is low, what action does the CPU scheduler typically take that can lead to thrashing?",It decreases the degree of multiprogramming.,It suspends processes with high page-fault rates.,It increases the degree of multiprogramming.,It allocates more frames to existing processes.,It switches to a local page-replacement algorithm.,C,The text describes the scenario where 'CPU scheduler sees decreasing CPU utilization → increases multiprogramming.' This attempt to increase CPU utilization can inadvertently lead to thrashing.
"Which type of page-replacement algorithm, when used, contributes to the spiraling effect of thrashing by allowing processes to steal frames from others?",Local replacement algorithm,Priority replacement algorithm,Working-set algorithm,Global page-replacement algorithm,Page-fault frequency algorithm,D,"The text mentions, 'Global page-replacement algorithm used (replaces pages without regard to process),' as a cause where processes can take frames from others."
"When multiple processes are faulting excessively, what is the effect on the ready queue?",The ready queue becomes full of processes waiting for the CPU.,The ready queue empties as processes wait for the paging device.,The ready queue is unaffected.,Processes in the ready queue are automatically granted more frames.,Processes are moved from the ready queue to the suspended state.,B,"The text states, 'Faulting processes use paging device → ready queue empties.' Processes are waiting for I/O (paging device) rather than being ready for the CPU."
What happens to system throughput when thrashing occurs?,It increases slightly.,It remains stable.,It plunges sharply.,"It becomes negligible, approaching zero.",It fluctuates unpredictably.,C,"The text states, 'Thrashing occurs → system throughput plunges.'"
How does thrashing affect the effective memory-access time?,It decreases tremendously.,It remains constant.,It increases tremendously.,It becomes negligible.,It stabilizes at a minimal level.,C,"The text states, 'Page-fault rate increases tremendously → effective memory-access time increases.'"
"Based on Figure 10.6.1 (CPU utilization vs. degree of multiprogramming), what occurs if the degree of multiprogramming is increased beyond an optimal point, leading to thrashing?",CPU utilization continues to increase at a faster rate.,CPU utilization remains constant.,CPU utilization drops sharply.,CPU utilization experiences minor fluctuations.,Multiprogramming is automatically reduced.,C,"The text explains: 'Multiprogramming increases → CPU utilization increases (slower) until max. Further increase → thrashing, CPU utilization drops sharply.'"
What is the direct action suggested to stop thrashing once it has begun?,Increase the degree of multiprogramming.,Decrease the degree of multiprogramming.,Allocate more frames to all processes.,Switch to a global page-replacement algorithm.,Increase the size of the working-set window.,B,The text explicitly states: 'To stop thrashing: decrease degree of multiprogramming.'
Which type of page replacement algorithm can be used to limit the effects of thrashing by preventing processes from stealing frames from others?,Global replacement algorithm.,FIFO replacement algorithm.,Local replacement algorithm.,Optimal replacement algorithm.,Least Recently Used (LRU) algorithm.,C,The text indicates: 'Limit thrashing effects: use local replacement algorithm (or priority replacement algorithm).'
What is the defining characteristic of a local replacement algorithm?,It replaces pages based on their global frequency of use.,It allows a process to select a page to replace from any frame in memory.,It prioritizes pages belonging to system processes over user processes.,A process selects pages to replace only from its own allocated frames.,It considers the total demand for frames across all processes.,D,The text defines it as: 'Local replacement: process selects only from its own frames.'
"Even with a local replacement algorithm, why might thrashing-related performance issues not be entirely solved?",Processes can still steal frames from the OS.,"Thrashing processes queue for the paging device, increasing service time for all processes.",The CPU scheduler still increases multiprogramming in response to low CPU utilization.,It requires significantly more physical memory than a global algorithm.,It does not prevent individual processes from faulting.,B,The text states: 'Problem not entirely solved: thrashing processes queue for paging device → increased average service time for page fault → increased effective access time for all processes.'
What is the fundamental way to prevent thrashing from occurring in the first place?,Continuously increase the degree of multiprogramming.,Decrease the system's physical memory.,Provide each process with enough frames.,Use a global page-replacement algorithm.,Ignore page-fault rates.,C,The text explicitly states: 'To prevent thrashing: provide process with enough frames.'
What model is suggested for determining how many frames a process actually needs?,The FIFO model.,The Page-Fault Frequency model.,The Locality model.,The Global Replacement model.,The LRU model.,C,The text states: 'How many frames needed? Look at frames actually used → locality model.'
"In the context of the locality model, what is a ""locality""?",The total set of all pages a program might ever access.,A fixed-size block of physical memory.,The set of pages actively used together by a process.,The geographical location of the CPU.,A metric for measuring page-fault rate.,C,The text defines: 'Locality: set of pages actively used together.'
"According to the locality model, how does a process behave during execution?",It randomly accesses pages across its entire virtual address space.,"It primarily accesses pages from a single, fixed locality throughout its lifetime.",It moves from one locality to another during execution.,"It only accesses pages from global memory, never local variables.",It consistently maintains a fixed working set size.,C,The text describes: 'Locality model: process moves from locality to locality during execution.'
Which of the following is given as an example of a program entering a new locality?,A page fault occurring.,A context switch to a different process.,A function call.,The system booting up.,Increasing the degree of multiprogramming.,C,"The text provides the example: 'Example: function call → new locality (function instructions, local variables, global variables subset).'"
What happens if a process is not allocated enough frames to hold its current locality?,It will cause other processes to receive more frames.,It will suspend itself until more frames are available.,It will likely result in thrashing.,It will increase the system's effective access time.,It will decrease the page-fault rate.,C,The text states: 'Not enough frames for locality → thrashing.'
The working-set model is based on which fundamental assumption?,Random memory access patterns.,The principle of global page replacement.,The locality assumption.,The concept of infinite memory.,Constant CPU utilization.,C,The text begins the section with: 'Based on locality assumption.'
What is the name of the parameter used in the working-set model to define the window of page references?,Alpha (α),Delta (Δ),Sigma (Σ),Omega (Ω),Gamma (Γ),B,The text specifies: 'Uses parameter Δ to define working-set window.'
"In the working-set model, what constitutes the ""working set""?",All pages that have ever been referenced by a process.,The set of pages in the most recent Δ page references.,The total number of available frames in physical memory.,The pages that are currently on the backing store.,The pages shared by all processes.,B,The text defines: 'Set of pages in most recent Δ references = working set'.
How does a page transition out of the working set if it is no longer actively used?,It is immediately removed upon the next page fault.,It drops from the working set Δ time units after its last reference.,It remains in the working set indefinitely.,It is swapped out to backing store by the OS.,It is moved to a global pool of pages.,B,The text explains: 'No longer used → drops from working set Δ time units after last reference.'
What is the consequence if the Δ parameter in the working-set model is chosen to be too small?,The working set will encompass several overlapping localities.,The working set will become infinitely large.,It won't encompass the entire locality.,It will lead to zero page faults.,It will always prevent thrashing.,C,The text describes this as a drawback: 'Δ too small: won't encompass entire locality.'
"If Δ in the working-set model were infinite, what would the working set consist of?",Only the currently executing page.,All pages touched during execution.,An empty set of pages.,The set of pages from the previous locality.,Pages that are shared between processes only.,B,The text clarifies the extreme case: 'Extreme: Δ infinite → working set is all pages touched during execution.'
"According to the working-set model, when does thrashing occur regarding total demand for frames (D) and total available frames (m)?",When D < m.,When D = m.,When D > m.,When D is zero.,When m is infinite.,C,The text states the condition for thrashing: 'If D > m (total available frames) → thrashing (some processes lack frames).'
What action does the OS take when the sum of working-set sizes for all active processes exceeds the total available frames?,It increases the degree of multiprogramming.,It allocates more frames to all processes proportionally.,It suspends a process.,It automatically increases the physical memory.,"It ignores the demand, assuming processes will adapt.",C,The text outlines the OS behavior: 'Sum of working-set sizes exceeds available frames → OS suspends a process.'
Which of the following is a key benefit of using the working-set model for memory management?,It complicates tracking of process memory usage.,It inherently leads to lower CPU utilization.,"It prevents thrashing, keeps multiprogramming high, and optimizes CPU utilization.",It requires significantly less physical memory than other models.,It guarantees zero page faults.,C,"The text summarizes the benefits: 'Prevents thrashing, keeps multiprogramming high, optimizes CPU utilization.'"
What is a primary difficulty in implementing the working-set model in practice?,Calculating the total available frames.,Defining the working-set size for new processes.,Tracking the moving working-set window.,Suspending processes without user intervention.,Ensuring enough physical memory is always available.,C,The text identifies the challenge: 'Difficulty: tracking moving working-set window.'
Which two mechanisms are typically used together to approximate the working-set window?,Global timer and page table entries.,Fixed-interval timer interrupt and reference bit.,CPU utilization monitor and process ID.,Disk access patterns and cache hits.,Program counter and stack pointer.,B,The text states the approximation method: 'Approximation: fixed-interval timer interrupt + reference bit.'
What is the main problem that the Page-Fault Frequency (PFF) strategy aims to prevent?,Low CPU utilization.,Excessive process suspension.,Thrashing due to a high page-fault rate.,Inaccurate working-set approximations.,Insufficient multiprogramming.,C,The text states the problem addressed by PFF: 'Problem: prevent thrashing (high page-fault rate).'
"According to the Page-Fault Frequency (PFF) strategy, what action should be taken if a process's actual PFF exceeds an established upper limit?",Remove a frame from the process.,Suspend the process.,Allocate another frame to the process.,Decrease the degree of multiprogramming.,"Ignore the PFF, as it will self-correct.",C,The text specifies the PFF control mechanism: 'Actual PFF exceeds upper limit → allocate another frame.'
"According to the Page-Fault Frequency (PFF) strategy, what action should be taken if a process's actual PFF falls below an established lower limit?",Allocate another frame to the process.,Suspend the process.,Remove a frame from the process.,Increase the degree of multiprogramming.,Force the process to generate more page faults.,C,The text specifies the PFF control mechanism: 'Actual PFF falls below lower limit → remove a frame.'
"If, under the PFF strategy, the page-fault frequency increases for processes but there are no free frames available, what is the typical OS response?",Increase the physical memory size dynamically.,Force all processes to shrink their working sets.,"Select a process, swap it out to backing store, and reallocate its freed frames.",Discard pages from high-PFF processes to reduce their demand.,Temporarily disable page fault handling.,C,"The text outlines this scenario: 'If PFF increases and no free frames: select process, swap out to backing store. Freed frames distributed to high-PFF processes.'"
What is considered the best practice in current computing environments to avoid thrashing and excessive swapping?,Rely solely on advanced page replacement algorithms.,Continuously adjust the degree of multiprogramming.,Include enough physical memory.,Prioritize disk I/O over CPU utilization.,Implement sophisticated working-set tracking.,C,The text states the best practice: 'Best practice: include enough physical memory to avoid thrashing/swapping.'
"What is the main benefit of including enough physical memory, as per current best practice?",It makes all other memory management strategies obsolete.,It reduces the cost of system hardware.,It provides the best user experience.,It simplifies the working-set model implementation.,It makes the system immune to all forms of performance degradation.,C,The text states the outcome: 'Provides best user experience (smartphones to large servers).'
"According to the section glossary, what is the definition of ""thrashing""?",A low rate of paging memory.,Occurs when there is an abundance of physical memory.,High rate of paging memory; occurs when insufficient physical memory to meet virtual memory demand.,The process of allocating more frames to a process.,The total demand for frames by all processes.,C,The glossary defines thrashing as 'High rate of paging memory; occurs when insufficient physical memory to meet virtual memory demand.'
"As defined in the glossary, what is a ""local replacement algorithm""?",A page replacement algorithm that allows processes to steal frames from others.,A page replacement algorithm that prioritizes global efficiency over individual process needs.,A page replacement algorithm that avoids thrashing by not allowing a process to steal frames from other processes.,An algorithm used exclusively for prepaging.,An algorithm that determines the optimal working-set window.,C,The glossary defines local replacement algorithm as a 'Page replacement algorithm that avoids thrashing by not allowing a process to steal frames from other processes.'
"What is the ""locality model"" defined as in the section glossary?",A model for page replacement based on the working-set strategy.,A model that describes random memory accesses.,A model used to determine the optimal degree of multiprogramming.,A model for allocating shared memory segments.,A model that defines the page-fault frequency.,A,The glossary defines locality model as a 'Model for page replacement based on the working-set strategy.'
"According to the glossary, what is the ""working-set model""?",A model for determining the global page-fault rate.,A memory access model based on tracking the set of most recently accessed pages.,A model that defines the minimum number of frames required for any process.,A model used to detect deadlocks in memory management.,A model for prioritizing processes for CPU access.,B,The glossary defines working-set model as a 'Memory access model based on tracking the set of most recently accessed pages.'
"How is ""working-set window"" defined in the glossary?",The total number of pages in physical memory.,The maximum number of page faults allowed in a given time.,"Limited set of most recently accessed pages (a ""window"" view of the entire set of accessed pages).",A measure of CPU utilization over time.,The range of virtual addresses accessible by a process.,C,"The glossary defines working-set window as 'Limited set of most recently accessed pages (a ""window"" view of the entire set of accessed pages).'"
"What does the glossary define as the ""working set""?",All pages that have been modified since the last save.,The set of pages in the most recent page references.,The collection of all pages currently swapped out to disk.,The total amount of available physical memory.,The number of active processes in the system.,B,The glossary defines working set as 'The set of pages in the most recent page references.'
"What is ""page-fault frequency"" defined as in the glossary?",The total number of page faults that have occurred since system boot.,The average time taken to handle a page fault.,The frequency of page faults.,The rate at which pages are written to disk.,The maximum allowable page faults for a process.,C,The glossary defines page-fault frequency as 'The frequency of page faults.'
Which term in the glossary refers to an algorithm that functions similarly to a local replacement algorithm in preventing processes from stealing frames from others to avoid thrashing?,Locality model,Working-set model,Page-fault frequency,Priority replacement algorithm,Global replacement algorithm,D,"The glossary states 'priority replacement algorithm: Page replacement algorithm that avoids thrashing by not allowing a process to steal frames from other processes,' which is the same function described for the local replacement algorithm in the main text."
What memory management technique is memory compression primarily presented as an alternative to?,Paging,Swapping to disk,Virtualization,Cache management,Process scheduling,A,The text explicitly states: 'Alternative to paging: memory compression.'
What is the fundamental mechanism by which memory compression reduces memory usage?,It deallocates unused memory regions.,It increases the size of individual memory frames.,It consolidates the contents of several frames into a single frame.,It moves less frequently used data to a faster storage device.,It encrypts memory contents to reduce their footprint.,C,The text describes memory compression as the process to 'Compress several frames into a single frame.'
What is a key advantage of memory compression over traditional memory management techniques?,It eliminates the need for any form of memory management.,It increases the physical RAM capacity of a system.,It reduces memory usage without requiring pages to be swapped to disk.,It allows direct execution of programs from compressed files.,It provides a faster way to access data stored on secondary storage.,C,The text highlights that memory compression 'Reduces memory usage without swapping pages.'
"According to the example provided, what condition typically initiates the page replacement process that can lead to memory compression?",When the CPU utilization exceeds a certain threshold.,When the free-frame list goes above a predefined limit.,When the free-frame list falls below a specific threshold.,When an application requests more memory than is physically available.,When the system detects a memory leak in a running process.,C,The text states: 'Free-frame list below threshold -> triggers page replacement.'
"During the memory compression process, what happens to selected frames (e.g., 15, 3, 35) after they are identified for replacement, instead of writing them to swap space?",They are immediately discarded to free up space.,They are moved to a high-priority queue for immediate processing.,They are encrypted and then stored in a secure memory region.,They are compressed and stored together into a single page frame.,They are decompressed and spread across multiple available frames.,D,"The text specifies: 'Instead of writing to swap space, compress frames (e.g., three) into single page frame.'"
"In the memory compression example (Figure 10.7.2), what is the first action taken with the target frame (e.g., Frame 7) that will store the compressed data?",It is immediately written to swap space.,It is moved to the list of compressed frames.,It is removed from the free-frame list.,It is marked as read-only to prevent accidental overwrites.,It is filled with placeholder data to prepare for compression.,C,The text describes the process starting with: 'Frame 7 removed from free-frame list.'
"Following the compression of multiple frames (e.g., 15, 3, 35) into a single frame (e.g., Frame 7), what happens to the original individual frames (15, 3, 35)?",They are immediately swapped out to disk.,They are marked as unavailable for future use.,"They are moved to the free-frame list, becoming available for other uses.",They remain allocated to the original process until it terminates.,They are deleted from memory entirely.,C,"The text states: 'Frames 15, 3, 35 moved to free-frame list.'"
What is the system's response if a program attempts to reference a page that has been compressed and stored within a consolidated frame?,The system issues a warning and prevents access.,The compressed data is directly accessed without modification.,"A page fault occurs, leading to the decompression and restoration of the original pages.",The system automatically swaps the compressed frame to disk.,The reference is redirected to an alternative memory location.,C,"The text clearly states: 'If compressed frame referenced -> page fault, decompressed, restoring original pages.'"
How do mobile operating systems like Android and iOS generally handle standard swapping and paging mechanisms?,They rely heavily on standard swapping and paging.,They implement a significantly modified version of traditional swapping.,They do not generally support standard swapping/paging.,They use swapping and paging exclusively for critical system processes.,They offload all paging operations to a dedicated hardware component.,C,"The text specifies: 'Mobile systems (Android, iOS) generally don't support standard swapping/paging.'"
"Given the general lack of standard swapping/paging in mobile systems, what is the role of memory compression in their memory-management strategy?",It is a supplementary feature activated only under extreme memory pressure.,It is an experimental feature not widely adopted.,It is an integral component of their memory-management strategy.,It is an optional tool for developers to optimize app performance.,"It is primarily used for long-term data storage, not active memory management.",C,The text notes: 'Memory compression integral to their memory-management strategy.'
Which two major operating systems are explicitly mentioned as supporting memory compression?,Linux and Unix,Windows 7 and macOS (prior to 10.9),Windows 10 and macOS,Chrome OS and Android,Ubuntu and Fedora,C,The text states: 'Windows 10 and macOS support memory compression.'
"On Windows 10, which specific type of applications running on mobile devices are identified as candidates for memory compression?",Traditional Win32 applications,Command-line interface (CLI) tools,Universal Windows Platform (UWP) apps,Legacy .NET Framework applications,Games designed with DirectX 9,C,The text specifies: 'Windows 10: Universal Windows Platform (UWP) apps on mobile devices are candidates.'
"How does macOS (Version 10.9+) manage memory when free memory becomes short, regarding compression and paging?",It immediately pages all memory contents to disk.,"It first compresses frequently used pages, then pages the least recently used.","It compresses LRU (Least Recently Used) pages, and only then pages them if necessary.",It prioritizes paging critical system processes before considering compression.,It expands compressed data to free up space before resorting to paging.,C,"The text details: 'macOS (Version 10.9+): compresses LRU pages when free memory is short, then pages if needed.'"
"Based on performance tests on macOS, how does memory compression compare to paging to an SSD?",Paging to SSD is significantly faster.,Both methods exhibit similar performance characteristics.,Memory compression is faster than paging to SSD.,Memory compression often leads to system crashes on SSDs.,Paging to SSD is more power-efficient.,C,The text states: 'Performance tests: memory compression faster than paging to SSD on macOS.'
What is a necessary resource allocation requirement for memory compression to function?,It requires a large amount of dedicated swap space on the hard drive.,It requires a specialized hardware module for decompression.,It requires allocating free frames to store the compressed pages.,It requires a high-speed network connection for cloud-based storage.,It requires exclusive access to the CPU's L1 cache.,C,The text states: 'Memory compression requires allocating free frames for compressed pages.'
What is an example of the significant memory saving possible with memory compression as mentioned in the text?,Reducing 5 frames to 2,Reducing 4 frames to 2,Reducing 3 frames to 1,Reducing 2 frames to 1,Reducing 10 frames to 5,C,"The text provides an example: 'Significant memory saving possible (e.g., 3 frames to 1).'"
What two opposing factors are constantly contended with when designing memory compression algorithms?,CPU clock speed and memory bandwidth,Disk I/O speed and network latency,Compression speed and compression ratio,Power consumption and system temperature,Algorithm complexity and ease of implementation,C,The text identifies: 'Contention between compression speed and compression ratio (amount of reduction).'
What is generally true about memory compression algorithms that aim for higher compression ratios?,They are usually faster and less computationally intensive.,They require less memory to perform the compression.,They tend to be slower and more computationally expensive.,They are less effective at reducing the overall memory footprint.,"They are only suitable for static, non-changing data.",C,"The text clarifies: 'Higher compression ratios -> slower, more computationally expensive algorithms.'"
"How can the performance of memory compression algorithms be improved, according to the text?",By decreasing the number of available CPU cores.,By reducing the target compression ratio significantly.,By optimizing for single-core performance only.,By leveraging parallel compression using multiple cores.,By offloading all compression tasks to the GPU.,D,The text suggests: 'Improved by parallel compression using multiple cores.'
"Which of the following pairs are mentioned as examples of fast memory compression algorithms, and what is their typical effectiveness?","JPEG and MPEG, compressing to 90-95% of original size.","Microsoft's Xpress and Apple's WKdm, compressing to 30-50% of original size.","ZIP and RAR, compressing to 10-20% of original size.","Lempel-Ziv and Huffman, compressing to 70-80% of original size.","Gzip and Bzip2, compressing to 50-60% of original size.",B,"The text provides: 'Examples: Microsoft's Xpress, Apple's WKdm -> fast, compress to 30-50% original size.'"
"According to the Section Glossary, what is the definition of 'memory compression'?",A technique for permanently deallocating memory regions.,A method to store unused memory frames on a solid-state drive.,An alternative to paging; compresses frame contents to decrease memory usage.,A process of encrypting data stored in RAM for security.,A system that allows the CPU to access memory faster.,C,The glossary defines 'memory compression' as 'Alternative to paging; compresses frame contents to decrease memory usage.'
"Based on the Section Glossary, what does 'Universal Windows Platform (UWP)' refer to?",A universal standard for USB device connectivity across all operating systems.,A virtualization technology that allows Windows apps to run on any device.,A Windows 10 architecture providing a common app platform for all devices running it.,A cross-platform development environment for Linux and macOS.,A cloud-based service for storing and accessing Windows applications.,C,The glossary defines 'universal Windows platform (UWP)' as 'Windows 10 architecture providing common app platform for all devices running it.'
"What is measured by 'compression ratio' in the context of memory compression, according to the Section Glossary?",The speed at which data can be compressed per second.,The amount of CPU resources consumed during compression.,The effectiveness of compression (ratio of compressed to uncompressed space).,The maximum number of frames that can be compressed at once.,The time it takes to decompress a compressed frame.,C,The glossary defines 'compression ratio' as 'Measurement of compression effectiveness (ratio of compressed to uncompressed space).'
How are pages typically allocated when a user-mode process requests memory?,From a dedicated user-mode heap.,Directly from physically contiguous memory.,From the kernel's free page frame list.,By allocating entire slabs from the kernel pool.,Via the buddy system allocator.,C,The text states that 'User-mode process requests memory -> pages allocated from kernel's free page frame list.'
"What is a common consequence of a user-mode process requesting a small amount of memory (e.g., a single byte) when pages are allocated from the kernel's free page frame list?",External fragmentation.,Memory thrashing.,Internal fragmentation.,Page fault.,Memory leak.,C,The text explains: 'Single byte request -> entire page frame granted -> internal fragmentation.'
How is the kernel's free page frame list primarily populated?,By kernel module insertions.,Through the process of booting the system.,By page-replacement algorithms.,By user-mode application memory releases.,By the system's power-of-2 allocator.,C,"The text specifies: 'List populated by page-replacement algorithms (e.g., Section 10.4).'"
Which characteristic describes the physical location of free pages available to user-mode processes?,They are always physically contiguous.,They are typically scattered throughout physical memory.,"They are reserved in a single, large block.",They are allocated in fixed-size segments only.,They are always located at the start of physical memory.,B,The text states: 'Free pages scattered throughout physical memory.'
Which of the following is a primary reason why kernel memory is often allocated from a different free-memory pool than user-mode memory?,To prevent user-mode processes from accessing kernel data.,"Kernel requests are always for very large, fixed-size blocks.","Kernel requests often involve varying data structure sizes, some less than a page.","User-mode processes require physically contiguous memory, unlike the kernel.",Kernel memory is volatile and frequently swapped out.,C,"One stated reason is: 'Kernel requests varying data structure sizes, some less than a page. Must use memory conservatively, minimize fragmentation waste.'"
What is a common practice in many operating systems regarding kernel code and data in relation to paging?,They are always subjected to aggressive paging.,They are typically not subjected to paging.,They are paged out only under extreme memory pressure.,They reside exclusively in swap space.,They are fragmented across many non-contiguous pages.,B,The text notes: 'Many OS do not subject kernel code/data to paging.'
"Why might kernel memory sometimes require physically contiguous pages, even though user-mode pages don't necessarily need them?",For improved cache performance.,To simplify virtual memory address translation for the kernel.,Because hardware devices often interact directly with physical memory without a virtual memory interface.,To ensure better security isolation between kernel modules.,To facilitate faster context switching between kernel threads.,C,The text states: 'Hardware devices interact directly with physical memory (no virtual memory interface). May require physically contiguous pages.'
Which two strategies are specifically mentioned for managing kernel free memory?,Paging and Segmentation.,First-fit and Best-fit.,Buddy system and Slab allocation.,Swapping and Demand Paging.,External fragmentation and Internal fragmentation.,C,"The text lists: 'Strategies for managing kernel free memory: ""buddy system"" and ""slab allocation"".'"
From what kind of memory segment does the buddy system primarily allocate memory?,Fixed-size segments of virtually contiguous pages.,Dynamically sized segments from a global heap.,Fixed-size segments of physically contiguous pages.,Variable-size pages from the user-mode pool.,Smallest available page frames from scattered physical memory.,C,The text states the buddy system 'Allocates memory from fixed-size segment of physically contiguous pages.'
What does a 'power-of-2 allocator' in the buddy system imply about how memory requests are satisfied?,Requests are always rounded down to the nearest power of 2.,Memory is allocated in units that are multiples of 2.,Requests are satisfied in units sized as a power of 2.,"The system can only allocate 2, 4, 8, 16, or 32 KB segments.",It uses a binary tree structure for allocation tracking.,C,The text and glossary define 'power-of-2 allocator' as satisfying 'requests in units sized as a power of 2.'
"If a memory request in a buddy system is not appropriately sized (e.g., 11 KB), how is it satisfied?",It is rejected due to invalid size.,It is rounded down to the next lowest power of 2.,It is satisfied with an exact match from a pre-allocated pool.,It is rounded up to the next highest power of 2.,It is split into multiple smaller requests.,D,The text states: 'Request not appropriately sized -> rounded up to next highest power of 2. Example: 11 KB request -> satisfied with 16-KB segment.'
What is the primary advantage of the coalescing technique in the buddy system?,It reduces external fragmentation by compacting memory.,It allows for quick combination of adjacent buddies to form larger segments.,It ensures that all allocated memory is physically contiguous.,It minimizes the overhead of tracking free memory blocks.,It pre-allocates memory to specific kernel data structures.,B,The text states: 'Advantage: quickly combine adjacent buddies to form larger segments using coalescing.'
What is a significant drawback of the buddy system related to memory utilization?,It suffers from severe external fragmentation.,It cannot allocate physically contiguous memory.,It causes internal fragmentation due to rounding up requests.,It has high overhead for memory allocation and deallocation.,It is only suitable for very small memory requests.,C,The text identifies 'rounding up to next highest power of 2 causes internal fragmentation' as a drawback.
"If a 33-KB memory request is made to a buddy system allocator, what size segment will typically be allocated, leading to internal fragmentation?",32 KB.,48 KB.,64 KB.,128 KB.,An exact 33 KB segment.,C,The text provides this specific example: 'Example: 33-KB request -> 64-KB segment allocated.'
"In the buddy system, what is the maximum percentage of an allocated unit that cannot be guaranteed to be *less* than wasted due to internal fragmentation?",10%,25%,50%,75%,100%,C,The text states: 'Cannot guarantee less than 50% of allocated unit wasted.'
"In the context of slab allocation, what is a 'slab'?",A single object that has been freed.,A temporary data copy used for performance.,One or more physically contiguous pages.,A queue of available kernel objects.,A block of memory of a power-of-2 size.,C,The text and glossary define a 'slab' as 'one or more physically contiguous pages.'
"According to the text, what does a 'cache' consist of within the slab allocation system?",A single kernel object.,One or more slabs.,A collection of diverse kernel data structures.,A power-of-2 sized memory segment.,A list of free page frames.,B,The text and glossary define a 'cache' as 'one or more slabs.'
For what purpose is a single cache typically created in slab allocation?,For all available free memory.,For each unique kernel data structure.,For user-mode process page tables.,For temporary file system buffers.,For physically contiguous memory blocks only.,B,"The text states: 'Single cache for each unique kernel data structure (e.g., process descriptors, file objects, semaphores).'"
What are caches in the slab allocation system populated with?,Empty memory pages.,Pointers to physically contiguous blocks.,"Objects, which are instantiations of kernel data structures.",Binary trees representing free memory.,Power-of-2 sized memory chunks.,C,The text states: 'Each cache populated with objects (instantiations of kernel data structure).'
"When a cache is created in the slab allocation algorithm, what is the initial state of the objects allocated to it?",All objects are marked `used`.,"Some objects are `used`, and some are `free`.",All objects are initially `free`.,Objects are only allocated upon request.,Objects are pending deletion.,C,The text states: 'Cache created -> objects (initially `free`) allocated to cache.'
"When a new object is needed in the slab allocation algorithm, how does the allocator fulfill the request?",By allocating a brand new page frame.,By creating a new slab and placing the object there.,By assigning any `free` object from the cache and marking it `used`.,By searching for a power-of-2 sized block.,By deallocating an existing `used` object.,C,The text explains: 'New object needed -> allocator assigns any `free` object from cache. Object marked `used`.'
"In Linux's slab states, what does it mean for a slab to be classified as 'Full'?",It has exhausted its contiguous physical memory.,All objects within that slab are marked `used`.,It contains both `used` and `free` objects.,It is awaiting new objects to be allocated.,Its cache is entirely full.,B,The text defines 'Full' as: 'All objects in slab `used`.'
What is the characteristic of a slab classified as 'Empty' in Linux's slab allocator?,It is ready to be deallocated.,All objects in the slab are marked `free`.,It has at least one `used` object.,It has been recently created but has no objects yet.,It has insufficient contiguous pages.,B,The text defines 'Empty' as: 'All objects in slab `free`.'
A slab is in the 'Partial' state in Linux when:,It has some free space but is mostly full.,It has not yet been assigned to a specific cache.,It has both `used` and `free` objects.,It contains only objects that are partially initialized.,It requires more physically contiguous pages.,C,The text defines 'Partial' as: 'Slab has both `used` and `free` objects.'
"When the slab allocator needs to satisfy a request, what is its first priority for obtaining a free object?",Allocating a new slab from contiguous physical pages.,Requesting memory from the buddy system.,Finding a free object in an empty slab.,Finding a free object in a partial slab.,Waiting for an object to be released and returned to the cache.,D,The text outlines the order: 'First: free object in a partial slab.'
"If the slab allocator cannot find a free object in a partial slab, what is its next step to satisfy a request?",It allocates a new slab.,It searches for an object in another cache.,It looks for a free object from an empty slab.,It triggers a page fault to get more memory.,It rounds up the request to the next power of 2.,C,The text outlines the order: 'If none [partial slab objects]: free object from an empty slab.'
Under what circumstances does the slab allocator allocate a new slab from contiguous physical pages to satisfy a request?,Whenever a `used` object is released.,Only for very large kernel data structures.,If there are no free objects in partial or empty slabs.,When the system is booting up.,When a cache becomes 'Full.',C,"The text outlines the order: 'If no empty slabs: new slab allocated from contiguous physical pages, assigned to cache; object memory allocated from new slab.'"
Which benefit is directly associated with the slab allocator's approach to memory management regarding fragmentation?,It eliminates external fragmentation by compacting memory.,It minimizes internal fragmentation by rounding up requests.,"No memory is wasted due to fragmentation, as exact amounts are returned.",It converts internal fragmentation into external fragmentation.,It uses a power-of-2 allocator to reduce fragmentation.,C,The text lists as a main benefit: 'No memory wasted due to fragmentation. ... Kernel requests memory -> exact amount returned.'
Another key benefit of the slab allocator is its ability to satisfy memory requests quickly. Why is this possible?,It relies on page replacement algorithms for fast page retrieval.,It combines adjacent free memory blocks instantly.,Objects are created in advance and readily available in caches.,It uses a smaller total memory footprint than other allocators.,It avoids interactions with hardware devices.,C,The text states: 'Memory requests satisfied quickly. ... Objects created in advance -> quickly allocated from cache.'
In which operating system kernel did the slab allocator first appear?,Linux 2.2+,Windows NT,Solaris 2.4,macOS (OS X),UNIX System V,C,The text states: 'First appeared in Solaris 2.4 kernel.'
For what specific type of systems is the SLOB allocator in Linux designed?,High-performance servers with ample memory.,"Systems with limited memory, such as embedded systems.",Multi-processor systems requiring per-CPU queues.,Systems primarily running user-mode applications.,Systems that use virtual memory extensively.,B,"The text states: 'SLOB allocator: for systems with limited memory (e.g., embedded systems).'"
Which of the following describes the memory lists maintained by the SLOB allocator for different object sizes?,"Small (<1 KB), Medium (<4 KB), Large (>4 KB).","Tiny (<128 bytes), Small (<512 bytes), Medium (<2 KB).","Small (<256 bytes), Medium (<1,024 bytes), Large (other objects < page size).","Blocks (fixed-size), Pages (variable-size), Slabs (contiguous).","Allocated, Free, Partial.",C,"The text specifies: 'Maintains three lists: `small` (<256 bytes), `medium` (<1,024 bytes), `large` (other objects < page size).'"
What allocation policy does the SLOB allocator use to fulfill requests from its internal lists?,Best-fit.,Worst-fit.,First-fit.,Next-fit.,Buddy system.,C,The text states: 'Allocates from appropriate list using first-fit policy.'
What is the current status of the SLUB allocator in the Linux kernel (Version 2.6.24+)?,It is an optional allocator for specific hardware.,"It is the default allocator, having replaced SLAB.",It is used only for very large memory allocations.,It is deprecated and no longer maintained.,It is primarily for user-mode memory management.,B,"The text states: 'SLUB allocator: default for Linux kernel (Version 2.6.24+), replaced SLAB.'"
One way SLUB reduced SLAB overhead was by storing metadata where?,With each object in the slab.,"In a separate, dedicated metadata cache.","In the `page` structure, not with each slab.",In per-CPU queues.,In the kernel's virtual address space.,C,The text states: 'Stores metadata in `page` structure (not with each slab).'
"Which significant memory saving feature does SLUB offer, particularly beneficial on multi-processor systems, compared to SLAB?",Reduced internal fragmentation for small objects.,Elimination of external fragmentation.,No per-CPU queues for objects.,Dynamic resizing of slabs.,Faster coalescing of freed memory.,C,The text states: 'No per-CPU queues for objects (significant memory saving on multi-processor systems).'
"How does the SLUB allocator generally perform, especially in systems with more processors, compared to SLAB?",It has slightly worse performance due to increased complexity.,It shows no significant difference in performance.,It provides better performance with more processors.,Its performance degrades with an increasing number of processors.,It is optimized for single-processor systems only.,C,The text states: 'Better performance with more processors.'
"In the context of the buddy system, what are 'buddies'?",Memory requests that are rounded up to the next power of 2.,Two adjacent memory segments of unequal size.,Pairs of equal size in buddy memory allocation.,Kernel objects stored in a cache.,Memory blocks that are permanently allocated.,C,The glossary defines 'buddies' as 'Pairs of equal size in buddy memory allocation.'
Which of the following best describes the process of 'coalescing' in memory management?,Breaking down a large memory segment into smaller power-of-2 units.,Satisfying a memory request by rounding it up to the next highest power of 2.,Combining freed memory in adjacent buddies into larger segments.,Allocating pre-initialized objects from a cache.,Moving active memory pages to secondary storage.,C,The glossary defines 'coalescing' as 'Combining freed memory in adjacent buddies into larger segments.'
What is the fundamental principle of 'slab allocation' as a memory management method?,Allocating memory in power-of-2 sized blocks only.,Splitting a slab into object-sized chunks to eliminate fragmentation.,Relocating non-contiguous memory blocks to form larger segments.,Always returning an entire page frame for any request size.,Using a first-fit policy for all memory requests.,B,"The glossary defines 'slab allocation' as a 'Memory allocation method; slab split into object-sized chunks, eliminating fragmentation.'"
"In the context of kernel memory allocation, what does 'object' refer to?",A physically contiguous block of memory.,A power-of-2 sized memory unit.,An instance of a class or data structure.,A freed memory segment in the buddy system.,A virtual memory page.,C,The glossary defines 'object' as an 'Instance of a class or data structure.'
The text mentions `struct task_struct` as an example. What kind of kernel object is `struct task_struct`?,A file object.,A semaphore.,A process descriptor.,A cache.,A slab.,C,"The text states: 'Scenario: kernel requests memory for process descriptor (struct task_struct, ~1.7 KB).'"
What is the primary problem that 'pure demand paging' often faces when a process initially starts?,Excessive CPU utilization due to constant context switching.,"A large number of page faults, especially in the initial phase.",Insufficient physical memory leading to thrashing.,Difficulty in allocating contiguous memory blocks.,High overhead from TLB misses.,B,"Pure demand paging experiences a large number of page faults when a process starts, particularly due to initial locality, as pages are brought in only when referenced."
What is the main purpose of 'prepaging'?,To reduce the overall memory footprint of a process.,To prevent high initial paging activity when a process begins.,To ensure that all pages are modified before being written to disk.,To increase the system's bus transfer rate.,To optimize the page replacement algorithm.,B,"Prepaging is an attempt to prevent high initial paging, which is characteristic of pure demand paging when a process starts."
Which strategy describes how prepaging attempts to reduce initial page faults?,It limits the number of pages a process can request at once.,It brings all necessary pages into memory only when they are referenced.,It attempts to bring some or all potentially needed pages into memory at once.,It swaps out infrequently used pages to secondary storage.,It prioritizes pages based on their modification status.,C,"Prepaging's strategy is to bring some or all needed pages into memory at once, proactively, before they are explicitly requested by a page fault."
"In the context of prepaging, which model is cited as an example where the working set for a suspended process can be remembered and brought back upon resumption?",Least Recently Used (LRU) model,"First-In, First-Out (FIFO) model",Working-set model,Optimal replacement model,Clock replacement model,C,"The working-set model is given as an example for prepaging, where the working set of a suspended process is remembered and automatically brought back into memory upon process resumption."
"What is the primary advantage of prepaging, as discussed in the text?",It eliminates the need for a page table.,It reduces the cost of context switching.,It can potentially reduce total overhead by trading prepaging cost against page fault servicing cost.,It guarantees that no page will ever be swapped out.,It always results in higher memory utilization.,C,"The advantage of prepaging is the potential trade-off between the cost of prepaging and the cost of servicing individual page faults. If the prepaged pages are indeed used, it saves on the overhead of multiple page faults."
What is a significant risk associated with prepaging?,It can lead to an increase in TLB misses.,It may cause system thrashing if too many processes are prepaged.,"Many of the prepaged pages may not actually be used, wasting memory and I/O.",It requires significant hardware changes to implement.,It complicates the process of allocating virtual memory.,C,"A significant risk of prepaging is that many of the pages brought into memory proactively may not be used, leading to wasted memory and I/O resources."
"In the cost analysis of prepaging, if 's' pages are prepaged and 'α' is the fraction of those pages actually used, under what condition does prepaging 'win' (i.e., is beneficial)?",When α is approximately 0.,When α is approximately 0.5.,When α is approximately 1.,When s is very large.,When the cost of servicing a page fault is very low.,C,"Prepaging wins if 'α' (the fraction of prepaged pages actually used) is approximately 1, meaning most prepaged pages are indeed utilized, saving numerous page faults."
"Regarding prepaging, what is generally true about prepaging executable programs versus files?",Prepaging executable programs is more predictable due to their sequential nature.,Prepaging files is more difficult due to unclear page requirements.,Prepaging executable programs is typically easier as all code is needed upfront.,Prepaging files is more predictable because they are often accessed sequentially.,Neither executable programs nor files can be effectively prepaged.,D,"Prepaging files is described as more predictable because files are often accessed sequentially, making it easier to determine which pages might be needed next. Prepaging executable programs is noted as difficult."
Which Linux system call is mentioned as a mechanism for prefetching file contents into memory?,fork(),mmap(),readahead(),execve(),sync(),C,"The Linux `readahead()` system call is specifically mentioned as a way to prefetch file contents into memory, exemplifying prepaging for files."
"When designing a new machine, what is a key consideration regarding page size?","There is a single, universally best page size for all systems.","Page sizes are typically fixed at 4KB, regardless of system design.",The optimal page size depends on various factors and there is no single best size.,Page sizes must always be prime numbers.,Page size is irrelevant to system performance.,C,"The text states that there is no single best page size, and the decision depends on various factors that support different sizes."
What is the typical characteristic of page sizes in computer systems?,They are invariably odd numbers.,They are always multiples of 10.,"They are invariably powers of 2, typically ranging from 4,096 to 4,194,304 bytes.",They are determined by the amount of available RAM.,"They are always less than 1,024 bytes.",C,"Page sizes are invariably powers of 2, typically ranging from 4,096 ($2^{12}$) to 4,194,304 ($2^{22}$) bytes."
How does decreasing the page size affect the page table size?,"It decreases the number of pages, thus decreasing the page table size.","It increases the number of pages, thus increasing the page table size.",It has no effect on page table size.,It makes page table size dynamic and unpredictable.,It allows page tables to be stored entirely in hardware.,B,"Decreasing the page size means a given virtual memory space will be divided into more pages, thereby increasing the number of entries needed in the page table, which increases the page table size."
"For a 4 MB virtual memory, which page size would result in a smaller page table?","1,024 bytes per page","8,192 bytes per page",Both result in the same page table size.,Page table size is not affected by page size.,It depends on the number of active processes.,B,"Larger page sizes result in fewer pages for a given virtual memory space, thus reducing the size of the page table. For 4 MB virtual memory, 8,192 bytes per page (512 pages) results in a smaller page table than 1,024 bytes per page (4,096 pages)."
Which page size is generally desirable for minimizing the size of the page table?,"Very small page sizes (e.g., 64 bytes)","Medium page sizes (e.g., 4KB)",Large page sizes,Page size has no impact on page table size.,Dynamic page sizes that change during runtime.,C,"Large page sizes are desirable for page table size because fewer pages are needed to cover the same virtual address space, resulting in a smaller page table for each active process."
What is 'internal fragmentation' in the context of memory utilization with paging?,Memory lost due to pages being swapped out to disk.,"Unused space within an allocated page, because a process typically doesn't end exactly on a page boundary.","Memory fragmented into many small, non-contiguous blocks.",Overhead associated with managing the page table.,Memory wasted by system calls.,B,"Internal fragmentation refers to the unused part of the final page allocated to a process, as a process rarely ends exactly on a page boundary, leaving some space within that page allocated but unused."
"To minimize internal fragmentation, what characteristic should the page size have?",It should be as large as possible.,It should be a prime number.,It should be a small page size.,It should be equal to the process's total memory requirement.,It should be dynamically adjusted during execution.,C,"Minimizing internal fragmentation requires a small page size, as the average waste is half of the final page; a smaller page means less wasted space."
"Which component of I/O time is proportional to the page size, arguing for a small page size?",Seek time,Latency time,Transfer time,Queueing time,Processing time,C,"Transfer time, which is the time to actually move the data, is proportional to the page size. This factor, by itself, argues for a smaller page size to reduce transfer time."
Why do larger page sizes generally lead to less total I/O time despite increased transfer time per page?,Because larger pages fit better into disk cache.,"Because latency and seek times often dwarf the transfer time, making fewer, larger transfers more efficient.",Because larger pages enable parallel I/O operations.,Because they reduce the overall number of memory references.,Because they are easier for the memory controller to manage.,B,"Latency and seek times contribute significantly to total I/O time and are incurred once per I/O operation. By having larger pages, fewer I/O operations are needed, so even if transfer time per page increases, the reduction in seek and latency overheads often results in less total I/O time."
"In the context of I/O time, what does the text argue regarding page size?",Smaller page sizes minimize total I/O time.,Larger page sizes minimize total I/O time.,I/O time is independent of page size.,Only transfer time affects I/O efficiency.,Optimal I/O time is achieved with highly variable page sizes.,B,"The text argues that to minimize I/O time, a larger page size is needed, as the fixed overheads (seek, latency) dwarf the transfer time, making fewer, larger transfers more efficient."
How does a smaller page size affect locality and resolution in memory management?,"It decreases locality and resolution, leading to more unused data being brought in.","It improves locality and resolution, isolating more precisely the memory actually needed.",It has no impact on locality but significantly increases I/O.,It complicates memory management without tangible benefits.,It forces the system to use external fragmentation.,B,"Smaller page sizes are said to improve locality and resolution because each page can match a program's locality more accurately, allowing the system to isolate only the memory actually needed, thereby reducing total I/O and allocated memory."
What is the primary effect of a larger page size on the number of page faults?,It increases the number of page faults.,It significantly reduces the number of page faults.,It has no impact on the number of page faults.,It increases the overhead of each page fault.,It only affects page faults for very small processes.,B,"Larger page sizes mean that a single page can encompass a larger portion of a process's working set, reducing the frequency of page faults. The text provides an example where a 200 KB page results in 1 page fault, compared to 102,400 faults for 1-byte pages for a 200 KB process."
What has been the historical trend in page size selection for computer systems?,A consistent move towards smaller page sizes.,A fluctuation between large and small page sizes without a clear trend.,"A trend toward larger page sizes, even for mobile systems.",Page sizes becoming obsolete in modern architectures.,A fixed page size adopted universally since the 1990s.,C,"The historical trend has been toward larger page sizes, and modern systems, including Linux with 'huge pages', use much larger page sizes."
What is the definition of 'hit ratio' in the context of a TLB?,The speed at which the TLB can translate an address.,The percentage of virtual address translations that are resolved by the TLB.,The total amount of memory that the TLB can address.,The number of entries in the TLB.,The ratio of TLB misses to page faults.,B,Hit ratio of TLB is defined as the percentage of virtual address translations resolved in the TLB.
How is 'TLB reach' calculated?,Number of TLB entries divided by page size.,Number of TLB entries multiplied by page size.,Total virtual memory divided by number of TLB entries.,Physical memory size multiplied by page size.,Hit ratio multiplied by total memory.,B,TLB reach is calculated as the number of entries in the TLB multiplied by the page size.
What is the ideal scenario for a process regarding TLB reach?,The TLB hit ratio should be exactly 50%.,The process's entire working set should be able to fit within the TLB's reach.,The TLB should only contain entries for the kernel space.,The TLB should be as small as possible to conserve power.,The TLB should manage only physical addresses.,B,"Ideally, the working set for a process should be stored within the TLB to minimize the time spent resolving memory references in the page table."
What is a consequence of insufficient TLB reach for a process?,Increased internal fragmentation.,The process spends more time resolving memory references in the page table.,A higher frequency of page faults.,Reduced CPU utilization.,Physical memory becomes over-allocated.,B,"If the TLB reach is insufficient, the process will spend more time resolving memory references by traversing the page table in main memory, which is much slower than a TLB lookup."
"How can TLB reach be increased, according to the text?",By decreasing the number of TLB entries.,By decreasing the page size.,By increasing the number of TLB entries or by increasing the page size.,By reducing the processor's clock speed.,By using a software-only memory management unit.,C,"TLB reach can be increased by either doubling the number of TLB entries (which can be expensive) or by increasing the page size (e.g., from 4 KB to 16 KB, quadrupling the reach)."
What is a potential downside of increasing the page size to improve TLB reach?,It always leads to a decrease in the TLB hit ratio.,It can lead to increased internal fragmentation for some applications.,It significantly increases the cost of TLB entries.,It makes it impossible to support multiple page sizes.,It requires frequent flushing of the TLB.,B,"While increasing page size helps TLB reach, a stated downside is increased fragmentation for some applications, as more unused space might be allocated within larger pages."
Which ARM v8 architecture TLB entry bit is used to indicate that an entry maps contiguous (adjacent) blocks of memory?,Present bit,Dirty bit,Contiguous bit,Accessed bit,Execute bit,C,"The ARM v8 TLB entry includes a 'contiguous bit' which, when set, indicates that the entry maps contiguous (adjacent) blocks of memory."
What is the primary purpose of using inverted page tables?,To accelerate virtual-to-physical address translations.,To reduce the physical memory needed for virtual-to-physical address translations.,To eliminate the need for secondary storage in paging systems.,To provide a direct mapping from physical addresses to virtual addresses.,To increase the system's TLB hit ratio.,B,The main purpose of inverted page tables is to reduce the amount of physical memory required to store virtual-to-physical address translation information.
How do inverted page tables typically structure their entries?,"One entry per virtual page, indexed by process ID.","One entry per process, listing all its virtual pages.","One entry per page of physical memory, indexed by a combination of process-id and page-number.","One entry per CPU core, mapping its memory regions.","Multiple entries for each virtual page, for redundancy.",C,Inverted page tables work by having one entry for each page of physical memory. This entry is then typically indexed by a combination of process-id and page-number to locate the corresponding virtual page.
What is a significant downside of inverted page tables compared to traditional per-process page tables?,They require more physical memory for translation information.,They cannot support demand paging.,They no longer contain complete information about a process's logical address space.,They increase the TLB miss rate.,They are much slower for address translation.,C,"A significant downside is that inverted page tables no longer contain complete information about the logical address space of a process, which is necessary for handling page faults effectively."
How do systems using inverted page tables solve the problem of not having complete logical address space information for demand paging?,They store all logical address information directly in the TLB.,They use an external page table (one per process) that contains virtual page location information.,They entirely avoid demand paging.,They consult the disk for every address translation.,They dynamically regenerate logical address space information on demand.,B,"The solution is to keep an external page table (one per process), which functions like a traditional page table and contains the necessary virtual page location information. These external tables are only referenced on page faults."
What is the primary benefit of designing demand paging to be transparent to the user program?,It allows programs to run on systems with less physical memory than their virtual address space.,It simplifies the compilation process for all programming languages.,It removes the need for hardware-level memory management units.,It ensures that all I/O operations are handled asynchronously.,It prevents memory leaks from occurring in user applications.,A,"Demand paging's transparency allows programs to use a virtual address space that can be much larger than the available physical memory, without the programmer needing to manage memory explicitly."
"How can system performance with demand paging be improved, even though it's designed to be transparent?",By strictly enforcing a minimum page size.,By making the user or compiler aware of the paging mechanism and optimizing program structure.,By disabling the TLB for certain applications.,By always prepaging the entire process at startup.,By increasing the frequency of page table updates.,B,"The text states that system performance can be improved if the user or compiler is aware of demand paging, allowing for optimizations in program structure and data access patterns."
Consider initializing a 128x128 integer array on a system with 128-word pages. Which access order would result in significantly fewer page faults if less than 128 frames are allocated?,"Row-major order (	exttt{data[i][j]} with outer loop 	exttt{j}, inner loop 	exttt{i})","Column-major order (	exttt{data[i][j]} with outer loop 	exttt{i}, inner loop 	exttt{j})",Both orders would result in the same number of page faults.,A random access order would be most efficient.,"The number of page faults depends only on the total array size, not access order.",B,"Column-major order (outer loop 'i', inner loop 'j') results in much better locality for a C-style row-major stored array, zeroing all words on one page before moving to the next, drastically reducing page faults (from 16,384 to 128 in the example)."
Which programming construct is given as an example of good locality of reference?,A hash table,A linked list,A stack,A binary search tree,A sparse matrix,C,"A stack is given as an example of good locality because access always occurs at the top, meaning references are concentrated within a small, contiguous memory region."
Which programming construct is given as an example of bad locality of reference?,An array processed sequentially,A stack,A queue,A hash table,A tightly-packed struct,D,"A hash table is cited as an example of bad locality because it tends to scatter references across different memory locations, leading to more page faults."
How can separating code and data segments and using reentrant code improve paging performance?,It allows the system to completely eliminate page faults.,"It ensures that code pages are read-only and never modified, thus becoming 'clean pages' that don't need to be paged out.",It reduces the total physical memory required by the system.,It simplifies the process of creating shared libraries.,It increases the frequency of TLB hits for data segments.,B,"Separating code and data, and using reentrant code, means code pages are read-only. This makes them 'clean pages' that do not need to be written back to disk when replaced, improving performance."
What optimization can a loader perform to improve paging performance?,Always placing routines across page boundaries.,Avoiding placing routines across page boundaries to keep them within one page.,Randomly distributing routine segments in memory.,Increasing the number of virtual address spaces.,Reducing the total number of physical memory frames.,B,"A loader can improve performance by avoiding placing routines across page boundaries, aiming to keep each routine entirely within a single page to reduce page faults."
What is the primary reason pages need to be 'locked' in memory in demand paging systems?,To prevent unauthorized access to critical system data.,To ensure that I/O buffers are not paged out while an I/O operation is in progress.,To optimize the TLB hit ratio for frequently accessed pages.,To reduce internal fragmentation in memory.,To increase the system's overall memory capacity.,B,"Pages need to be locked in memory to prevent I/O buffers from being paged out to disk while a separate I/O processor is accessing them, which would lead to incorrect data transfer."
Which of the following describes a problem scenario that necessitates page locking?,A process frequently accesses pages that are scattered across different physical frames.,A low-priority process acquires a page that a high-priority process needs.,"An I/O operation targets a user memory buffer, but that buffer's page is replaced before I/O completes.",The system runs out of physical memory due to too many active processes.,"The page table grows too large, slowing down address translation.",C,"The problem scenario described is when a process issues I/O to a buffer, but that buffer's page is paged out (replaced) before the I/O operation finishes, leading to data corruption or errors."
"What is one common solution to the I/O interlock problem, besides copying data to/from system memory?",Increasing the page size to reduce page faults.,Using a dedicated I/O processor that only accesses system memory.,Allowing pages to be locked into memory using a 'lock bit' associated with each frame.,Disabling demand paging during I/O operations.,Implementing a faster page replacement algorithm.,C,"One common solution is to allow pages to be locked into memory. This is typically done by setting a 'lock bit' associated with each memory frame, preventing it from being selected for replacement."
What is the function of a 'lock bit' associated with a memory frame?,"It marks the page as dirty, indicating it needs to be written to disk.",It prevents the frame from being selected for replacement by the page replacement algorithm.,It indicates that the page is currently being accessed by the CPU.,It signifies that the page is part of the kernel's memory space.,It counts the number of times a page has been referenced.,B,"A locked frame, indicated by a set 'lock bit', cannot be selected for replacement by the page replacement algorithm, ensuring it stays in memory."
Which entity commonly locks some or all of its pages into memory because it often cannot tolerate a page fault?,A user web browser,A text editor application,The operating system kernel,A background utility program,A spreadsheet application,C,"The OS kernel typically locks some or all of its pages into memory because many operating systems cannot tolerate a kernel page fault, which could cause a system crash."
What is the term for a user process requesting to lock its pages into memory?,Swapping,Thrashing,Pinning,Caching,Relocation,C,"'Pinning' is the term used when user processes request to lock their pages into memory to prevent them from being paged out, often used by applications like databases."
What is a potential danger or risk associated with the use of lock bits for pages?,It significantly increases the CPU overhead.,"It might lead to a page being locked but never unlocked due to a bug, rendering the frame unusable.",It makes the system vulnerable to external attacks.,It requires special hardware support that is rarely available.,It always decreases the system's overall memory utilization.,B,"A significant danger of lock bits is the possibility of a bug causing a page to be locked but never unlocked, making that memory frame permanently unusable for other purposes."
"According to the text, what is 'prepaging'?",The process of writing modified pages back to disk.,Bringing pages into memory before they are requested.,The act of removing pages from memory to free up space.,Organizing pages in a hierarchical structure.,Translating virtual addresses to physical addresses.,B,Prepaging is defined as 'Bringing pages into memory before they are requested.'
What does 'hit ratio' refer to in the context of a cache like a TLB?,The total number of successful lookups.,The percentage of times a cache provides a valid lookup.,The speed at which data can be retrieved from the cache.,The number of times data is written to the cache.,The ratio of cache size to main memory size.,B,"Hit ratio is defined as the 'Percentage of times a cache provides a valid lookup (e.g., TLB effectiveness).'"
What is the definition of 'TLB reach'?,The speed of address translation by the TLB.,The number of entries in the TLB.,The amount of memory addressable by the translation look-aside buffer.,The maximum number of page faults a TLB can handle.,The ratio of virtual memory to physical memory.,C,TLB reach is defined as the 'Amount of memory addressable by the translation look-aside buffer.'
What are 'huge pages' in memory management?,Pages that contain an unusually large amount of data.,A feature designating a region of physical memory for especially large pages.,Pages that are too large to fit into the TLB.,Pages used exclusively by the operating system kernel.,Pages that have been corrupted and require special handling.,B,Huge pages are defined as a 'Feature designating a region of physical memory for especially large pages.'
"In ARM v8 CPUs, what is the purpose of the 'contiguous bit' in a TLB entry?",It indicates if the TLB entry has been modified.,It signals that the entry maps contiguous memory blocks.,It marks the entry for removal from the TLB.,It identifies the process ID associated with the entry.,It controls the read/write permissions of the mapped memory.,B,"The contiguous bit is defined as 'In ARM v8 CPUs, a TLB bit indicating mapping to contiguous memory blocks.'"
What does it mean for pages to be 'locked' in memory?,They are encrypted to prevent unauthorized access.,"They are fixed in place, preventing them from being paged out.",They are marked as read-only.,They are unavailable for use by user processes.,They are reserved for future memory allocations.,B,Locked pages are defined as 'Fixed in place; pages locked in memory to prevent paging out.'
What is 'pinning' in the context of page management?,Attaching a physical page to a specific virtual address.,Locking pages into memory to prevent them from being paged out.,Storing pages on a solid-state drive for faster access.,Marking pages as invalid in the page table.,The process of initially loading pages into memory.,B,Pinning is defined as 'Locking pages into memory to prevent them from being paged out.'
