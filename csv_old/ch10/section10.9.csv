Question,Option A,Option B,Option C,Option D,Option E,Answer,Explanation
What is the primary problem that 'pure demand paging' often faces when a process initially starts?,Excessive CPU utilization due to constant context switching.,"A large number of page faults, especially in the initial phase.",Insufficient physical memory leading to thrashing.,Difficulty in allocating contiguous memory blocks.,High overhead from TLB misses.,B,"Pure demand paging experiences a large number of page faults when a process starts, particularly due to initial locality, as pages are brought in only when referenced."
What is the main purpose of 'prepaging'?,To reduce the overall memory footprint of a process.,To prevent high initial paging activity when a process begins.,To ensure that all pages are modified before being written to disk.,To increase the system's bus transfer rate.,To optimize the page replacement algorithm.,B,"Prepaging is an attempt to prevent high initial paging, which is characteristic of pure demand paging when a process starts."
Which strategy describes how prepaging attempts to reduce initial page faults?,It limits the number of pages a process can request at once.,It brings all necessary pages into memory only when they are referenced.,It attempts to bring some or all potentially needed pages into memory at once.,It swaps out infrequently used pages to secondary storage.,It prioritizes pages based on their modification status.,C,"Prepaging's strategy is to bring some or all needed pages into memory at once, proactively, before they are explicitly requested by a page fault."
"In the context of prepaging, which model is cited as an example where the working set for a suspended process can be remembered and brought back upon resumption?",Least Recently Used (LRU) model,"First-In, First-Out (FIFO) model",Working-set model,Optimal replacement model,Clock replacement model,C,"The working-set model is given as an example for prepaging, where the working set of a suspended process is remembered and automatically brought back into memory upon process resumption."
"What is the primary advantage of prepaging, as discussed in the text?",It eliminates the need for a page table.,It reduces the cost of context switching.,It can potentially reduce total overhead by trading prepaging cost against page fault servicing cost.,It guarantees that no page will ever be swapped out.,It always results in higher memory utilization.,C,"The advantage of prepaging is the potential trade-off between the cost of prepaging and the cost of servicing individual page faults. If the prepaged pages are indeed used, it saves on the overhead of multiple page faults."
What is a significant risk associated with prepaging?,It can lead to an increase in TLB misses.,It may cause system thrashing if too many processes are prepaged.,"Many of the prepaged pages may not actually be used, wasting memory and I/O.",It requires significant hardware changes to implement.,It complicates the process of allocating virtual memory.,C,"A significant risk of prepaging is that many of the pages brought into memory proactively may not be used, leading to wasted memory and I/O resources."
"In the cost analysis of prepaging, if 's' pages are prepaged and 'α' is the fraction of those pages actually used, under what condition does prepaging 'win' (i.e., is beneficial)?",When α is approximately 0.,When α is approximately 0.5.,When α is approximately 1.,When s is very large.,When the cost of servicing a page fault is very low.,C,"Prepaging wins if 'α' (the fraction of prepaged pages actually used) is approximately 1, meaning most prepaged pages are indeed utilized, saving numerous page faults."
"Regarding prepaging, what is generally true about prepaging executable programs versus files?",Prepaging executable programs is more predictable due to their sequential nature.,Prepaging files is more difficult due to unclear page requirements.,Prepaging executable programs is typically easier as all code is needed upfront.,Prepaging files is more predictable because they are often accessed sequentially.,Neither executable programs nor files can be effectively prepaged.,D,"Prepaging files is described as more predictable because files are often accessed sequentially, making it easier to determine which pages might be needed next. Prepaging executable programs is noted as difficult."
Which Linux system call is mentioned as a mechanism for prefetching file contents into memory?,fork(),mmap(),readahead(),execve(),sync(),C,"The Linux `readahead()` system call is specifically mentioned as a way to prefetch file contents into memory, exemplifying prepaging for files."
"When designing a new machine, what is a key consideration regarding page size?","There is a single, universally best page size for all systems.","Page sizes are typically fixed at 4KB, regardless of system design.",The optimal page size depends on various factors and there is no single best size.,Page sizes must always be prime numbers.,Page size is irrelevant to system performance.,C,"The text states that there is no single best page size, and the decision depends on various factors that support different sizes."
What is the typical characteristic of page sizes in computer systems?,They are invariably odd numbers.,They are always multiples of 10.,"They are invariably powers of 2, typically ranging from 4,096 to 4,194,304 bytes.",They are determined by the amount of available RAM.,"They are always less than 1,024 bytes.",C,"Page sizes are invariably powers of 2, typically ranging from 4,096 ($2^{12}$) to 4,194,304 ($2^{22}$) bytes."
How does decreasing the page size affect the page table size?,"It decreases the number of pages, thus decreasing the page table size.","It increases the number of pages, thus increasing the page table size.",It has no effect on page table size.,It makes page table size dynamic and unpredictable.,It allows page tables to be stored entirely in hardware.,B,"Decreasing the page size means a given virtual memory space will be divided into more pages, thereby increasing the number of entries needed in the page table, which increases the page table size."
"For a 4 MB virtual memory, which page size would result in a smaller page table?","1,024 bytes per page","8,192 bytes per page",Both result in the same page table size.,Page table size is not affected by page size.,It depends on the number of active processes.,B,"Larger page sizes result in fewer pages for a given virtual memory space, thus reducing the size of the page table. For 4 MB virtual memory, 8,192 bytes per page (512 pages) results in a smaller page table than 1,024 bytes per page (4,096 pages)."
Which page size is generally desirable for minimizing the size of the page table?,"Very small page sizes (e.g., 64 bytes)","Medium page sizes (e.g., 4KB)",Large page sizes,Page size has no impact on page table size.,Dynamic page sizes that change during runtime.,C,"Large page sizes are desirable for page table size because fewer pages are needed to cover the same virtual address space, resulting in a smaller page table for each active process."
What is 'internal fragmentation' in the context of memory utilization with paging?,Memory lost due to pages being swapped out to disk.,"Unused space within an allocated page, because a process typically doesn't end exactly on a page boundary.","Memory fragmented into many small, non-contiguous blocks.",Overhead associated with managing the page table.,Memory wasted by system calls.,B,"Internal fragmentation refers to the unused part of the final page allocated to a process, as a process rarely ends exactly on a page boundary, leaving some space within that page allocated but unused."
"To minimize internal fragmentation, what characteristic should the page size have?",It should be as large as possible.,It should be a prime number.,It should be a small page size.,It should be equal to the process's total memory requirement.,It should be dynamically adjusted during execution.,C,"Minimizing internal fragmentation requires a small page size, as the average waste is half of the final page; a smaller page means less wasted space."
"Which component of I/O time is proportional to the page size, arguing for a small page size?",Seek time,Latency time,Transfer time,Queueing time,Processing time,C,"Transfer time, which is the time to actually move the data, is proportional to the page size. This factor, by itself, argues for a smaller page size to reduce transfer time."
Why do larger page sizes generally lead to less total I/O time despite increased transfer time per page?,Because larger pages fit better into disk cache.,"Because latency and seek times often dwarf the transfer time, making fewer, larger transfers more efficient.",Because larger pages enable parallel I/O operations.,Because they reduce the overall number of memory references.,Because they are easier for the memory controller to manage.,B,"Latency and seek times contribute significantly to total I/O time and are incurred once per I/O operation. By having larger pages, fewer I/O operations are needed, so even if transfer time per page increases, the reduction in seek and latency overheads often results in less total I/O time."
"In the context of I/O time, what does the text argue regarding page size?",Smaller page sizes minimize total I/O time.,Larger page sizes minimize total I/O time.,I/O time is independent of page size.,Only transfer time affects I/O efficiency.,Optimal I/O time is achieved with highly variable page sizes.,B,"The text argues that to minimize I/O time, a larger page size is needed, as the fixed overheads (seek, latency) dwarf the transfer time, making fewer, larger transfers more efficient."
How does a smaller page size affect locality and resolution in memory management?,"It decreases locality and resolution, leading to more unused data being brought in.","It improves locality and resolution, isolating more precisely the memory actually needed.",It has no impact on locality but significantly increases I/O.,It complicates memory management without tangible benefits.,It forces the system to use external fragmentation.,B,"Smaller page sizes are said to improve locality and resolution because each page can match a program's locality more accurately, allowing the system to isolate only the memory actually needed, thereby reducing total I/O and allocated memory."
What is the primary effect of a larger page size on the number of page faults?,It increases the number of page faults.,It significantly reduces the number of page faults.,It has no impact on the number of page faults.,It increases the overhead of each page fault.,It only affects page faults for very small processes.,B,"Larger page sizes mean that a single page can encompass a larger portion of a process's working set, reducing the frequency of page faults. The text provides an example where a 200 KB page results in 1 page fault, compared to 102,400 faults for 1-byte pages for a 200 KB process."
What has been the historical trend in page size selection for computer systems?,A consistent move towards smaller page sizes.,A fluctuation between large and small page sizes without a clear trend.,"A trend toward larger page sizes, even for mobile systems.",Page sizes becoming obsolete in modern architectures.,A fixed page size adopted universally since the 1990s.,C,"The historical trend has been toward larger page sizes, and modern systems, including Linux with 'huge pages', use much larger page sizes."
What is the definition of 'hit ratio' in the context of a TLB?,The speed at which the TLB can translate an address.,The percentage of virtual address translations that are resolved by the TLB.,The total amount of memory that the TLB can address.,The number of entries in the TLB.,The ratio of TLB misses to page faults.,B,Hit ratio of TLB is defined as the percentage of virtual address translations resolved in the TLB.
How is 'TLB reach' calculated?,Number of TLB entries divided by page size.,Number of TLB entries multiplied by page size.,Total virtual memory divided by number of TLB entries.,Physical memory size multiplied by page size.,Hit ratio multiplied by total memory.,B,TLB reach is calculated as the number of entries in the TLB multiplied by the page size.
What is the ideal scenario for a process regarding TLB reach?,The TLB hit ratio should be exactly 50%.,The process's entire working set should be able to fit within the TLB's reach.,The TLB should only contain entries for the kernel space.,The TLB should be as small as possible to conserve power.,The TLB should manage only physical addresses.,B,"Ideally, the working set for a process should be stored within the TLB to minimize the time spent resolving memory references in the page table."
What is a consequence of insufficient TLB reach for a process?,Increased internal fragmentation.,The process spends more time resolving memory references in the page table.,A higher frequency of page faults.,Reduced CPU utilization.,Physical memory becomes over-allocated.,B,"If the TLB reach is insufficient, the process will spend more time resolving memory references by traversing the page table in main memory, which is much slower than a TLB lookup."
"How can TLB reach be increased, according to the text?",By decreasing the number of TLB entries.,By decreasing the page size.,By increasing the number of TLB entries or by increasing the page size.,By reducing the processor's clock speed.,By using a software-only memory management unit.,C,"TLB reach can be increased by either doubling the number of TLB entries (which can be expensive) or by increasing the page size (e.g., from 4 KB to 16 KB, quadrupling the reach)."
What is a potential downside of increasing the page size to improve TLB reach?,It always leads to a decrease in the TLB hit ratio.,It can lead to increased internal fragmentation for some applications.,It significantly increases the cost of TLB entries.,It makes it impossible to support multiple page sizes.,It requires frequent flushing of the TLB.,B,"While increasing page size helps TLB reach, a stated downside is increased fragmentation for some applications, as more unused space might be allocated within larger pages."
Which ARM v8 architecture TLB entry bit is used to indicate that an entry maps contiguous (adjacent) blocks of memory?,Present bit,Dirty bit,Contiguous bit,Accessed bit,Execute bit,C,"The ARM v8 TLB entry includes a 'contiguous bit' which, when set, indicates that the entry maps contiguous (adjacent) blocks of memory."
What is the primary purpose of using inverted page tables?,To accelerate virtual-to-physical address translations.,To reduce the physical memory needed for virtual-to-physical address translations.,To eliminate the need for secondary storage in paging systems.,To provide a direct mapping from physical addresses to virtual addresses.,To increase the system's TLB hit ratio.,B,The main purpose of inverted page tables is to reduce the amount of physical memory required to store virtual-to-physical address translation information.
How do inverted page tables typically structure their entries?,"One entry per virtual page, indexed by process ID.","One entry per process, listing all its virtual pages.","One entry per page of physical memory, indexed by a combination of process-id and page-number.","One entry per CPU core, mapping its memory regions.","Multiple entries for each virtual page, for redundancy.",C,Inverted page tables work by having one entry for each page of physical memory. This entry is then typically indexed by a combination of process-id and page-number to locate the corresponding virtual page.
What is a significant downside of inverted page tables compared to traditional per-process page tables?,They require more physical memory for translation information.,They cannot support demand paging.,They no longer contain complete information about a process's logical address space.,They increase the TLB miss rate.,They are much slower for address translation.,C,"A significant downside is that inverted page tables no longer contain complete information about the logical address space of a process, which is necessary for handling page faults effectively."
How do systems using inverted page tables solve the problem of not having complete logical address space information for demand paging?,They store all logical address information directly in the TLB.,They use an external page table (one per process) that contains virtual page location information.,They entirely avoid demand paging.,They consult the disk for every address translation.,They dynamically regenerate logical address space information on demand.,B,"The solution is to keep an external page table (one per process), which functions like a traditional page table and contains the necessary virtual page location information. These external tables are only referenced on page faults."
What is the primary benefit of designing demand paging to be transparent to the user program?,It allows programs to run on systems with less physical memory than their virtual address space.,It simplifies the compilation process for all programming languages.,It removes the need for hardware-level memory management units.,It ensures that all I/O operations are handled asynchronously.,It prevents memory leaks from occurring in user applications.,A,"Demand paging's transparency allows programs to use a virtual address space that can be much larger than the available physical memory, without the programmer needing to manage memory explicitly."
"How can system performance with demand paging be improved, even though it's designed to be transparent?",By strictly enforcing a minimum page size.,By making the user or compiler aware of the paging mechanism and optimizing program structure.,By disabling the TLB for certain applications.,By always prepaging the entire process at startup.,By increasing the frequency of page table updates.,B,"The text states that system performance can be improved if the user or compiler is aware of demand paging, allowing for optimizations in program structure and data access patterns."
Consider initializing a 128x128 integer array on a system with 128-word pages. Which access order would result in significantly fewer page faults if less than 128 frames are allocated?,"Row-major order (	exttt{data[i][j]} with outer loop 	exttt{j}, inner loop 	exttt{i})","Column-major order (	exttt{data[i][j]} with outer loop 	exttt{i}, inner loop 	exttt{j})",Both orders would result in the same number of page faults.,A random access order would be most efficient.,"The number of page faults depends only on the total array size, not access order.",B,"Column-major order (outer loop 'i', inner loop 'j') results in much better locality for a C-style row-major stored array, zeroing all words on one page before moving to the next, drastically reducing page faults (from 16,384 to 128 in the example)."
Which programming construct is given as an example of good locality of reference?,A hash table,A linked list,A stack,A binary search tree,A sparse matrix,C,"A stack is given as an example of good locality because access always occurs at the top, meaning references are concentrated within a small, contiguous memory region."
Which programming construct is given as an example of bad locality of reference?,An array processed sequentially,A stack,A queue,A hash table,A tightly-packed struct,D,"A hash table is cited as an example of bad locality because it tends to scatter references across different memory locations, leading to more page faults."
How can separating code and data segments and using reentrant code improve paging performance?,It allows the system to completely eliminate page faults.,"It ensures that code pages are read-only and never modified, thus becoming 'clean pages' that don't need to be paged out.",It reduces the total physical memory required by the system.,It simplifies the process of creating shared libraries.,It increases the frequency of TLB hits for data segments.,B,"Separating code and data, and using reentrant code, means code pages are read-only. This makes them 'clean pages' that do not need to be written back to disk when replaced, improving performance."
What optimization can a loader perform to improve paging performance?,Always placing routines across page boundaries.,Avoiding placing routines across page boundaries to keep them within one page.,Randomly distributing routine segments in memory.,Increasing the number of virtual address spaces.,Reducing the total number of physical memory frames.,B,"A loader can improve performance by avoiding placing routines across page boundaries, aiming to keep each routine entirely within a single page to reduce page faults."
What is the primary reason pages need to be 'locked' in memory in demand paging systems?,To prevent unauthorized access to critical system data.,To ensure that I/O buffers are not paged out while an I/O operation is in progress.,To optimize the TLB hit ratio for frequently accessed pages.,To reduce internal fragmentation in memory.,To increase the system's overall memory capacity.,B,"Pages need to be locked in memory to prevent I/O buffers from being paged out to disk while a separate I/O processor is accessing them, which would lead to incorrect data transfer."
Which of the following describes a problem scenario that necessitates page locking?,A process frequently accesses pages that are scattered across different physical frames.,A low-priority process acquires a page that a high-priority process needs.,"An I/O operation targets a user memory buffer, but that buffer's page is replaced before I/O completes.",The system runs out of physical memory due to too many active processes.,"The page table grows too large, slowing down address translation.",C,"The problem scenario described is when a process issues I/O to a buffer, but that buffer's page is paged out (replaced) before the I/O operation finishes, leading to data corruption or errors."
"What is one common solution to the I/O interlock problem, besides copying data to/from system memory?",Increasing the page size to reduce page faults.,Using a dedicated I/O processor that only accesses system memory.,Allowing pages to be locked into memory using a 'lock bit' associated with each frame.,Disabling demand paging during I/O operations.,Implementing a faster page replacement algorithm.,C,"One common solution is to allow pages to be locked into memory. This is typically done by setting a 'lock bit' associated with each memory frame, preventing it from being selected for replacement."
What is the function of a 'lock bit' associated with a memory frame?,"It marks the page as dirty, indicating it needs to be written to disk.",It prevents the frame from being selected for replacement by the page replacement algorithm.,It indicates that the page is currently being accessed by the CPU.,It signifies that the page is part of the kernel's memory space.,It counts the number of times a page has been referenced.,B,"A locked frame, indicated by a set 'lock bit', cannot be selected for replacement by the page replacement algorithm, ensuring it stays in memory."
Which entity commonly locks some or all of its pages into memory because it often cannot tolerate a page fault?,A user web browser,A text editor application,The operating system kernel,A background utility program,A spreadsheet application,C,"The OS kernel typically locks some or all of its pages into memory because many operating systems cannot tolerate a kernel page fault, which could cause a system crash."
What is the term for a user process requesting to lock its pages into memory?,Swapping,Thrashing,Pinning,Caching,Relocation,C,"'Pinning' is the term used when user processes request to lock their pages into memory to prevent them from being paged out, often used by applications like databases."
What is a potential danger or risk associated with the use of lock bits for pages?,It significantly increases the CPU overhead.,"It might lead to a page being locked but never unlocked due to a bug, rendering the frame unusable.",It makes the system vulnerable to external attacks.,It requires special hardware support that is rarely available.,It always decreases the system's overall memory utilization.,B,"A significant danger of lock bits is the possibility of a bug causing a page to be locked but never unlocked, making that memory frame permanently unusable for other purposes."
"According to the text, what is 'prepaging'?",The process of writing modified pages back to disk.,Bringing pages into memory before they are requested.,The act of removing pages from memory to free up space.,Organizing pages in a hierarchical structure.,Translating virtual addresses to physical addresses.,B,Prepaging is defined as 'Bringing pages into memory before they are requested.'
What does 'hit ratio' refer to in the context of a cache like a TLB?,The total number of successful lookups.,The percentage of times a cache provides a valid lookup.,The speed at which data can be retrieved from the cache.,The number of times data is written to the cache.,The ratio of cache size to main memory size.,B,"Hit ratio is defined as the 'Percentage of times a cache provides a valid lookup (e.g., TLB effectiveness).'"
What is the definition of 'TLB reach'?,The speed of address translation by the TLB.,The number of entries in the TLB.,The amount of memory addressable by the translation look-aside buffer.,The maximum number of page faults a TLB can handle.,The ratio of virtual memory to physical memory.,C,TLB reach is defined as the 'Amount of memory addressable by the translation look-aside buffer.'
What are 'huge pages' in memory management?,Pages that contain an unusually large amount of data.,A feature designating a region of physical memory for especially large pages.,Pages that are too large to fit into the TLB.,Pages used exclusively by the operating system kernel.,Pages that have been corrupted and require special handling.,B,Huge pages are defined as a 'Feature designating a region of physical memory for especially large pages.'
"In ARM v8 CPUs, what is the purpose of the 'contiguous bit' in a TLB entry?",It indicates if the TLB entry has been modified.,It signals that the entry maps contiguous memory blocks.,It marks the entry for removal from the TLB.,It identifies the process ID associated with the entry.,It controls the read/write permissions of the mapped memory.,B,"The contiguous bit is defined as 'In ARM v8 CPUs, a TLB bit indicating mapping to contiguous memory blocks.'"
What does it mean for pages to be 'locked' in memory?,They are encrypted to prevent unauthorized access.,"They are fixed in place, preventing them from being paged out.",They are marked as read-only.,They are unavailable for use by user processes.,They are reserved for future memory allocations.,B,Locked pages are defined as 'Fixed in place; pages locked in memory to prevent paging out.'
What is 'pinning' in the context of page management?,Attaching a physical page to a specific virtual address.,Locking pages into memory to prevent them from being paged out.,Storing pages on a solid-state drive for faster access.,Marking pages as invalid in the page table.,The process of initially loading pages into memory.,B,Pinning is defined as 'Locking pages into memory to prevent them from being paged out.'
